<!DOCTYPE html>
<html>
<head>
    <meta charset="utf-8">
<meta name="pinterest" content="nopin">
<meta name="viewport" content="width=device-width,minimum-scale=1,initial-scale=1">
<meta name="generator" content="Hugo 0.87.0" />



<link rel="canonical" href="https://davidejones.github.io/hugo-hn/2021/08/18/hash-collision-in-apple-neuralhash-model/">


    <title>Hash collision in Apple NeuralHash model - Hugo Hacker News</title>
    
<meta name="description" content="">

<meta property="og:title" content="Hash collision in Apple NeuralHash model - Hugo Hacker News">
<meta property="og:type" content="article">
<meta property="og:url" content="https://davidejones.github.io/hugo-hn/2021/08/18/hash-collision-in-apple-neuralhash-model/">
<meta property="og:image" content="https://davidejones.github.io/hugo-hn/images/default.png">
<meta property="og:site_name" content="Hugo Hacker News">
<meta property="og:description" content="">
<meta property="og:locale" content="ja_JP">

<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:site" content="Hugo Hacker News">
<meta name="twitter:url" content="https://davidejones.github.io/hugo-hn/2021/08/18/hash-collision-in-apple-neuralhash-model/">
<meta name="twitter:title" content="Hash collision in Apple NeuralHash model - Hugo Hacker News">
<meta name="twitter:description" content="">
<meta name="twitter:image" content="https://davidejones.github.io/hugo-hn/images/default.png">


<script type="application/ld+json">
  {
    "@context": "http://schema.org",
    "@type": "NewsArticle",
    "mainEntityOfPage": {
      "@type": "WebPage",
      "@id":"https:\/\/davidejones.github.io\/hugo-hn\/"
    },
    "headline": "Hash collision in Apple NeuralHash model - Hugo Hacker News",
    "image": {
      "@type": "ImageObject",
      "url": "https:\/\/davidejones.github.io\/hugo-hn\/images\/default.png",
      "height": 800,
      "width": 800
    },
    "datePublished": "2021-08-18T09:13:12JST",
    "dateModified": "2021-08-18T09:13:12JST",
    "author": {
      "@type": "Person",
      "name": "Hugo Hacker News"
    },
    "publisher": {
      "@type": "Organization",
      "name": "Hugo Hacker News",
      "logo": {
        "@type": "ImageObject",
        "url": "https:\/\/davidejones.github.io\/hugo-hn\/images/logo.png",
        "width": 600,
        "height": 60
      }
    },
    "description": ""
  }
</script>



    <link href="https://fonts.googleapis.com/css?family=Open+Sans" rel="stylesheet">

    
    
    
    
        
    
    
    <link href="https://davidejones.github.io/hugo-hn/style.min.130df59cea3bafd8a515eeb4a51c215616cb41f2a9520765b9f1574d3cfedf2c485abb4a8d785adcd7acdc2400797ba6abffe5b0bb4612cb79bc0b884aac89e8.css" rel="stylesheet" />
</head>
<body class="post">
    <header>
        <a href="https://davidejones.github.io/hugo-hn/">Hugo Hacker News</a>
        <nav>
            <ul>
                
                    <li><a href="/hugo-hn/">new</a></li>
                
                    <li><a href="/hugo-hn/comments/">comments</a></li>
                
                    <li><a href="/hugo-hn/categories/show/">show</a></li>
                
                    <li><a href="/hugo-hn/categories/ask/">ask</a></li>
                
                    <li><a href="/hugo-hn/categories/jobs/">jobs</a></li>
                
                    <li><a href="https://news.ycombinator.com/submit">submit</a></li>
                
            </ul>
        </nav>
        <select onchange="myChangeHandler(this)">
            
                <option value="/hugo-hn/">new</option>
            
                <option value="/hugo-hn/comments/">comments</option>
            
                <option value="/hugo-hn/categories/show/">show</option>
            
                <option value="/hugo-hn/categories/ask/">ask</option>
            
                <option value="/hugo-hn/categories/jobs/">jobs</option>
            
                <option value="https://news.ycombinator.com/submit">submit</option>
            
        </select>
    </header>
    <main>
        
<article>
    <header>
        <h1><a href="https://github.com/AsuharietYgvar/AppleNeuralHash2ONNX/issues/1">Hash collision in Apple NeuralHash model</a></h1>
        
    </header>
    
        
            


    
    <div class="comment ">
        <p><a href="https://news.ycombinator.com/user?id=dang" target="_blank">dang</a>   <span class="timeago" data-date="2021-08-18 19:11:40 &#43;0000 UTC">2021-08-18 19:11:40 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            Ongoing related threads:<p><i>Apple defends anti-child abuse imagery tech after claims of ‘hash collisions’</i> - <a href="https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=28225706" rel="nofollow">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=28225706</a><p><i>Convert Apple NeuralHash model for CSAM Detection to ONNX</i> - <a href="https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=28218391" rel="nofollow">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=28218391</a>
        </div>
        <div class="children">
            
        </div>
    </div>


        
            


    
    <div class="comment hasChildren">
        <p><a href="https://news.ycombinator.com/user?id=topynate" target="_blank">topynate</a>   <span class="timeago" data-date="2021-08-18 12:58:51 &#43;0000 UTC">2021-08-18 12:58:51 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            Second preimage attacks are trivial because of how the algorithm works. The image goes through a neural network (one to which everyone has access), the output vector is put through a linear transformation, and <i>that</i> vector is binarized, then cryptographically hashed. It&#x27;s trivial to perturb any image you might wish so as to be close to the original output vector. This will result in it having the same binarization, hence the same hash. I believe the neural network is a pretty conventional convolutional one, so adversarial perturbations will exist that are invisible to the naked eye.<p>This is useful for two purposes I can think of. One, you can randomize all the vectors on all of your images. Two, you can make problems for others by giving them harmless-looking images that have been cooked to give particular hashes. I&#x27;m not sure how bad those problems would be – at some point a police officer does have to look at the image in order to get probable cause. Perhaps it could lead to your Apple account being suspended, however.
        </div>
        <div class="children">
            
                


    
    <div class="comment hasChildren">
        <p><a href="https://news.ycombinator.com/user?id=Kalium" target="_blank">Kalium</a>   <span class="timeago" data-date="2021-08-18 13:08:16 &#43;0000 UTC">2021-08-18 13:08:16 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            A police raid on a person&#x27;s home, or even a gentler thorough search, can be enough to quite seriously disrupt a person&#x27;s life. Certainly having the police walk away with all your electronics in evidence bags will complicate trying to work remotely.<p>Of course, this is assuming everything works as intended and they don&#x27;t find anything else they can use to charge you with something as they search your home. If you smoke cannabis while being in the wrong state, you&#x27;re now in several more kinds of trouble.
        </div>
        <div class="children">
            
                


    
    <div class="comment hasChildren">
        <p><a href="https://news.ycombinator.com/user?id=topynate" target="_blank">topynate</a>   <span class="timeago" data-date="2021-08-18 13:16:51 &#43;0000 UTC">2021-08-18 13:16:51 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            This could happen with a perturbed image, but I doubt it. Apple will send the suspicious images to the relevant authorities. Those authorities will then look at the images. The chances are low that they will then seek a search, even though the images are innocent upon visual inspection. But maybe in some places a ping from Apple is good enough for a search and seizure.
        </div>
        <div class="children">
            
                


    
    <div class="comment hasChildren">
        <p><a href="https://news.ycombinator.com/user?id=falcolas" target="_blank">falcolas</a>   <span class="timeago" data-date="2021-08-18 14:48:26 &#43;0000 UTC">2021-08-18 14:48:26 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            FWIW, they won&#x27;t send the images. Even in the pursuit of knocking back CSAM, there are strict restrictions on the transmission and viewing of CSAM - in some cases even the defendant&#x27;s lawyers don&#x27;t usually see the images themselves in preparation for a trial, just a description of the contents. Apple employees or contractors will likely not look at the images themselves, only visual hashes.<p>They will instead contact the police and say &quot;Person X has Y images that are on list Z,&quot; and let the police get a warrant based off that information and execute it to check for actual CSAM.
        </div>
        <div class="children">
            
                


    
    <div class="comment hasChildren">
        <p><a href="https://news.ycombinator.com/user?id=topynate" target="_blank">topynate</a>   <span class="timeago" data-date="2021-08-18 15:01:23 &#43;0000 UTC">2021-08-18 15:01:23 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            On reflection, yes, there must be warrants involved. I&#x27;m raising my estimate of how likely it is that innocent people get raided due to this. The warrant would properly only be to search iCloud, not some guy&#x27;s house, but I can easily see overly-broad warrants being issued.
        </div>
        <div class="children">
            
                


    
    <div class="comment hasChildren">
        <p><a href="https://news.ycombinator.com/user?id=gowld" target="_blank">gowld</a>   <span class="timeago" data-date="2021-08-18 15:27:43 &#43;0000 UTC">2021-08-18 15:27:43 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            &gt; The warrant would properly only be to search iCloud,<p>iCloud is encrypted, so that warrant is useless.<p>They need to unlock and search the device.
        </div>
        <div class="children">
            
                


    
    <div class="comment ">
        <p><a href="https://news.ycombinator.com/user?id=ggreer" target="_blank">ggreer</a>   <span class="timeago" data-date="2021-08-18 20:43:18 &#43;0000 UTC">2021-08-18 20:43:18 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            The data is encrypted, but Apple has the keys. If they get a warrant, they&#x27;ll decrypt your data and hand it over. See page 11 of Apple&#x27;s law enforcement process guidelines[1]:<p>&gt; iCloud content may include email, stored photos, documents, contacts, calendars, bookmarks, Safari Browsing History, Maps Search History, Messages and iOS device backups. iOS device backups may include photos and videos in the Camera Roll, device settings, app data, iMessage, Business Chat, SMS, and MMS messages and voicemail. All iCloud content data stored by Apple is encrypted at the location of the server. When third-party vendors are used to store data, Apple never gives them the encryption keys. Apple retains the encryption keys in its U.S. data centers. iCloud content, as it exists in the customer’s account, may be provided in response to a search warrant issued upon a showing of probable cause, or customer consent.<p>1. <a href="https:&#x2F;&#x2F;www.apple.com&#x2F;legal&#x2F;privacy&#x2F;law-enforcement-guidelines-us.pdf" rel="nofollow">https:&#x2F;&#x2F;www.apple.com&#x2F;legal&#x2F;privacy&#x2F;law-enforcement-guidelin...</a>
        </div>
        <div class="children">
            
        </div>
    </div>


            
                


    
    <div class="comment hasChildren">
        <p><a href="https://news.ycombinator.com/user?id=topynate" target="_blank">topynate</a>   <span class="timeago" data-date="2021-08-18 15:34:29 &#43;0000 UTC">2021-08-18 15:34:29 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            Yes, it&#x27;s encrypted, but part of this anti-CSAM strategy is a threshold encryption scheme that allows Apple to decrypt photos if a certain number of them have suspicious hashes.
        </div>
        <div class="children">
            
                


    
    <div class="comment hasChildren">
        <p><a href="https://news.ycombinator.com/user?id=Kalium" target="_blank">Kalium</a>   <span class="timeago" data-date="2021-08-18 16:17:53 &#43;0000 UTC">2021-08-18 16:17:53 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            Apple having any kind of ability to decrypt user contents is disconcerting. It means they can be subpoenaed for that information.
        </div>
        <div class="children">
            
                


    
    <div class="comment ">
        <p><a href="https://news.ycombinator.com/user?id=heavyset_go" target="_blank">heavyset_go</a>   <span class="timeago" data-date="2021-08-18 20:45:10 &#43;0000 UTC">2021-08-18 20:45:10 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            &gt; <i>It means they can be subpoenaed for that information.</i><p>They regularly are, and they regularly give up customer data in order to comply with subpeonas[1]. They give up customer data in response to government requests for 150,000 users&#x2F;accounts a year[1].<p>[1] <a href="https:&#x2F;&#x2F;www.apple.com&#x2F;legal&#x2F;transparency&#x2F;us.html" rel="nofollow">https:&#x2F;&#x2F;www.apple.com&#x2F;legal&#x2F;transparency&#x2F;us.html</a>
        </div>
        <div class="children">
            
        </div>
    </div>


            
        </div>
    </div>


            
                


    
    <div class="comment hasChildren">
        <p><a href="https://news.ycombinator.com/user?id=bbatsell" target="_blank">bbatsell</a>   <span class="timeago" data-date="2021-08-18 19:02:33 &#43;0000 UTC">2021-08-18 19:02:33 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            No, the threshold encryption only allows for the 30+ cryptographic “vouchers” to be unlocked, which contain details about the hash matching as well as a “visual derivative” of the image. We don’t know any details about the visual derivative.
        </div>
        <div class="children">
            
                


    
    <div class="comment hasChildren">
        <p><a href="https://news.ycombinator.com/user?id=sudosysgen" target="_blank">sudosysgen</a>   <span class="timeago" data-date="2021-08-18 19:33:34 &#43;0000 UTC">2021-08-18 19:33:34 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            Im guessing the visual derivative is a difference of sorts between the image and the CSAM? Of course not sure.
        </div>
        <div class="children">
            
                


    
    <div class="comment ">
        <p><a href="https://news.ycombinator.com/user?id=HALtheWise" target="_blank">HALtheWise</a>   <span class="timeago" data-date="2021-08-18 19:40:06 &#43;0000 UTC">2021-08-18 19:40:06 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            No, since apple _definitely_ isn&#x27;t sending CSAM photos to your phone so they can be differ. Most likely, the visual derivative is a thumbnail or blurred version of the image.
        </div>
        <div class="children">
            
        </div>
    </div>


            
        </div>
    </div>


            
        </div>
    </div>


            
        </div>
    </div>


            
                


    
    <div class="comment ">
        <p><a href="https://news.ycombinator.com/user?id=heavyset_go" target="_blank">heavyset_go</a>   <span class="timeago" data-date="2021-08-18 20:42:47 &#43;0000 UTC">2021-08-18 20:42:47 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            It&#x27;s &quot;encrypted&quot;, but Apple holds the keys and they regularly give up customers&#x27; data in response to government requests for it[1].<p>[1] <a href="https:&#x2F;&#x2F;www.apple.com&#x2F;legal&#x2F;transparency&#x2F;us.html" rel="nofollow">https:&#x2F;&#x2F;www.apple.com&#x2F;legal&#x2F;transparency&#x2F;us.html</a>
        </div>
        <div class="children">
            
        </div>
    </div>


            
                


    
    <div class="comment ">
        <p><a href="https://news.ycombinator.com/user?id=alfiedotwtf" target="_blank">alfiedotwtf</a>   <span class="timeago" data-date="2021-08-18 19:23:55 &#43;0000 UTC">2021-08-18 19:23:55 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            Didn&#x27;t the FBI stop Apple from encrypting iCloud, and only Messenger is e2e?
        </div>
        <div class="children">
            
        </div>
    </div>


            
        </div>
    </div>


            
        </div>
    </div>


            
                


    
    <div class="comment ">
        <p><a href="https://news.ycombinator.com/user?id=__blockcipher__" target="_blank">__blockcipher__</a>   <span class="timeago" data-date="2021-08-18 21:05:00 &#43;0000 UTC">2021-08-18 21:05:00 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            &gt; in some cases even the defendant&#x27;s lawyers don&#x27;t usually see the images themselves in preparation for a trial, just a description of the contents<p>Man that seems horrible. So you just have to trust the description is accurate? You’d think there’d at least be a “private viewing room” type thing (I get the obvious concern of not giving them a file to take home)
        </div>
        <div class="children">
            
        </div>
    </div>


            
        </div>
    </div>


            
                


    
    <div class="comment ">
        <p><a href="https://news.ycombinator.com/user?id=Kalium" target="_blank">Kalium</a>   <span class="timeago" data-date="2021-08-18 13:27:12 &#43;0000 UTC">2021-08-18 13:27:12 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            In broad strokes, I agree with you. I think you&#x27;re absolutely correct that most trained, educated, technologically sophisticated law enforcement bodies will do exactly as you suggest and decide that there&#x27;s not enough to investigate.<p>That said, I&#x27;m not willing to say it won&#x27;t happen. There are too many law enforcement entities of wildly varying levels of professionalism, staffing, and technical sophistication. Someone innocent, somewhere, is likely to have a multi-year legal drama because their local PD got an email from Apple.<p>And we haven&#x27;t even gotten to subjects like how some LEOs will happily plant evidence once they decide you&#x27;re guilty.
        </div>
        <div class="children">
            
        </div>
    </div>


            
        </div>
    </div>


            
                


    
    <div class="comment hasChildren">
        <p><a href="https://news.ycombinator.com/user?id=cirrus3" target="_blank">cirrus3</a>   <span class="timeago" data-date="2021-08-18 23:50:06 &#43;0000 UTC">2021-08-18 23:50:06 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            The police do not show up until a human has compared the matched image to the actual.<p>Just stop.
        </div>
        <div class="children">
            
                


    
    <div class="comment hasChildren">
        <p><a href="https://news.ycombinator.com/user?id=teachrdan" target="_blank">teachrdan</a>   <span class="timeago" data-date="2021-08-18 23:52:46 &#43;0000 UTC">2021-08-18 23:52:46 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            The more collisions, the more chances of a false positive by a (tired, underpaid) human. I don&#x27;t envy the innocent person whose home gets raided by a SWAT team convinced they&#x27;re busting a child sex trafficker.
        </div>
        <div class="children">
            
                


    
    <div class="comment hasChildren">
        <p><a href="https://news.ycombinator.com/user?id=hda2" target="_blank">hda2</a>   <span class="timeago" data-date="2021-08-19 02:57:57 &#43;0000 UTC">2021-08-19 02:57:57 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            The database is known to contain non-csam images like porn. I doubt the reviewer will be qualified to discern which is which.
        </div>
        <div class="children">
            
                


    
    <div class="comment ">
        <p><a href="https://news.ycombinator.com/user?id=Operyl" target="_blank">Operyl</a>   <span class="timeago" data-date="2021-08-19 10:53:07 &#43;0000 UTC">2021-08-19 10:53:07 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            Not that I doubt your motives, but can I get your source on this? It seems like a huge blunder if so.
        </div>
        <div class="children">
            
        </div>
    </div>


            
        </div>
    </div>


            
        </div>
    </div>


            
        </div>
    </div>


            
        </div>
    </div>


            
                


    
    <div class="comment hasChildren">
        <p><a href="https://news.ycombinator.com/user?id=e_proxus" target="_blank">e_proxus</a>   <span class="timeago" data-date="2021-08-18 18:59:43 &#43;0000 UTC">2021-08-18 18:59:43 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            Wouldn’t it also just be possible to turn a jailbroken iDevice into a CSAM cleaner&#x2F;hider?<p>You could take actual CSAM, check if it matches the hashes and keep modifying the material until it doesn’t (adding borders, watermarking, changing dimensions etc.). Then just save it as usual without any risk.
        </div>
        <div class="children">
            
                


    
    <div class="comment ">
        <p><a href="https://news.ycombinator.com/user?id=cyanite" target="_blank">cyanite</a>   <span class="timeago" data-date="2021-08-18 19:51:05 &#43;0000 UTC">2021-08-18 19:51:05 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            No it’s not. The client hashes against a blinded set, and doesn’t know whether or not the hash is a hit or miss.
        </div>
        <div class="children">
            
        </div>
    </div>


            
        </div>
    </div>


            
        </div>
    </div>


        
            


    
    <div class="comment hasChildren">
        <p><a href="https://news.ycombinator.com/user?id=tambourine_man" target="_blank">tambourine_man</a>   <span class="timeago" data-date="2021-08-18 13:18:25 &#43;0000 UTC">2021-08-18 13:18:25 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            Neuralhashes are far from my area of expertise, but I&#x27;ve been following Apple closely ever since its foundation and have probably watched every public video of Craig since the NeXT take over and here is my take: I&#x27;ve never seen him so off balance before as in his latest interview with Joanna Stern. Not even in the infamous “shaking mouse hand close up” of the early days.<p>Whatever you say about Apple, they are an extremely well oiled communication machine. Every C-level phrase has a well thought out message to deliver.<p>This interview was a train wreck. Joanna kept asking: please, in simple terms, to a hesitant and inarticulate Craig. It was so bad that she had to produce infographics to fill the communication void left by Apple.<p>They usually do their best to “take control” of the narrative. They were clearly caught way off guard here. And that&#x27;s revealing.
        </div>
        <div class="children">
            
                


    
    <div class="comment hasChildren">
        <p><a href="https://news.ycombinator.com/user?id=cwizou" target="_blank">cwizou</a>   <span class="timeago" data-date="2021-08-18 13:59:09 &#43;0000 UTC">2021-08-18 13:59:09 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            I think they clearly didn&#x27;t anticipate that people would perceive it as anything but a breach of trust, that their device was working against them (even for a good cause, against the worst people).<p>And because of this they calibrated their communication completely wrong, focusing on the on device part as being more private. Using the same line of thinking they use for putting Siri on device.<p>And the follow up was an uncoordinated mess that didn&#x27;t help either (as you rightly pointed out with Craig&#x27;s interview). In the Neuenschwander interview [1], he stated this :<p>&gt; The hash list is built into the operating system, we have one global operating system and don’t have the ability to target updates to individual users and so hash lists will be shared by all users when the system is enabled.<p>This still has me confused, here&#x27;s my understanding so far (please feel free to correct me)<p>- Apple is shipping a neural network trained on the dataset that generates NeuralHashes<p>- Apple also ships (where ?) a &quot;blinded&quot; (by an eliptic curve algo) table lookup that match (all possible?!) NeuralHashes to a key<p>- This key is used to encrypt the NeuralHash and the derivative image (that would be used by the manual review) and this bundle is called the voucher<p>- A final check is done on server using the secret used to generate the elliptic curve to reverse the NeuralHash and check it server side against the known database<p>- If 30 or more are detected, decrypt all vouchers and send the derivative images to manual review.<p>I think I&#x27;m missing something regarding the blinded table as I don&#x27;t see what it brings to the table in that scenario, apart from adding a complex key generation for the vouchers. If that table only contained the NeuralHashes of known CSAM images as keys, that would be as good as giving the list to people knowing the model is easily extracted. And if it&#x27;s not a table lookup but just a cryptographic function, I don&#x27;t see where the blinded table is coming from in Apple&#x27;s documentation [2].<p>Assuming above assumptions are correct, I&#x27;m paradoxically feeling a tiny bit better about that system on a technical level (I still think doing anything client side is a very bad precedent), but what a mess did they put themselves into.<p>Had they done this purely server side (and to be frank there&#x27;s not much difference, the significant part seems to be done server side) this would have been a complete non-event.<p>[1] : <a href="https:&#x2F;&#x2F;daringfireball.net&#x2F;linked&#x2F;2021&#x2F;08&#x2F;11&#x2F;panzarino-neuenschwander-interview" rel="nofollow">https:&#x2F;&#x2F;daringfireball.net&#x2F;linked&#x2F;2021&#x2F;08&#x2F;11&#x2F;panzarino-neuen...</a><p>[2] This is my understanding based on the repository and what&#x27;s written page 6-7 : <a href="https:&#x2F;&#x2F;www.apple.com&#x2F;child-safety&#x2F;pdf&#x2F;CSAM_Detection_Technical_Summary.pdf" rel="nofollow">https:&#x2F;&#x2F;www.apple.com&#x2F;child-safety&#x2F;pdf&#x2F;CSAM_Detection_Techni...</a>
        </div>
        <div class="children">
            
                


    
    <div class="comment ">
        <p><a href="https://news.ycombinator.com/user?id=brokenmachine" target="_blank">brokenmachine</a>   <span class="timeago" data-date="2021-08-19 00:45:47 &#43;0000 UTC">2021-08-19 00:45:47 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            Thanks for the description.<p>That&#x27;s a *huge* amount of crypto mumbo-jumbo for a system to <i>scan your data on your own device</i> and send it to the authorities.<p>They must really care about children!!<p>If only this system was in place while Trump, Jeffrey Epstein, and Prince Andrew were raping children, surely none of that would have happened!! &#x2F;s
        </div>
        <div class="children">
            
        </div>
    </div>


            
        </div>
    </div>


            
                


    
    <div class="comment hasChildren">
        <p><a href="https://news.ycombinator.com/user?id=HatchedLake721" target="_blank">HatchedLake721</a>   <span class="timeago" data-date="2021-08-18 15:09:18 &#43;0000 UTC">2021-08-18 15:09:18 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            Can you link the interview please?
        </div>
        <div class="children">
            
                


    
    <div class="comment hasChildren">
        <p><a href="https://news.ycombinator.com/user?id=brandon272" target="_blank">brandon272</a>   <span class="timeago" data-date="2021-08-18 15:36:34 &#43;0000 UTC">2021-08-18 15:36:34 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            I assume it is this one: <a href="https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=OQUO1DSwYN0" rel="nofollow">https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=OQUO1DSwYN0</a><p>This was painful to watch.
        </div>
        <div class="children">
            
                


    
    <div class="comment ">
        <p><a href="https://news.ycombinator.com/user?id=tambourine_man" target="_blank">tambourine_man</a>   <span class="timeago" data-date="2021-08-18 17:12:06 &#43;0000 UTC">2021-08-18 17:12:06 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            That&#x27;s the one, yes. Thank you.
        </div>
        <div class="children">
            
        </div>
    </div>


            
        </div>
    </div>


            
        </div>
    </div>


            
        </div>
    </div>


        
            


    
    <div class="comment hasChildren">
        <p><a href="https://news.ycombinator.com/user?id=FabHK" target="_blank">FabHK</a>   <span class="timeago" data-date="2021-08-18 10:20:30 &#43;0000 UTC">2021-08-18 10:20:30 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            How can you use it for targeted attacks?<p>This is what would need to happen:<p>1. Attacker generates images that collide with known CSAM material in the database (the NeuralHashes of which, unless I&#x27;m mistaken, are not available)<p>2. Attacker sends that to innocent person<p>3. Innocent person accepts and stores the picture<p>4. Actually, need to run step 1-3 at least 30 times<p>5. Innocent person has iCloud syncing enabled<p>6. Apple&#x27;s CSAM detection then flags these, and they&#x27;re manually reviewed<p>7. Apple reviewer confuses a featureless blob of gray with CSAM material, several times<p>Note that other cloud providers have been scanning uploaded photos for years. What has changed wrt targeted attacks against innocent people?
        </div>
        <div class="children">
            
                


    
    <div class="comment hasChildren">
        <p><a href="https://news.ycombinator.com/user?id=fsloth" target="_blank">fsloth</a>   <span class="timeago" data-date="2021-08-18 11:01:10 &#43;0000 UTC">2021-08-18 11:01:10 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            &quot;How can you use it for targeted attacks?&quot;<p>Just insert a known CSAM image on target&#x27;s device. Done.<p>I presume this could be used against a rival political party to ruin their reputation - insert bunch of CSAM images on their devices. &quot;Party X is revealed as an abuse ring&quot;. This goes oh-so-very-nicely with Qanon conspiracy theories which even don&#x27;t require any evidence to propagate widely.<p>Wait for Apple to find the images. When police investigation is opened, make it very public. Start a social media campaign at the same time.<p>It&#x27;s enough to <i>fabricate evidence only for a while</i> - the public perception of the individual or the group will be perpetually altered, even though it would surface later that the CSAM material was inserted by hostile third party.<p>You have to think about what nation state entities that are now clients of Pegasus and so on could do with this. Not how safe the individual component is.
        </div>
        <div class="children">
            
                


    
    <div class="comment ">
        <p><a href="https://news.ycombinator.com/user?id=kreitje" target="_blank">kreitje</a>   <span class="timeago" data-date="2021-08-18 11:28:50 &#43;0000 UTC">2021-08-18 11:28:50 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            FL Rep Randy Fine filed a report with the Florida Department of Law Enforcement that the sheriff was going to plant CSAM on his computer and arrest him for it.<p>They are even in the same political party.<p><a href="https:&#x2F;&#x2F;www.reddit.com&#x2F;r&#x2F;321&#x2F;comments&#x2F;jt32rs&#x2F;fdle_report_between_randy_fine_and_wayne_ivey&#x2F;" rel="nofollow">https:&#x2F;&#x2F;www.reddit.com&#x2F;r&#x2F;321&#x2F;comments&#x2F;jt32rs&#x2F;fdle_report_bet...</a>
        </div>
        <div class="children">
            
        </div>
    </div>


            
                


    
    <div class="comment ">
        <p><a href="https://news.ycombinator.com/user?id=JimBard2" target="_blank">JimBard2</a>   <span class="timeago" data-date="2021-08-18 12:58:59 &#43;0000 UTC">2021-08-18 12:58:59 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            Indeed this is not new and has probably been happening for many years already. There are services advertising on dark net markets to “ruin someone’s life” which means you pay some Ukrainian guy $300 in Bitcoin and he plants CSAM on a target’s computer.
        </div>
        <div class="children">
            
        </div>
    </div>


            
                


    
    <div class="comment hasChildren">
        <p><a href="https://news.ycombinator.com/user?id=JKCalhoun" target="_blank">JKCalhoun</a>   <span class="timeago" data-date="2021-08-18 14:39:51 &#43;0000 UTC">2021-08-18 14:39:51 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            &gt; Just insert a known CSAM image on target&#x27;s device.<p>Or maybe thirty. You have to surpass the threshold.<p>Also, if Twitter, Google, Microsoft are already deploying CSAM scanning in their services .... why are we not hearing about all the &quot;swatting&quot;?
        </div>
        <div class="children">
            
                


    
    <div class="comment ">
        <p><a href="https://news.ycombinator.com/user?id=faeriechangling" target="_blank">faeriechangling</a>   <span class="timeago" data-date="2021-08-19 01:34:48 &#43;0000 UTC">2021-08-19 01:34:48 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            Their implementations are not on-device and thus it&#x27;s actually significantly more difficult to reverse engineer.  Apples unique implementation of on-device scanning is much easier to reverse engineer and thus exploit.<p>Now apple is in this crappy situation where they can&#x27;t claim their software is secure because it&#x27;s open source and auditable, but they also can&#x27;t claim it&#x27;s secure because it&#x27;s closed source and they fixed the problems in some later version because this entire debacle has likely destroyed all faith in their competence.  If apple is in the position of having to boast &quot;Trust us bro, your iPhone won&#x27;t be exploited to get you SWATTED over CSAM anymore, we patched it&quot; the big question is why is apple voluntarily adding something to their devices where the failure mode is violent imprisonment and severe loss of reputation when they are not completely competent?<p>This entire debacle reminds me of this video: <a href="https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=tVq1wgIN62E" rel="nofollow">https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=tVq1wgIN62E</a>
        </div>
        <div class="children">
            
        </div>
    </div>


            
                


    
    <div class="comment hasChildren">
        <p><a href="https://news.ycombinator.com/user?id=romwell" target="_blank">romwell</a>   <span class="timeago" data-date="2021-08-18 18:25:31 &#43;0000 UTC">2021-08-18 18:25:31 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            &gt;Also, if Twitter, Google, Microsoft are already deploying CSAM scanning in their services .... why are we not hearing about all the &quot;swatting&quot;?<p>&gt;their services<p>&gt;T H E I R     S E R V I C E S<p>Because it&#x27;s on their SERVICES, not on their user&#x27;s DEVICES, for one.<p>Also, regardless of swatting, that&#x27;s why we have an issue with Apple.
        </div>
        <div class="children">
            
                


    
    <div class="comment hasChildren">
        <p><a href="https://news.ycombinator.com/user?id=stephenr" target="_blank">stephenr</a>   <span class="timeago" data-date="2021-08-18 19:11:25 &#43;0000 UTC">2021-08-18 19:11:25 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            It only happens on Apple devices right before the content is uploaded to the service.<p>How is that a meaningful difference for the stated end goals, that can explain the lack of precedent.
        </div>
        <div class="children">
            
                


    
    <div class="comment ">
        <p><a href="https://news.ycombinator.com/user?id=croutonwagon" target="_blank">croutonwagon</a>   <span class="timeago" data-date="2021-08-18 21:26:25 &#43;0000 UTC">2021-08-18 21:26:25 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            I think this is where a disconnect is occurring.<p>In this specific case yes. That is what is supposed to happen.<p>But Apple also sets the standard that this is just the beginning, not the end.  They say as much on page 3 in bold, differentiated color ink<p><a href="https:&#x2F;&#x2F;www.apple.com&#x2F;child-safety&#x2F;pdf&#x2F;Expanded_Protections_for_Children_Technology_Summary.pdf" rel="nofollow">https:&#x2F;&#x2F;www.apple.com&#x2F;child-safety&#x2F;pdf&#x2F;Expanded_Protections_...</a><p>And there’s nothing to stop them from scanning all images on a device. Or scanning all content for keywords or whatever. iCloud being used as a qualifier is a red herring to what this change is capable of.<p>Maybe someone shooting guns is now unacceptable, kids have been kicked from schools for posting them on Facebook or having them in their rooms on zoom. What if it’s kids shooting guns? There are so many possibilities of how this could be misused, abused or even just an oopsie, sorry I upended your life to solve a problem that is so very rare.<p>Add to that their messaging has been muddy at best. And it incited a flame war. A big part of that is iCloud is not a single thing. It’s a service, it can sync snd hold iMessages, it can sync backups, or in my case We have shared iCloud albums that we use to share images with family. Others are free to upload and share.  In fact that’s our only use of iCloud other than find my. They say iCloud photos as if that’s just a single thing but it’s easy to extrapolate that to images in iMessages, backups etc.<p>And the non profit that hosts this database is not publicly accountable. They have public employees on their payroll but really they can put whatever they want in that database. They have no accountability or public disclosure requirements.<p>So even I, when their main page was like 3 articles was a bit perturbed and put off. I’m not going to ditch my iPhone, mainly because it’s work assigned but I have been keeping a keen eye on what’s happening, how it’s happening and will keep an eye out for their chnages they are promising. I’m also going to guess they won’t nearly be as high profile in the future.
        </div>
        <div class="children">
            
        </div>
    </div>


            
        </div>
    </div>


            
                


    
    <div class="comment ">
        <p><a href="https://news.ycombinator.com/user?id=cyanite" target="_blank">cyanite</a>   <span class="timeago" data-date="2021-08-18 19:58:15 &#43;0000 UTC">2021-08-18 19:58:15 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            &gt; Because it&#x27;s on their SERVICES, not on their user&#x27;s DEVICES, for one.<p>Effectively the same for Apple. It’s only when uploading the photo. Doing it on device means the server side gets less information.
        </div>
        <div class="children">
            
        </div>
    </div>


            
        </div>
    </div>


            
                


    
    <div class="comment hasChildren">
        <p><a href="https://news.ycombinator.com/user?id=short_sells_poo" target="_blank">short_sells_poo</a>   <span class="timeago" data-date="2021-08-18 14:51:49 &#43;0000 UTC">2021-08-18 14:51:49 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            Who cares that Bad Company XYZ already well known for not caring about customer privacy does it? Wouldn&#x27;t you want to push back against even more increasing surveillance? Apply was beating the drum of privacy while it was convenient, wouldn&#x27;t you want to hold their feet to the fire now that they seemed to do a U-turn?
        </div>
        <div class="children">
            
                


    
    <div class="comment hasChildren">
        <p><a href="https://news.ycombinator.com/user?id=sweetheart" target="_blank">sweetheart</a>   <span class="timeago" data-date="2021-08-18 14:58:57 &#43;0000 UTC">2021-08-18 14:58:57 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            Their point is that the attack vector being described isn’t new, as CSAM could already be weaponized against folks, and we never really ever hear if that happening. So the OP is simply saying that perhaps it’s not an issue we need to worry about. I happen to agree with them.
        </div>
        <div class="children">
            
                


    
    <div class="comment hasChildren">
        <p><a href="https://news.ycombinator.com/user?id=short_sells_poo" target="_blank">short_sells_poo</a>   <span class="timeago" data-date="2021-08-18 15:10:39 &#43;0000 UTC">2021-08-18 15:10:39 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            So in your mind, because so far we&#x27;ve seen no evidence that this has been abused, it&#x27;s nothing to worry about going forward? And that making an existing situation even more widespread is also completely OK?
        </div>
        <div class="children">
            
                


    
    <div class="comment ">
        <p><a href="https://news.ycombinator.com/user?id=sweetheart" target="_blank">sweetheart</a>   <span class="timeago" data-date="2021-08-18 18:22:07 &#43;0000 UTC">2021-08-18 18:22:07 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            &gt; So in your mind, because so far we&#x27;ve seen no evidence that this has been abused, it&#x27;s nothing to worry about going forward?<p>Yeah, basically. It doesn&#x27;t seem like people actually use CSAM to screw over innocent folks, so I don&#x27;t think we need to worry about it. What Apple is doing doesn&#x27;t really make that any easier, so it&#x27;s either already a problem, or not a problem.<p>&gt; And that making an existing situation even more widespread is also completely OK?<p>I don&#x27;t know if I&#x27;d say any of this is &quot;completely OK&quot;, as I don&#x27;t think I&#x27;ve fully formed my opinion on this whole Apple CSAM debate, but I at least agree with OP that I don&#x27;t think we need to suddenly worry about people weaponizing CSAM all of a sudden when it&#x27;s been an option for years now with no real stories of anyone actually being victimized.
        </div>
        <div class="children">
            
        </div>
    </div>


            
                


    
    <div class="comment ">
        <p><a href="https://news.ycombinator.com/user?id=colejohnson66" target="_blank">colejohnson66</a>   <span class="timeago" data-date="2021-08-18 15:57:29 &#43;0000 UTC">2021-08-18 15:57:29 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            It’s that the situation can’t be a “slippery slope” if there’s no evidence of there being one prior
        </div>
        <div class="children">
            
        </div>
    </div>


            
        </div>
    </div>


            
        </div>
    </div>


            
        </div>
    </div>


            
        </div>
    </div>


            
                


    
    <div class="comment ">
        <p><a href="https://news.ycombinator.com/user?id=hannasanarion" target="_blank">hannasanarion</a>   <span class="timeago" data-date="2021-08-18 18:44:03 &#43;0000 UTC">2021-08-18 18:44:03 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            &gt; I presume this could be used against a rival political party to ruin their reputation - insert bunch of CSAM images on their devices.<p>Okay, and then what? You think people will just look at this cute picture of a dog and be like &quot;welp, the computer says it&#x27;s a photo of child abuse, so we&#x27;re taking you to jail anyway&quot;?
        </div>
        <div class="children">
            
        </div>
    </div>


            
                


    
    <div class="comment ">
        <p><a href="https://news.ycombinator.com/user?id=russdpale" target="_blank">russdpale</a>   <span class="timeago" data-date="2021-08-18 17:29:06 &#43;0000 UTC">2021-08-18 17:29:06 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            You don&#x27;t have to add a real picture, just add the hash of a benign, (new) cat picture to the db, then put the cat picture on the phone, then release a statement saying the person has popped up on your list. By the time the truth comes out the damage is done.
        </div>
        <div class="children">
            
        </div>
    </div>


            
                


    
    <div class="comment ">
        <p><a href="https://news.ycombinator.com/user?id=FabHK" target="_blank">FabHK</a>   <span class="timeago" data-date="2021-08-18 20:48:19 &#43;0000 UTC">2021-08-18 20:48:19 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            &gt; &quot;How can you use it for targeted attacks?&quot;
&gt; Just insert a known CSAM image on target&#x27;s device. Done.<p>Yes, but then the hash collision (topic of this article) is irrelevant.
        </div>
        <div class="children">
            
        </div>
    </div>


            
                


    
    <div class="comment hasChildren">
        <p><a href="https://news.ycombinator.com/user?id=sam0x17" target="_blank">sam0x17</a>   <span class="timeago" data-date="2021-08-18 19:48:07 &#43;0000 UTC">2021-08-18 19:48:07 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            &gt; Just insert a known CSAM image on target&#x27;s device. Done.<p>You don&#x27;t even need to go that far. You just need to generate 31 false positive images and send them to an innocent user.
        </div>
        <div class="children">
            
                


    
    <div class="comment ">
        <p><a href="https://news.ycombinator.com/user?id=cyanite" target="_blank">cyanite</a>   <span class="timeago" data-date="2021-08-18 19:55:58 &#43;0000 UTC">2021-08-18 19:55:58 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            But how will you do that without access to the non-blinded hash table? Then you need access to that first.<p>Also, “sending” them to a user isn’t enough; they need to be stored in the photo library, and iCloud Photo Library needs to be enabled.
        </div>
        <div class="children">
            
        </div>
    </div>


            
        </div>
    </div>


            
                


    
    <div class="comment hasChildren">
        <p><a href="https://news.ycombinator.com/user?id=cyanite" target="_blank">cyanite</a>   <span class="timeago" data-date="2021-08-18 19:54:28 &#43;0000 UTC">2021-08-18 19:54:28 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            &gt; Just insert a known CSAM image on target&#x27;s device. Done.<p>What do you mean “just”? That’s not usually very simple. It needs to go into the actual photo library. Also, you need like 30 of them inserted.<p>&gt; I presume this could be used against a rival political party<p>Yes, but it’s not much different from now, since most cloud photo providers scan for this cloud-side. So that’s more an argument against scanning all together.
        </div>
        <div class="children">
            
                


    
    <div class="comment ">
        <p><a href="https://news.ycombinator.com/user?id=lowkey_" target="_blank">lowkey_</a>   <span class="timeago" data-date="2021-08-18 20:23:13 &#43;0000 UTC">2021-08-18 20:23:13 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            WhatsApp for example automatically stores images you receive in your Photos library, so that removes a step, and those will thus be automatically uploaded to iCloud.<p>The one failsafe would be Apple&#x27;s manual reviewers, but we haven&#x27;t heard much about that process yet.
        </div>
        <div class="children">
            
        </div>
    </div>


            
                


    
    <div class="comment hasChildren">
        <p><a href="https://news.ycombinator.com/user?id=tick_tock_tick" target="_blank">tick_tock_tick</a>   <span class="timeago" data-date="2021-08-18 20:18:33 &#43;0000 UTC">2021-08-18 20:18:33 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            &gt; It needs to go into the actual photo library.<p>iMessage photos received are automatically synced so no. Finding 30 photos take zero time at all on Tor. Hell finding a .onion site that doesn&#x27;t have CP randomly spammed is harder.....
        </div>
        <div class="children">
            
                


    
    <div class="comment ">
        <p><a href="https://news.ycombinator.com/user?id=bsql" target="_blank">bsql</a>   <span class="timeago" data-date="2021-08-18 21:24:28 &#43;0000 UTC">2021-08-18 21:24:28 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            iMessage photos do not automatically add photos to your photo library.  Yes they’re synced between devices but afaik Apple isn’t deploying this hashing technology on iMessages between devices.  Only for iCloud photos in the photo library.
        </div>
        <div class="children">
            
        </div>
    </div>


            
        </div>
    </div>


            
        </div>
    </div>


            
        </div>
    </div>


            
                


    
    <div class="comment hasChildren">
        <p><a href="https://news.ycombinator.com/user?id=y7" target="_blank">y7</a>   <span class="timeago" data-date="2021-08-18 10:53:09 &#43;0000 UTC">2021-08-18 10:53:09 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            Cross-posting from another thread [1]:<p>1. Obtain known CSAM that is likely in the database and generate its NeuralHash.<p>2. Use an image-scaling attack [2] together with adversarial collisions to generate a perturbed image such that its NeuralHash is in the database and its image derivative looks like CSAM.<p>A difference compared to server-side CSAM detection could be that they verify the entire image, and not just the image derivative, before notifying the authorities.<p>[1] <a href="https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=28218922" rel="nofollow">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=28218922</a><p>[2] <a href="https:&#x2F;&#x2F;bdtechtalks.com&#x2F;2020&#x2F;08&#x2F;03&#x2F;machine-learning-adversarial-image-scaling&#x2F;" rel="nofollow">https:&#x2F;&#x2F;bdtechtalks.com&#x2F;2020&#x2F;08&#x2F;03&#x2F;machine-learning-adversar...</a>
        </div>
        <div class="children">
            
                


    
    <div class="comment hasChildren">
        <p><a href="https://news.ycombinator.com/user?id=FabHK" target="_blank">FabHK</a>   <span class="timeago" data-date="2021-08-18 11:11:56 &#43;0000 UTC">2021-08-18 11:11:56 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            Right. So, sending actual CSAM would also work as an attack, but would be detected by the victim and could be corrected (delete images).<p>But a conceivable novel avenue of attack would be to find an image that:<p>1. Does not look like CSAM to the innocent victim in the original<p>2. Does match known CSAM by NeuralHash<p>3. Does look like CSAM in the &quot;visual derivative&quot; reviewed by Apple, as you highlight.
        </div>
        <div class="children">
            
                


    
    <div class="comment hasChildren">
        <p><a href="https://news.ycombinator.com/user?id=avianlyric" target="_blank">avianlyric</a>   <span class="timeago" data-date="2021-08-18 11:46:02 &#43;0000 UTC">2021-08-18 11:46:02 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            Reading the imagine scaling attack article, it’s looks like it’s pretty easy to manufacture an image that:<p>1. Looks like an innocuous image, indeed even an image the victim is expecting to receive.<p>2. Downscales in such a way to produce a CSAM match.<p>3. Downscales for the derivative image to create actual CSAM for the review process.<p>Which is a pretty scary attack vector.
        </div>
        <div class="children">
            
                


    
    <div class="comment ">
        <p><a href="https://news.ycombinator.com/user?id=zepto" target="_blank">zepto</a>   <span class="timeago" data-date="2021-08-18 15:11:30 &#43;0000 UTC">2021-08-18 15:11:30 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            Where does it say anything that indicates #1 and #3 are both possible?
        </div>
        <div class="children">
            
        </div>
    </div>


            
                


    
    <div class="comment hasChildren">
        <p><a href="https://news.ycombinator.com/user?id=FabHK" target="_blank">FabHK</a>   <span class="timeago" data-date="2021-08-18 11:53:28 &#43;0000 UTC">2021-08-18 11:53:28 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            Depends very much on the process Apple uses to make the &quot;visual derivative&quot;, though. Also, defence by producing the original innocuous image (and showing that it triggers both parts of Apple&#x27;s process, NeuralHash and human review of the visual derivative) should be possible, though a lot of damage might&#x27;ve been done by then.
        </div>
        <div class="children">
            
                


    
    <div class="comment hasChildren">
        <p><a href="https://news.ycombinator.com/user?id=avianlyric" target="_blank">avianlyric</a>   <span class="timeago" data-date="2021-08-18 11:59:35 &#43;0000 UTC">2021-08-18 11:59:35 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            &gt; Also, defence by producing the original innocuous image<p>At this point you’re already inside the guts of the justice system, and have been accused of distributing CSAM. Indeed depending on how diligent the prosecutor is, you might need to wait till trial before you can defend yourself.<p>At that point you’re life as you know is already fucked. The only thing proving your innocence (and the need to do so is itself a complete miscarriage of justice) will save you from is a prison sentence.
        </div>
        <div class="children">
            
                


    
    <div class="comment ">
        <p><a href="https://news.ycombinator.com/user?id=ta988" target="_blank">ta988</a>   <span class="timeago" data-date="2021-08-18 13:04:25 &#43;0000 UTC">2021-08-18 13:04:25 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            And now you will be accused of trying to hide illegal material in innocuous images.
        </div>
        <div class="children">
            
        </div>
    </div>


            
                


    
    <div class="comment hasChildren">
        <p><a href="https://news.ycombinator.com/user?id=zepto" target="_blank">zepto</a>   <span class="timeago" data-date="2021-08-18 15:13:39 &#43;0000 UTC">2021-08-18 15:13:39 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            This isn’t true at all.<p>If the creation of fakes is as easy as claimed, Neuralhash evidence alone will become inadmissible.<p>There are <i>plenty</i> of lawyers and money waiting to establish this.
        </div>
        <div class="children">
            
                


    
    <div class="comment hasChildren">
        <p><a href="https://news.ycombinator.com/user?id=IncRnd" target="_blank">IncRnd</a>   <span class="timeago" data-date="2021-08-19 02:11:57 &#43;0000 UTC">2021-08-19 02:11:57 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            &gt; This isn’t true at all.<p>&gt; If the creation of fakes is as easy as claimed, Neuralhash evidence alone will become inadmissible.<p>Okay.  <a href="https:&#x2F;&#x2F;github.com&#x2F;anishathalye&#x2F;neural-hash-collider" rel="nofollow">https:&#x2F;&#x2F;github.com&#x2F;anishathalye&#x2F;neural-hash-collider</a>
        </div>
        <div class="children">
            
                


    
    <div class="comment hasChildren">
        <p><a href="https://news.ycombinator.com/user?id=spurgu" target="_blank">spurgu</a>   <span class="timeago" data-date="2021-08-19 03:14:09 &#43;0000 UTC">2021-08-19 03:14:09 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            Uh? So his if statement is true?
        </div>
        <div class="children">
            
                


    
    <div class="comment ">
        <p><a href="https://news.ycombinator.com/user?id=IncRnd" target="_blank">IncRnd</a>   <span class="timeago" data-date="2021-08-19 03:21:16 &#43;0000 UTC">2021-08-19 03:21:16 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            Please read what is written right before that...  You are taking something out of context.
        </div>
        <div class="children">
            
        </div>
    </div>


            
        </div>
    </div>


            
        </div>
    </div>


            
        </div>
    </div>


            
        </div>
    </div>


            
        </div>
    </div>


            
        </div>
    </div>


            
                


    
    <div class="comment hasChildren">
        <p><a href="https://news.ycombinator.com/user?id=GeckoEidechse" target="_blank">GeckoEidechse</a>   <span class="timeago" data-date="2021-08-18 14:05:13 &#43;0000 UTC">2021-08-18 14:05:13 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            &gt; So, sending actual CSAM would also work as an attack, but would be detected by the victim and could be corrected (delete images).<p>What if they are placed on the iDevice covertly? Say you want to remove politician X from office. If you got the money or influence you could use a tool like Pegasus (or whatever else there is out there that we don&#x27;t know of) to place actual CSAM images on their iDevice. Preferably with an older timestamp so that it doesn&#x27;t appear as the newest image on their timeline. iCloud notices unsynced images and syncs them while performing the CSAM check, it comes back positive  with human review (cause it was actual CSAM) and voilà X got the FBI knocking on their door. Even if X can somehow later proof innocence by this time they&#x27;ll likely have been removed from office over the allegations.<p>Thinking about it now it&#x27;s probably even easier:  
Messaging apps like WhatsApp allow you to save received images directly to camera roll which then auto-syncs with iCloud (if enabled). So you can just blast 30+ (or whatever the requirement was) CSAM images to your victim while they are asleep and by the time they check their phone in the morning the images will already have been processed and an investigation started.
        </div>
        <div class="children">
            
                


    
    <div class="comment ">
        <p><a href="https://news.ycombinator.com/user?id=zepto" target="_blank">zepto</a>   <span class="timeago" data-date="2021-08-18 15:14:36 &#43;0000 UTC">2021-08-18 15:14:36 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            If you are placing images covertly, you can just use real CSAM or other compromat.
        </div>
        <div class="children">
            
        </div>
    </div>


            
        </div>
    </div>


            
                


    
    <div class="comment ">
        <p><a href="https://news.ycombinator.com/user?id=Sebb767" target="_blank">Sebb767</a>   <span class="timeago" data-date="2021-08-18 12:28:35 &#43;0000 UTC">2021-08-18 12:28:35 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            &gt; but would be detected by the victim and could be corrected (delete images).<p>I doubt deleting them (assuming the victim sees them) works once the image has been scanned. And, given that this probably comes with a sufficient smear campaign, deleting them will be portraye. as evidence of guilt
        </div>
        <div class="children">
            
        </div>
    </div>


            
        </div>
    </div>


            
                


    
    <div class="comment hasChildren">
        <p><a href="https://news.ycombinator.com/user?id=bostonsre" target="_blank">bostonsre</a>   <span class="timeago" data-date="2021-08-18 13:53:52 &#43;0000 UTC">2021-08-18 13:53:52 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            Why would someone do that? Why not just send the original if both are flagged as the original?
        </div>
        <div class="children">
            
                


    
    <div class="comment ">
        <p><a href="https://news.ycombinator.com/user?id=notriddle" target="_blank">notriddle</a>   <span class="timeago" data-date="2021-08-18 14:00:21 &#43;0000 UTC">2021-08-18 14:00:21 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            The victim needs to store the image in their iCloud, so it needs to not look like CSAM to them.
        </div>
        <div class="children">
            
        </div>
    </div>


            
                


    
    <div class="comment hasChildren">
        <p><a href="https://news.ycombinator.com/user?id=Bjartr" target="_blank">Bjartr</a>   <span class="timeago" data-date="2021-08-18 14:26:31 &#43;0000 UTC">2021-08-18 14:26:31 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            Because having actual CSAM images is illegal.
        </div>
        <div class="children">
            
                


    
    <div class="comment ">
        <p><a href="https://news.ycombinator.com/user?id=mmcwilliams" target="_blank">mmcwilliams</a>   <span class="timeago" data-date="2021-08-18 15:44:05 &#43;0000 UTC">2021-08-18 15:44:05 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            Doesn’t that make step 1 more dangerous for the attacker than the intended victim? And following this through to its logical conclusion; the intended victim would have images that upon manual review by law enforcement would be found to be not CSAM.
        </div>
        <div class="children">
            
        </div>
    </div>


            
        </div>
    </div>


            
        </div>
    </div>


            
                


    
    <div class="comment ">
        <p><a href="https://news.ycombinator.com/user?id=" target="_blank"></a>   <span class="timeago" data-date="2021-08-18 11:09:51 &#43;0000 UTC">2021-08-18 11:09:51 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            
        </div>
        <div class="children">
            
        </div>
    </div>


            
                


    
    <div class="comment ">
        <p><a href="https://news.ycombinator.com/user?id=" target="_blank"></a>   <span class="timeago" data-date="2021-08-18 11:01:30 &#43;0000 UTC">2021-08-18 11:01:30 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            
        </div>
        <div class="children">
            
        </div>
    </div>


            
        </div>
    </div>


            
                


    
    <div class="comment hasChildren">
        <p><a href="https://news.ycombinator.com/user?id=st_goliath" target="_blank">st_goliath</a>   <span class="timeago" data-date="2021-08-18 13:02:29 &#43;0000 UTC">2021-08-18 13:02:29 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            &gt; 7. Apple reviewer....<p>This part IMO makes Apple itself the most likely &quot;target&quot;, but for a different kind of attack.<p>Just wait until someone who wasn&#x27;t supposed to, somewhere, somehow gets their hands on some of the actual hashes (IMO bound to happen <i>eventually</i>). Also remember that with Apple, we now have an oracle that can tell us. And with all the media attention around the issue, this might further incentivize people to try.<p>From that I can picture a chain of events something like this:<p>1. Somebody writes a script that generates pre-image collisions like in the post, but for actual hashes Apple uses.<p>2. The script ends up on the Internet. News reporting picks it up and it spreads around a little. This also means trolls get their hands on it.<p>3. Tons of colliding image are created by people all over the planet and sent around to even more people. Not for targeted attacks, but simply for the lulz.<p>4. Newer scripts show up eventually, e.g. for perturbing existing images or similar stunts. More news reporting follows, accelerating the effect and possibly also spreading perturbed images around themselves. Perturbed images (cat pictures, animated gifs, etc...) get uploaded to places like 9gag, reaching large audiences.<p>5. Repeat steps 1-4 until the Internet and the news grow bored with it.<p>During that entire process, potentially each of those images that ends up on an iDevice will have to be manually reviewed...
        </div>
        <div class="children">
            
                


    
    <div class="comment hasChildren">
        <p><a href="https://news.ycombinator.com/user?id=Grustaf" target="_blank">Grustaf</a>   <span class="timeago" data-date="2021-08-18 13:15:51 &#43;0000 UTC">2021-08-18 13:15:51 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            Do you think Apple might perhaps halt the system if the script get wide publication?
        </div>
        <div class="children">
            
                


    
    <div class="comment hasChildren">
        <p><a href="https://news.ycombinator.com/user?id=dylan604" target="_blank">dylan604</a>   <span class="timeago" data-date="2021-08-18 13:32:27 &#43;0000 UTC">2021-08-18 13:32:27 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            I&#x27;ve only seen Apple admit defeat once, and that was regarding the trashcan MacPro. Otherwise, it&#x27;s &quot;you&#x27;re holding it wrong&quot; type of victim blaming as they quietly revise the issue on the next version.<p>Can anyone else think of times where Apple has admitted to something bad on their end and then reversed&#x2F;walked away from whatever it was?
        </div>
        <div class="children">
            
                


    
    <div class="comment hasChildren">
        <p><a href="https://news.ycombinator.com/user?id=jtmarl1n" target="_blank">jtmarl1n</a>   <span class="timeago" data-date="2021-08-18 14:50:34 &#43;0000 UTC">2021-08-18 14:50:34 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            The Apple AirPower mat comes to mind although there are rumors they haven&#x27;t abandoned the effort completely. Butterfly keyboard seems to finally be acknowledged as a bad idea and took several years to get there.<p>The next Macbook refresh will be interesting as there are rumors they are bring back several I&#x2F;O ports that were removed when switching to all USB-C.<p>I agree with your overall point, just some things that came to mind when reading your question.
        </div>
        <div class="children">
            
                


    
    <div class="comment ">
        <p><a href="https://news.ycombinator.com/user?id=dylan604" target="_blank">dylan604</a>   <span class="timeago" data-date="2021-08-18 15:46:41 &#43;0000 UTC">2021-08-18 15:46:41 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            ah yes, the butterfly keyboard. i must have blocked that from my mind after the horror it was. although, they didn&#x27;t admit anything on that one. that was just another &quot;you&#x27;re holding it wrong&quot; silent revision that was then touted as a new feature (rather than oops we fucked up).<p>The trashcan MacPro is still the only mea culpa I am aware of them actually owning the mistake.<p>The Airpower whatever was never really released as a product though, so it is a strange category. New question, is the Airpower whatever the only product offically announced on the big stage to never be released?
        </div>
        <div class="children">
            
        </div>
    </div>


            
        </div>
    </div>


            
                


    
    <div class="comment hasChildren">
        <p><a href="https://news.ycombinator.com/user?id=Grustaf" target="_blank">Grustaf</a>   <span class="timeago" data-date="2021-08-18 13:34:39 &#43;0000 UTC">2021-08-18 13:34:39 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            Do you really care what they &quot;admit&quot;? I thought you were worried about innocent people being framed. Obviously if a way to frame people gets widespread, Apple will stop it. They don&#x27;t want that publicity.
        </div>
        <div class="children">
            
                


    
    <div class="comment hasChildren">
        <p><a href="https://news.ycombinator.com/user?id=dylan604" target="_blank">dylan604</a>   <span class="timeago" data-date="2021-08-18 13:58:17 &#43;0000 UTC">2021-08-18 13:58:17 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            You clearly have me confused with someone else, as I never mentioned anything about innocent people being framed.<p>With Apple, nothing is &quot;obvious&quot;.
        </div>
        <div class="children">
            
                


    
    <div class="comment ">
        <p><a href="https://news.ycombinator.com/user?id=Grustaf" target="_blank">Grustaf</a>   <span class="timeago" data-date="2021-08-18 14:06:22 &#43;0000 UTC">2021-08-18 14:06:22 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            The comment above that I responded to seemed to talk about that. But in any case, I for one don&#x27;t care what Apple admits.<p>But I am certain they will not want all the bad publicity that would come if the system was widely abused, if you worry about that. That much is actually &quot;obvious&quot;, they are not stupid.
        </div>
        <div class="children">
            
        </div>
    </div>


            
        </div>
    </div>


            
        </div>
    </div>


            
        </div>
    </div>


            
        </div>
    </div>


            
        </div>
    </div>


            
                


    
    <div class="comment hasChildren">
        <p><a href="https://news.ycombinator.com/user?id=the8472" target="_blank">the8472</a>   <span class="timeago" data-date="2021-08-18 10:25:16 &#43;0000 UTC">2021-08-18 10:25:16 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            &gt; 7. Apple reviewer confuses a featureless blob of gray with CSAM material, several times<p>A better collision won&#x27;t be a grey blob, it&#x27;ll take some photoshopped and downscaled picture of a kid and massage the least significant bits until it is a collision.<p><a href="https:&#x2F;&#x2F;openai.com&#x2F;blog&#x2F;adversarial-example-research&#x2F;" rel="nofollow">https:&#x2F;&#x2F;openai.com&#x2F;blog&#x2F;adversarial-example-research&#x2F;</a>
        </div>
        <div class="children">
            
                


    
    <div class="comment hasChildren">
        <p><a href="https://news.ycombinator.com/user?id=sitharus" target="_blank">sitharus</a>   <span class="timeago" data-date="2021-08-18 10:42:55 &#43;0000 UTC">2021-08-18 10:42:55 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            So the person would have to accept and save an image that when looks enough like CSAM to confuse a reviewer…
        </div>
        <div class="children">
            
                


    
    <div class="comment hasChildren">
        <p><a href="https://news.ycombinator.com/user?id=formerly_proven" target="_blank">formerly_proven</a>   <span class="timeago" data-date="2021-08-18 10:59:04 &#43;0000 UTC">2021-08-18 10:59:04 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            Yes, the diligent review performed by the lowest-bidding subcontractor is an excellent defense against career-ending criminal accusations. Nothing can go wrong, this is fine.
        </div>
        <div class="children">
            
                


    
    <div class="comment hasChildren">
        <p><a href="https://news.ycombinator.com/user?id=saynay" target="_blank">saynay</a>   <span class="timeago" data-date="2021-08-18 11:23:05 &#43;0000 UTC">2021-08-18 11:23:05 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            I would think there is way easier ways to frame someone with CSAM then this. Like dump a thumbdrive of the stuff on them and report them to the police.
        </div>
        <div class="children">
            
                


    
    <div class="comment ">
        <p><a href="https://news.ycombinator.com/user?id=Sebb767" target="_blank">Sebb767</a>   <span class="timeago" data-date="2021-08-18 12:32:03 &#43;0000 UTC">2021-08-18 12:32:03 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            The police will not investigate every hint and a thumbdrive still has some plausible deniability. Evidence on your phone looks far worse and, thanks to this new process, law enforcement will receive actual evidence instead of just a hint.
        </div>
        <div class="children">
            
        </div>
    </div>


            
        </div>
    </div>


            
        </div>
    </div>


            
                


    
    <div class="comment hasChildren">
        <p><a href="https://news.ycombinator.com/user?id=hdjjhhvvhga" target="_blank">hdjjhhvvhga</a>   <span class="timeago" data-date="2021-08-18 11:07:18 &#43;0000 UTC">2021-08-18 11:07:18 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            My WhatsApp automatically saves all images to my photo roll. It has to be explicitly turned off. When the default is on, it&#x27;s enough that the image is received and the victim has CP on their phone. After the initial shock they delete it, but the image has already been sent to Apple, where a reviewer marked it as CP. Since the user already gave them their full address data in order to be able to use the app store, Appla can automatically send a report to the police.
        </div>
        <div class="children">
            
                


    
    <div class="comment hasChildren">
        <p><a href="https://news.ycombinator.com/user?id=tpush" target="_blank">tpush</a>   <span class="timeago" data-date="2021-08-18 13:39:14 &#43;0000 UTC">2021-08-18 13:39:14 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            &gt; [...] Appla can automatically send a report to the police.<p>Just to clarify, Apple doesn&#x27;t report anyone to the police. They report to NCMEC, who presumably contacts law enforcement.
        </div>
        <div class="children">
            
                


    
    <div class="comment ">
        <p><a href="https://news.ycombinator.com/user?id=dannyw" target="_blank">dannyw</a>   <span class="timeago" data-date="2021-08-18 17:13:49 &#43;0000 UTC">2021-08-18 17:13:49 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            FBI agents work for NCMEC. NCMEC is created by legislation. They ARE law enforcement, disguised as a non-profit.
        </div>
        <div class="children">
            
        </div>
    </div>


            
        </div>
    </div>


            
                


    
    <div class="comment hasChildren">
        <p><a href="https://news.ycombinator.com/user?id=zimpenfish" target="_blank">zimpenfish</a>   <span class="timeago" data-date="2021-08-18 11:25:04 &#43;0000 UTC">2021-08-18 11:25:04 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            &gt; but the image has already been sent to Apple, where a reviewer marked it as CP<p>No, the images are only decryptable after a threshold (which appears to be about 30) is breached.  If you&#x27;ve received 30 pieces of CSAM from WhatsApp contacts without blocking them and&#x2F;or stopping WhatsApp from automatically saving to iCloud, I gotta say, it&#x27;s on you at that point.
        </div>
        <div class="children">
            
                


    
    <div class="comment hasChildren">
        <p><a href="https://news.ycombinator.com/user?id=basilgohar" target="_blank">basilgohar</a>   <span class="timeago" data-date="2021-08-18 11:55:52 &#43;0000 UTC">2021-08-18 11:55:52 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            Just a side point, a single WhatsApp message can contain up to 30 images. 30 is the literal max of a single message. So ONE MESSAGE could theoretically contain enough images to trip this threshold.
        </div>
        <div class="children">
            
                


    
    <div class="comment ">
        <p><a href="https://news.ycombinator.com/user?id=zimpenfish" target="_blank">zimpenfish</a>   <span class="timeago" data-date="2021-08-18 12:05:11 &#43;0000 UTC">2021-08-18 12:05:11 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            &gt; a single WhatsApp message can contain up to 30 images<p>A fair point, yes, and somewhat scuppering towards my argument.
        </div>
        <div class="children">
            
        </div>
    </div>


            
        </div>
    </div>


            
                


    
    <div class="comment ">
        <p><a href="https://news.ycombinator.com/user?id=avianlyric" target="_blank">avianlyric</a>   <span class="timeago" data-date="2021-08-18 11:55:09 &#43;0000 UTC">2021-08-18 11:55:09 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            You’re aware that people sleep at night, and phones for the most part don’t, right?
        </div>
        <div class="children">
            
        </div>
    </div>


            
                


    
    <div class="comment ">
        <p><a href="https://news.ycombinator.com/user?id=BrianOnHN" target="_blank">BrianOnHN</a>   <span class="timeago" data-date="2021-08-18 11:43:51 &#43;0000 UTC">2021-08-18 11:43:51 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            Victim blaming because of failure to meet some seemingly arbitrary limit, ok.
        </div>
        <div class="children">
            
        </div>
    </div>


            
        </div>
    </div>


            
        </div>
    </div>


            
                


    
    <div class="comment ">
        <p><a href="https://news.ycombinator.com/user?id=kleene_op" target="_blank">kleene_op</a>   <span class="timeago" data-date="2021-08-18 13:07:08 &#43;0000 UTC">2021-08-18 13:07:08 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            Not necessarily.<p>If you know the method used by Apple to scale down flagged images before they are sent for review, you can make it so the scaled down version of the image shows a different, potentially misleading one instead:<p><a href="https:&#x2F;&#x2F;thume.ca&#x2F;projects&#x2F;2012&#x2F;11&#x2F;14&#x2F;magic-png-files&#x2F;" rel="nofollow">https:&#x2F;&#x2F;thume.ca&#x2F;projects&#x2F;2012&#x2F;11&#x2F;14&#x2F;magic-png-files&#x2F;</a><p>At the end of the day:<p>- You can trick the user into saving an innocent looking image<p>- You can trick Apple NN hashing function with a purposely generated hash<p>- You can trick the reviewer with an explicit thumbnail<p>There is no limit to how devilish one can be.
        </div>
        <div class="children">
            
        </div>
    </div>


            
                


    
    <div class="comment ">
        <p><a href="https://news.ycombinator.com/user?id=avianlyric" target="_blank">avianlyric</a>   <span class="timeago" data-date="2021-08-18 11:54:19 &#43;0000 UTC">2021-08-18 11:54:19 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            The reviewer may not be looking at the original image. But rather the visual derivative created during the hashing process and sent as part of the safety voucher.<p>In this scenario you could create an image that looks like anything, but where it’s visual derivative is CSAM material.<p>Currently iCloud isn’t encrypted, so Apple could just look at the original image. But in future is iCloud becomes encrypted, then the reporting will be don’t entirely based on the visual derivative.<p>Although Apple could change this by include a unique crypto key for each uploaded images within their inner safety voucher, allowing them to decrypt images that match for the review process.
        </div>
        <div class="children">
            
        </div>
    </div>


            
                


    
    <div class="comment ">
        <p><a href="https://news.ycombinator.com/user?id=the8472" target="_blank">the8472</a>   <span class="timeago" data-date="2021-08-18 10:49:59 &#43;0000 UTC">2021-08-18 10:49:59 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            Depending on what algorithm apple uses to generate the &quot;sample&quot; that&#x27;s shown to the reviewer it may be possible to generate a large image that looks innocent unless downscaled with that specific algorithm and to a specific resolution
        </div>
        <div class="children">
            
        </div>
    </div>


            
                


    
    <div class="comment ">
        <p><a href="https://news.ycombinator.com/user?id=labcomputer" target="_blank">labcomputer</a>   <span class="timeago" data-date="2021-08-18 15:42:22 &#43;0000 UTC">2021-08-18 15:42:22 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            So here&#x27;s something I find interesting about this whole discussion:  Everyone seems to assume the reviewers are honest actors.<p>It occurs to me that compromising an already-hired reviewer (either through blackmail or bribery) or even just planting your own insider on the review team might not be that difficult.<p>In fact, if your threat model includes nation-state adversaries, it seems crazy <i>not</i> to consider compromised reviewers.  How hard would it really be for the CIA or NSA to get a few of their (under cover) people on the review team?
        </div>
        <div class="children">
            
        </div>
    </div>


            
                


    
    <div class="comment ">
        <p><a href="https://news.ycombinator.com/user?id=Grustaf" target="_blank">Grustaf</a>   <span class="timeago" data-date="2021-08-18 13:17:01 &#43;0000 UTC">2021-08-18 13:17:01 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            Not one image. Ten, or maybe 50, who knows what the threshold is.
        </div>
        <div class="children">
            
        </div>
    </div>


            
                


    
    <div class="comment hasChildren">
        <p><a href="https://news.ycombinator.com/user?id=spoonjim" target="_blank">spoonjim</a>   <span class="timeago" data-date="2021-08-18 20:22:31 &#43;0000 UTC">2021-08-18 20:22:31 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            I don&#x27;t see how a perfectly legal and normal explicit photograph of someone&#x27;s 20-year-old wife would be indistinguishable to an Apple reviewer from CSAM, especially since some people look much younger or much older than their chronological age. So first, there would be the horrendous breach of privacy for an Apple goon to be looking at this picture in the first place, which the person in the photograph never consented to, and second, could put the couple in legal hot water for absolutely no reason.
        </div>
        <div class="children">
            
                


    
    <div class="comment ">
        <p><a href="https://news.ycombinator.com/user?id=brokenmachine" target="_blank">brokenmachine</a>   <span class="timeago" data-date="2021-08-19 01:12:58 &#43;0000 UTC">2021-08-19 01:12:58 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            The personal photo is unlikely to match a photo in the CSAM database though, or at least that&#x27;s what is claimed by Apple with no way to verify if it&#x27;s true or not.
        </div>
        <div class="children">
            
        </div>
    </div>


            
        </div>
    </div>


            
                


    
    <div class="comment hasChildren">
        <p><a href="https://news.ycombinator.com/user?id=goohle" target="_blank">goohle</a>   <span class="timeago" data-date="2021-08-18 12:02:56 &#43;0000 UTC">2021-08-18 12:02:56 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            Like this one: <a href="https:&#x2F;&#x2F;flickr.com&#x2F;photos&#x2F;tonysanchez&#x2F;2526100122" rel="nofollow">https:&#x2F;&#x2F;flickr.com&#x2F;photos&#x2F;tonysanchez&#x2F;2526100122</a>
        </div>
        <div class="children">
            
                


    
    <div class="comment ">
        <p><a href="https://news.ycombinator.com/user?id=tubbs" target="_blank">tubbs</a>   <span class="timeago" data-date="2021-08-18 12:21:14 &#43;0000 UTC">2021-08-18 12:21:14 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            I would suggest not clicking this link on a work device.
        </div>
        <div class="children">
            
        </div>
    </div>


            
        </div>
    </div>


            
        </div>
    </div>


            
                


    
    <div class="comment hasChildren">
        <p><a href="https://news.ycombinator.com/user?id=FabHK" target="_blank">FabHK</a>   <span class="timeago" data-date="2021-08-18 10:29:28 &#43;0000 UTC">2021-08-18 10:29:28 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            Remains to be shown whether that is possible, though.
        </div>
        <div class="children">
            
                


    
    <div class="comment hasChildren">
        <p><a href="https://news.ycombinator.com/user?id=michaelt" target="_blank">michaelt</a>   <span class="timeago" data-date="2021-08-18 10:38:18 &#43;0000 UTC">2021-08-18 10:38:18 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            Just yesterday, here on HN there was an article [1] about adversarial attacks that could make road signs get misread by ML recognition systems<p>I&#x27;d be astonished if it wasn&#x27;t possible to do the same thing here.<p>[1] <a href="https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=28204077" rel="nofollow">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=28204077</a>
        </div>
        <div class="children">
            
                


    
    <div class="comment hasChildren">
        <p><a href="https://news.ycombinator.com/user?id=FabHK" target="_blank">FabHK</a>   <span class="timeago" data-date="2021-08-18 10:48:21 &#43;0000 UTC">2021-08-18 10:48:21 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            But the remarkable thing there (and with all other adversarial attacks I&#x27;ve seen) is that the ML classifier is fooled, while for us humans it is obvious that it is still the original image (if maybe slightly perturbed).<p>But in the case of Apple&#x27;s CSAM detection, the collision would first have to fool the victim into seeing an innocent picture and storing it (presumably, they would not accept and store actual CSAM [^]), then fool the NeuralHash into thinking it was CSAM (ok, maybe possible, though classifiers &lt;&gt; perceptual hash), then fool the human reviewer into also seeing CSAM (unlike the innocent victim).<p>[^] If the premise is that the &quot;innocent victim&quot; would accept CSAM, then you might as well just send CSAM as an unscrupulous attacker.
        </div>
        <div class="children">
            
                


    
    <div class="comment ">
        <p><a href="https://news.ycombinator.com/user?id=marcus_holmes" target="_blank">marcus_holmes</a>   <span class="timeago" data-date="2021-08-18 12:22:49 &#43;0000 UTC">2021-08-18 12:22:49 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            Hmm, not quite:<p>step 1 - As others have pointed out, there are plenty of ways of getting an image onto someone&#x27;s phone without their explicit permission. WhatsApp (and I believe Messenger) do this by default; if someone sends you an image, it goes onto your phone and gets uploaded to iCloud.<p>step 2 - TFA proves that hash collision works, and fooling perceptual algorithms is already a known thing. This whole automatic screening process is known to be vulnerable already.<p>step 3 - Humans are harder to fool, but tech giants are not great at scaling human intervention; their tendency is to only use humans for exceptions because humans are expensive and unreliable. This is going to be a lowest-cost-bidder developing-country thing where the screeners are targeted on screening X images per hour, for a value of X that allows very little diligence. And the consequences of a false positive are probably going to be minimal - the screeners will be monitored for individual positive&#x2F;negative rates, but that&#x27;s about it. We&#x27;ve seen how this plays out for YouTube copyright claims, Google account cancellations, App store delistings, etc.<p>People&#x27;s lives are going to be ruined because of this tech. I understand that children&#x27;s lives are already being ruined because of abuse, but I don&#x27;t see that this tech is going to reduce that problem. If anything it will increase it (because <i>new</i> pictures of child abuse won&#x27;t be on the hash database).
        </div>
        <div class="children">
            
        </div>
    </div>


            
                


    
    <div class="comment ">
        <p><a href="https://news.ycombinator.com/user?id=labcomputer" target="_blank">labcomputer</a>   <span class="timeago" data-date="2021-08-18 15:48:24 &#43;0000 UTC">2021-08-18 15:48:24 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            &gt; then fool the human reviewer into also seeing CSAM (unlike the innocent victim).<p>Or just blackmail and&#x2F;or bribe the reviewers.  Presumably you could add some sort of &#x27;watermark&#x27; that would be obvious to compromised reviewers.  &quot;There&#x27;s $1000 in it for you if you click &#x27;yes&#x27; any time you see this watermark.  Be a shame if something happened to your mum.&quot;
        </div>
        <div class="children">
            
        </div>
    </div>


            
                


    
    <div class="comment ">
        <p><a href="https://news.ycombinator.com/user?id=faeriechangling" target="_blank">faeriechangling</a>   <span class="timeago" data-date="2021-08-19 01:45:42 &#43;0000 UTC">2021-08-19 01:45:42 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            Yes but the reviewers are not going to be viewing the original image, they are going to be viewing a 100x100 greyscale.<p>&gt;If the premise is that the &quot;innocent victim&quot; would accept CSAM, then you might as well just send CSAM as an unscrupulous attacker.<p>This adds trojan horses embedded in .jpg files as an attack vector, which while maybe not overly practical, I could certainly imagine some malicious troll uploading &quot;CSAM&quot; to some pornsite.
        </div>
        <div class="children">
            
        </div>
    </div>


            
        </div>
    </div>


            
                


    
    <div class="comment ">
        <p><a href="https://news.ycombinator.com/user?id=UncleMeat" target="_blank">UncleMeat</a>   <span class="timeago" data-date="2021-08-18 11:55:28 &#43;0000 UTC">2021-08-18 11:55:28 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            NN classifiers work differently than perceptual hashes and the mechanism to do this sort of attack is entirely different, though they seem superficially similar.
        </div>
        <div class="children">
            
        </div>
    </div>


            
        </div>
    </div>


            
                


    
    <div class="comment ">
        <p><a href="https://news.ycombinator.com/user?id=nabakin" target="_blank">nabakin</a>   <span class="timeago" data-date="2021-08-18 10:38:10 &#43;0000 UTC">2021-08-18 10:38:10 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            Unfortunately, it is very likely to be possible. Adversarial ML is extremely effective. I won&#x27;t be surprised if this is achieved within the day, if not sooner tbh.
        </div>
        <div class="children">
            
        </div>
    </div>


            
                


    
    <div class="comment ">
        <p><a href="https://news.ycombinator.com/user?id=mannerheim" target="_blank">mannerheim</a>   <span class="timeago" data-date="2021-08-18 10:31:42 &#43;0000 UTC">2021-08-18 10:31:42 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            It&#x27;s been done for image classification.
        </div>
        <div class="children">
            
        </div>
    </div>


            
        </div>
    </div>


            
        </div>
    </div>


            
                


    
    <div class="comment ">
        <p><a href="https://news.ycombinator.com/user?id=thinkingemote" target="_blank">thinkingemote</a>   <span class="timeago" data-date="2021-08-18 10:35:19 &#43;0000 UTC">2021-08-18 10:35:19 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            the issue is step 6 - review and action<p>Every single tech company is getting rid of manual human review towards an AI based approach. Human-ops they call it - they dont want their employees to be doing this harmful work, plus computers are cheaper and better at<p>We hear about failures of inhuman ops all the time on HN. people being banned, falsely accused, cancelled, accounts locked, credit denied. All because the decisions which were once by humans are now made by machine. This will happen eventually here too.<p>It&#x27;s the very reason why they have the neuralhash model. To remove the human reviewer.
        </div>
        <div class="children">
            
        </div>
    </div>


            
                


    
    <div class="comment hasChildren">
        <p><a href="https://news.ycombinator.com/user?id=mannerheim" target="_blank">mannerheim</a>   <span class="timeago" data-date="2021-08-18 10:30:22 &#43;0000 UTC">2021-08-18 10:30:22 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            &gt; 7. Apple reviewer confuses a featureless blob of gray with CSAM material, several times<p>Just because the PoC used a meaningless blob doesn&#x27;t mean that collisions have to be those. Plenty of examples of adversarial attacks on image recognition perturb real images to get the network to misidentify them, but to a human eye the image is unchanged.
        </div>
        <div class="children">
            
                


    
    <div class="comment hasChildren">
        <p><a href="https://news.ycombinator.com/user?id=nyuszika7h" target="_blank">nyuszika7h</a>   <span class="timeago" data-date="2021-08-18 10:44:43 &#43;0000 UTC">2021-08-18 10:44:43 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            The whole point flew over your head. If it&#x27;s unchanged to the human eye then surely the human reviewer will see that it&#x27;s a false positive?
        </div>
        <div class="children">
            
                


    
    <div class="comment hasChildren">
        <p><a href="https://news.ycombinator.com/user?id=mannerheim" target="_blank">mannerheim</a>   <span class="timeago" data-date="2021-08-18 11:25:36 &#43;0000 UTC">2021-08-18 11:25:36 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            No, it&#x27;s important to point that out lest people think collisions can only be generated with contrived examples. I haven&#x27;t studied neural hashes in particular, but for CNNs it&#x27;s extremely trivial to come up with adversarial examples for arbitrary images.<p>Anyway, as for human reviewers, depends on what the image being perturbed is. Computer repair employees have called the police on people who&#x27;ve had pictures of their children in the bath. My understanding is that Apple does not have the source images, only NCMEC, so Apple&#x27;s employees wouldn&#x27;t necessarily see that such a case is a false positive. One would hope that when it gets sent to NCMEC, their employees would compare to the source image and see that is a false positive, though.
        </div>
        <div class="children">
            
                


    
    <div class="comment hasChildren">
        <p><a href="https://news.ycombinator.com/user?id=int_19h" target="_blank">int_19h</a>   <span class="timeago" data-date="2021-08-19 00:54:52 &#43;0000 UTC">2021-08-19 00:54:52 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            Which would still be a privacy violation, since an actual human is looking at a photo you haven&#x27;t consented to share with them.
        </div>
        <div class="children">
            
                


    
    <div class="comment ">
        <p><a href="https://news.ycombinator.com/user?id=brokenmachine" target="_blank">brokenmachine</a>   <span class="timeago" data-date="2021-08-19 01:20:12 &#43;0000 UTC">2021-08-19 01:20:12 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            That will be clearly laid out on page 1174 of Apples ToS that you had to click to be able to use your $1200 phone for anything but a paperweight.
        </div>
        <div class="children">
            
        </div>
    </div>


            
        </div>
    </div>


            
        </div>
    </div>


            
        </div>
    </div>


            
        </div>
    </div>


            
                


    
    <div class="comment ">
        <p><a href="https://news.ycombinator.com/user?id=jsdalton" target="_blank">jsdalton</a>   <span class="timeago" data-date="2021-08-18 10:33:35 &#43;0000 UTC">2021-08-18 10:33:35 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            For #4, I know for a fact that my wife’s WhatsApp automatically stores pictures you send her to her iCloud. So the grey blob would definitely be there unless she actively deleted it.
        </div>
        <div class="children">
            
        </div>
    </div>


            
                


    
    <div class="comment ">
        <p><a href="https://news.ycombinator.com/user?id=gambiting" target="_blank">gambiting</a>   <span class="timeago" data-date="2021-08-18 11:12:15 &#43;0000 UTC">2021-08-18 11:12:15 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            I don&#x27;t know why you&#x27;d even go through this trouble. At least few years ago finding actual CP on TOR was trivial, not sure if the situation has changed or not. If you&#x27;re going to blackmail someone, just send actual illegal data, not something that <i>might</i> trigger detection scanners.<p>&gt;&gt; What has changed wrt targeted attacks against innocent people?<p>Anecdote: every single iphone user I know has iCloud sync enabled by default. Every single Android user I know doesn&#x27;t have google photos sync enabled by default.
        </div>
        <div class="children">
            
        </div>
    </div>


            
                


    
    <div class="comment hasChildren">
        <p><a href="https://news.ycombinator.com/user?id=Nextgrid" target="_blank">Nextgrid</a>   <span class="timeago" data-date="2021-08-18 10:36:01 &#43;0000 UTC">2021-08-18 10:36:01 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            &gt; the NeuralHashes of which, unless I&#x27;m mistaken, are not available<p>Given the scanning is client-side wouldn&#x27;t the client need a list of those hashes to check against? If so it&#x27;s just a matter of time before those are extracted and used in these attacks.
        </div>
        <div class="children">
            
                


    
    <div class="comment ">
        <p><a href="https://news.ycombinator.com/user?id=brokenmachine" target="_blank">brokenmachine</a>   <span class="timeago" data-date="2021-08-19 01:22:16 &#43;0000 UTC">2021-08-19 01:22:16 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            I think there&#x27;s some crypto mumbo-jumbo to make it so you can&#x27;t know if an image matched or not.
        </div>
        <div class="children">
            
        </div>
    </div>


            
        </div>
    </div>


            
                


    
    <div class="comment ">
        <p><a href="https://news.ycombinator.com/user?id=visarga" target="_blank">visarga</a>   <span class="timeago" data-date="2021-08-18 20:01:21 &#43;0000 UTC">2021-08-18 20:01:21 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            How can we know that the CSAM database is not already poisoned with adversarial images that actually target other kinds of content for different purposes? It would look like CSAM to the naked eye, and nobody can tell the images have been doctored.<p>When reports come in the images would not match, so they need to intercept them before they are discarded by Apple, maybe by having a mole in the team. But it&#x27;s  so much easier than other ways to have an iOS platform scanner for any purpose. Just let them find the doctored images and add them to the database and recruit a person in the Apple team.
        </div>
        <div class="children">
            
        </div>
    </div>


            
                


    
    <div class="comment ">
        <p><a href="https://news.ycombinator.com/user?id=anonymousab" target="_blank">anonymousab</a>   <span class="timeago" data-date="2021-08-18 13:28:17 &#43;0000 UTC">2021-08-18 13:28:17 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            &gt; 7. Apple reviewer confuses a featureless blob of gray with CSAM material, several times<p>I find it hard to believe that anyone has faith in any purported manual review by a modern tech giant. Assume the worst and you&#x27;ll still probably not go far enough.
        </div>
        <div class="children">
            
        </div>
    </div>


            
                


    
    <div class="comment hasChildren">
        <p><a href="https://news.ycombinator.com/user?id=cm2187" target="_blank">cm2187</a>   <span class="timeago" data-date="2021-08-18 10:33:22 &#43;0000 UTC">2021-08-18 10:33:22 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            Don’t imessage and whatsapp automatically store all images received in the iphone’s photo library?
        </div>
        <div class="children">
            
                


    
    <div class="comment hasChildren">
        <p><a href="https://news.ycombinator.com/user?id=vermilingua" target="_blank">vermilingua</a>   <span class="timeago" data-date="2021-08-18 10:41:29 &#43;0000 UTC">2021-08-18 10:41:29 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            iMessage no, WA by default yes, but can be disabled.
        </div>
        <div class="children">
            
                


    
    <div class="comment hasChildren">
        <p><a href="https://news.ycombinator.com/user?id=varajelle" target="_blank">varajelle</a>   <span class="timeago" data-date="2021-08-18 11:07:24 &#43;0000 UTC">2021-08-18 11:07:24 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            So no need of hash collisions then. One can simply directly send the child porn images to that person via WhatsApp and send her to jail.
        </div>
        <div class="children">
            
                


    
    <div class="comment hasChildren">
        <p><a href="https://news.ycombinator.com/user?id=philote" target="_blank">philote</a>   <span class="timeago" data-date="2021-08-18 12:58:53 &#43;0000 UTC">2021-08-18 12:58:53 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            But then you&#x27;re searching and finding child porn images to send.
        </div>
        <div class="children">
            
                


    
    <div class="comment ">
        <p><a href="https://news.ycombinator.com/user?id=GeckoEidechse" target="_blank">GeckoEidechse</a>   <span class="timeago" data-date="2021-08-18 14:11:05 &#43;0000 UTC">2021-08-18 14:11:05 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            I&#x27;d be surprised if there won&#x27;t be a darknet service that does exactly that (send CSAM via $POPULAR_MESSENGER) the moment Apple activates scanning.
        </div>
        <div class="children">
            
        </div>
    </div>


            
        </div>
    </div>


            
        </div>
    </div>


            
        </div>
    </div>


            
        </div>
    </div>


            
                


    
    <div class="comment ">
        <p><a href="https://news.ycombinator.com/user?id=johnla" target="_blank">johnla</a>   <span class="timeago" data-date="2021-08-18 16:54:47 &#43;0000 UTC">2021-08-18 16:54:47 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            I don&#x27;t think this can be used to harm an innocent person. It can raise a red flag but it would be quickly unraised and perhaps an investigation into the source of the fakeout images because THAT person had to have had the real images in possession.<p>If anything, this gives weapons to people against the scanner as we can now bomb the system with false positives rendering it impossible to use. I don&#x27;t know enough about cryptography but I wonder if there is any ramifications of the hash being broken.
        </div>
        <div class="children">
            
        </div>
    </div>


            
                


    
    <div class="comment ">
        <p><a href="https://news.ycombinator.com/user?id=" target="_blank"></a>   <span class="timeago" data-date="2021-08-18 14:18:37 &#43;0000 UTC">2021-08-18 14:18:37 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            
        </div>
        <div class="children">
            
        </div>
    </div>


            
                


    
    <div class="comment ">
        <p><a href="https://news.ycombinator.com/user?id=beiller" target="_blank">beiller</a>   <span class="timeago" data-date="2021-08-18 19:29:49 &#43;0000 UTC">2021-08-18 19:29:49 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            Maybe they could install malware that makes all camera images taken using a technique like stenography to cause false positive matches for all the photos taken by the device. Maybe they could share one photo album where all the images are hash collisions.
        </div>
        <div class="children">
            
        </div>
    </div>


            
                


    
    <div class="comment ">
        <p><a href="https://news.ycombinator.com/user?id=cirrus3" target="_blank">cirrus3</a>   <span class="timeago" data-date="2021-08-18 23:52:33 &#43;0000 UTC">2021-08-18 23:52:33 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            Are you being serious? #7 is literally &quot;Apple reviewer confuses a featureless blob of gray with CSAM material, several times&quot;<p>30 times.<p>30 times a human confused a blob with CSAM?
        </div>
        <div class="children">
            
        </div>
    </div>


            
                


    
    <div class="comment ">
        <p><a href="https://news.ycombinator.com/user?id=Johnny555" target="_blank">Johnny555</a>   <span class="timeago" data-date="2021-08-18 15:31:42 &#43;0000 UTC">2021-08-18 15:31:42 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            <i>Actually, need to run step 1-3 at least 30 times</i><p>You can do steps 2-3 all in one step &quot;Hey Bob, here&#x27;s a zip file of those funny cat pictures I was telling you about. Some of the files got corrupted and are grayed out for some reason&quot;.
        </div>
        <div class="children">
            
        </div>
    </div>


            
                


    
    <div class="comment hasChildren">
        <p><a href="https://news.ycombinator.com/user?id=f3d46600-b66e" target="_blank">f3d46600-b66e</a>   <span class="timeago" data-date="2021-08-18 14:26:08 &#43;0000 UTC">2021-08-18 14:26:08 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            What makes CSAM database private?<p>It&#x27;s my understanding that many tech companies (Microsoft? Dropbox? Google? Apple? Other?) (and many people in those companies) have access to the CSAM database, which essentially makes it public.
        </div>
        <div class="children">
            
                


    
    <div class="comment ">
        <p><a href="https://news.ycombinator.com/user?id=cyanite" target="_blank">cyanite</a>   <span class="timeago" data-date="2021-08-18 20:09:50 &#43;0000 UTC">2021-08-18 20:09:50 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            Well, the actual hash table on the device is blinded so the device doesn’t know if an image is a match or not. The server doesn’t learn the actual hash either, unless the threshold of 30 images is reached.
        </div>
        <div class="children">
            
        </div>
    </div>


            
        </div>
    </div>


            
                


    
    <div class="comment hasChildren">
        <p><a href="https://news.ycombinator.com/user?id=spicybright" target="_blank">spicybright</a>   <span class="timeago" data-date="2021-08-18 10:27:46 &#43;0000 UTC">2021-08-18 10:27:46 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            If you&#x27;re in close physical contact with a person (like at a job) you just wait for them to put their phone down while unlocked, and do all this.
        </div>
        <div class="children">
            
                


    
    <div class="comment hasChildren">
        <p><a href="https://news.ycombinator.com/user?id=FabHK" target="_blank">FabHK</a>   <span class="timeago" data-date="2021-08-18 10:32:01 &#43;0000 UTC">2021-08-18 10:32:01 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            Then, with all due respect, the attacker could just download actual CSAM.<p>&gt; If your adversary is the Mossad, YOU’RE GONNA DIE AND THERE’S NOTHING THAT YOU CAN DO ABOUT IT. The Mossad is not intimidated by the fact that you employ <a href="https:&#x2F;&#x2F;" rel="nofollow">https:&#x2F;&#x2F;</a>. If the Mossad wants your data, they’re going to use a drone to replace your cellphone with a piece of uranium that’s shaped like a cellphone, and when you die of tumors filled with tumors, they’re going to hold a press conference and say “It wasn’t us” as they wear t-shirts that say “IT WAS DEFINITELY US,” and then they’re going to buy all of your stuff at your estate sale so that they can directly look at the photos of your vacation instead of reading your insipid emails about them.<p><a href="https:&#x2F;&#x2F;www.usenix.org&#x2F;system&#x2F;files&#x2F;1401_08-12_mickens.pdf" rel="nofollow">https:&#x2F;&#x2F;www.usenix.org&#x2F;system&#x2F;files&#x2F;1401_08-12_mickens.pdf</a>
        </div>
        <div class="children">
            
                


    
    <div class="comment hasChildren">
        <p><a href="https://news.ycombinator.com/user?id=mrits" target="_blank">mrits</a>   <span class="timeago" data-date="2021-08-18 15:14:28 &#43;0000 UTC">2021-08-18 15:14:28 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            &quot;Then, with all due respect, the attacker could just download actual CSAM.&quot;<p>If you didn&#x27;t have Apple scanning your drive trying to find a new way for you to go to prison then it wouldn&#x27;t be a problem.
        </div>
        <div class="children">
            
                


    
    <div class="comment ">
        <p><a href="https://news.ycombinator.com/user?id=FabHK" target="_blank">FabHK</a>   <span class="timeago" data-date="2021-08-18 20:41:09 &#43;0000 UTC">2021-08-18 20:41:09 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            Yes. But this whole discussion is about potential problems&#x2F;exploits with hash collisions (see title).
        </div>
        <div class="children">
            
        </div>
    </div>


            
                


    
    <div class="comment ">
        <p><a href="https://news.ycombinator.com/user?id=cyanite" target="_blank">cyanite</a>   <span class="timeago" data-date="2021-08-18 20:05:10 &#43;0000 UTC">2021-08-18 20:05:10 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            It’s misleading to say that they scan your “drive”. They scan pictures <i>as</i> they are uploaded to iCloud Photo Library.<p>Also, most other major cloud photo providers scan images server side, leading to the same effect (but with them accessing more data).
        </div>
        <div class="children">
            
        </div>
    </div>


            
        </div>
    </div>


            
                


    
    <div class="comment hasChildren">
        <p><a href="https://news.ycombinator.com/user?id=incrudible" target="_blank">incrudible</a>   <span class="timeago" data-date="2021-08-18 11:16:01 &#43;0000 UTC">2021-08-18 11:16:01 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            Also, this XKCD:<p><a href="https:&#x2F;&#x2F;xkcd.com&#x2F;538&#x2F;" rel="nofollow">https:&#x2F;&#x2F;xkcd.com&#x2F;538&#x2F;</a><p>People are getting nerd-sniped about hash collisions. It&#x27;s completely irrelevant.<p>The real-world vector is that an attacker sends CSAM through one of the channels that will trigger a scan. Through iMessage, this should be possible in an unsolicited fashion (correct me if I&#x27;m wrong). Otherwise, it&#x27;s possible through a hacked device. Of course there&#x27;s plausible deniability here, but like with swatting, it&#x27;s not a situation you want to be in.
        </div>
        <div class="children">
            
                


    
    <div class="comment ">
        <p><a href="https://news.ycombinator.com/user?id=UseStrict" target="_blank">UseStrict</a>   <span class="timeago" data-date="2021-08-18 13:51:13 &#43;0000 UTC">2021-08-18 13:51:13 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            Plausible deniability or not, it could have real impact if Apple decides to implement the policy of locking your account after tripping the threshold, which you then have to wait or fight to get unlocked. Or now you have police records against you for an investigation that lead nowhere. It&#x27;s not a zero impact game if I can spam a bunch of grey blobs to people and potentially have a chain of human failures that leads police knock down your door.
        </div>
        <div class="children">
            
        </div>
    </div>


            
                


    
    <div class="comment ">
        <p><a href="https://news.ycombinator.com/user?id=deelowe" target="_blank">deelowe</a>   <span class="timeago" data-date="2021-08-18 13:28:38 &#43;0000 UTC">2021-08-18 13:28:38 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            FWIW, this sort of argument may not trigger a change in policy, but a technical failure in the hashing algorithm might.
        </div>
        <div class="children">
            
        </div>
    </div>


            
                


    
    <div class="comment ">
        <p><a href="https://news.ycombinator.com/user?id=cyanite" target="_blank">cyanite</a>   <span class="timeago" data-date="2021-08-18 20:06:24 &#43;0000 UTC">2021-08-18 20:06:24 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            &gt; Through iMessage, this should be possible in an unsolicited fashion<p>Sure, but those don’t go into your photo library, so it won’t trigger any scanning. Presumably people wouldn’t actively save CSAM into their library.
        </div>
        <div class="children">
            
        </div>
    </div>


            
                


    
    <div class="comment hasChildren">
        <p><a href="https://news.ycombinator.com/user?id=aardvarkr" target="_blank">aardvarkr</a>   <span class="timeago" data-date="2021-08-18 12:41:04 &#43;0000 UTC">2021-08-18 12:41:04 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            Love the relevant xkcd! And to reply to your point, simply sending unsolicited CSAM via iMessage doesn’t trigger anything. That message has to be saved to your phone then uploaded to iCloud. Someone else above said repeat this process 20-30 times so I presume it can’t be a single incident of CSAM. Seems really really hard to trigger this thing by accident or maliciously
        </div>
        <div class="children">
            
                


    
    <div class="comment ">
        <p><a href="https://news.ycombinator.com/user?id=ziml77" target="_blank">ziml77</a>   <span class="timeago" data-date="2021-08-18 12:54:45 &#43;0000 UTC">2021-08-18 12:54:45 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            People are saying that, by default, WhatsApp will save images directly to your camera roll without any interaction. That would be an easy way to trigger the CSAM detection remotely. There are many people who use WhatsApp so it&#x27;s a reasonable concern.
        </div>
        <div class="children">
            
        </div>
    </div>


            
        </div>
    </div>


            
        </div>
    </div>


            
                


    
    <div class="comment ">
        <p><a href="https://news.ycombinator.com/user?id=spicybright" target="_blank">spicybright</a>   <span class="timeago" data-date="2021-08-18 11:19:39 &#43;0000 UTC">2021-08-18 11:19:39 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            Have you not worked a minimum wage job in the US? It&#x27;s incredibly easy to gain phone access to semi-trusting people.<p>If you don&#x27;t like someone (which happens very often in this line of work) you could potentially screw someone over with this.
        </div>
        <div class="children">
            
        </div>
    </div>


            
                


    
    <div class="comment ">
        <p><a href="https://news.ycombinator.com/user?id=superjan" target="_blank">superjan</a>   <span class="timeago" data-date="2021-08-18 11:22:46 &#43;0000 UTC">2021-08-18 11:22:46 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            Great article!
        </div>
        <div class="children">
            
        </div>
    </div>


            
        </div>
    </div>


            
        </div>
    </div>


            
                


    
    <div class="comment ">
        <p><a href="https://news.ycombinator.com/user?id=Jcowell" target="_blank">Jcowell</a>   <span class="timeago" data-date="2021-08-18 20:49:59 &#43;0000 UTC">2021-08-18 20:49:59 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            one vector you can use to skip step 3 is to send on WhatsApp. I believe images sent via WhatsApp are auto saved by default last I recalled.
        </div>
        <div class="children">
            
        </div>
    </div>


            
                


    
    <div class="comment ">
        <p><a href="https://news.ycombinator.com/user?id=xucheng" target="_blank">xucheng</a>   <span class="timeago" data-date="2021-08-18 11:29:05 &#43;0000 UTC">2021-08-18 11:29:05 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            &gt; 4. Actually, need to run step 1-3 at least 30 times<p>Depending on how the secret sharing is used in Apple PSI, it may be possible that duplicating the same image 30 times would be enough.
        </div>
        <div class="children">
            
        </div>
    </div>


            
                


    
    <div class="comment ">
        <p><a href="https://news.ycombinator.com/user?id=dang" target="_blank">dang</a>   <span class="timeago" data-date="2021-08-18 18:47:31 &#43;0000 UTC">2021-08-18 18:47:31 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            We detached this subthread from <a href="https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=28219296" rel="nofollow">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=28219296</a> (it had become massive and this one can stand on its own).
        </div>
        <div class="children">
            
        </div>
    </div>


            
                


    
    <div class="comment hasChildren">
        <p><a href="https://news.ycombinator.com/user?id=eptcyka" target="_blank">eptcyka</a>   <span class="timeago" data-date="2021-08-18 11:23:42 &#43;0000 UTC">2021-08-18 11:23:42 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            I&#x27;m sure the reviewers will definitely be able to give each reported image enough time and attention they need, much like the people youtube employs to review videos discussing and exposing animal abuse, holocaust denial and other controversial topics. &lt;&#x2F;sarcasm&gt;
        </div>
        <div class="children">
            
                


    
    <div class="comment hasChildren">
        <p><a href="https://news.ycombinator.com/user?id=shadowgovt" target="_blank">shadowgovt</a>   <span class="timeago" data-date="2021-08-18 11:30:18 &#43;0000 UTC">2021-08-18 11:30:18 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            Difference in volume. Images that trip CSAM hash are a lot rarer than the content you just described.
        </div>
        <div class="children">
            
                


    
    <div class="comment ">
        <p><a href="https://news.ycombinator.com/user?id=eptcyka" target="_blank">eptcyka</a>   <span class="timeago" data-date="2021-08-18 15:02:27 &#43;0000 UTC">2021-08-18 15:02:27 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            I personally am not aware of how the perceptual hash values are distributed in it&#x27;s keyspace. Perceptual hashes can have issues with uniform distribution, since they are somewhat semantic and most content out there is too. As such, I wouldn&#x27;t make statements about how often collisions would occur.
        </div>
        <div class="children">
            
        </div>
    </div>


            
        </div>
    </div>


            
        </div>
    </div>


            
                


    
    <div class="comment hasChildren">
        <p><a href="https://news.ycombinator.com/user?id=soziawa" target="_blank">soziawa</a>   <span class="timeago" data-date="2021-08-18 10:29:42 &#43;0000 UTC">2021-08-18 10:29:42 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            &gt; 6. Apple&#x27;s CSAM detection then flags these, and they&#x27;re manually reviewed<p>Is the process actually documented anywhere? Afaik they are just saying that they are verifying a match. This could of course just be a person looking at the hash itself.
        </div>
        <div class="children">
            
                


    
    <div class="comment hasChildren">
        <p><a href="https://news.ycombinator.com/user?id=FabHK" target="_blank">FabHK</a>   <span class="timeago" data-date="2021-08-18 10:35:44 &#43;0000 UTC">2021-08-18 10:35:44 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            They look at the contents of the &quot;safety voucher&quot;, which contains the neural hash and a &quot;visual derivative&quot; of the original image (but not the original image itself).<p><a href="https:&#x2F;&#x2F;www.apple.com&#x2F;child-safety&#x2F;pdf&#x2F;CSAM_Detection_Technical_Summary.pdf" rel="nofollow">https:&#x2F;&#x2F;www.apple.com&#x2F;child-safety&#x2F;pdf&#x2F;CSAM_Detection_Techni...</a>
        </div>
        <div class="children">
            
                


    
    <div class="comment ">
        <p><a href="https://news.ycombinator.com/user?id=hughrr" target="_blank">hughrr</a>   <span class="timeago" data-date="2021-08-18 10:44:10 &#43;0000 UTC">2021-08-18 10:44:10 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            If it’s a visual derivative, whatever that means, then how does the reviewer know it matches the source image? Sounds like there’s a lot of non determinism in there.
        </div>
        <div class="children">
            
        </div>
    </div>


            
        </div>
    </div>


            
        </div>
    </div>


            
        </div>
    </div>


        
            


    
    <div class="comment hasChildren">
        <p><a href="https://news.ycombinator.com/user?id=halflings" target="_blank">halflings</a>   <span class="timeago" data-date="2021-08-18 10:08:26 &#43;0000 UTC">2021-08-18 10:08:26 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            Apple&#x27;s scheme includes operators manually verifying a low-res version of each image matching CSAM databases before any intervention. Of course, grey noise will never pass for CSAM and will fail that step.<p>The fact that you can randomly manipulate random noise until it matches the hash of an arbitrary image is not surprising.
The real challenge is generating a real image that could be mistaken for CSAM at low res + is actually benign (or else just send CSAM directly) + matches the hash of real CSAM.<p>This is why SHAttered [1] was such a big deal, but daily random SHA collisions aren&#x27;t.<p>[1] <a href="https:&#x2F;&#x2F;shattered.io&#x2F;" rel="nofollow">https:&#x2F;&#x2F;shattered.io&#x2F;</a>
        </div>
        <div class="children">
            
                


    
    <div class="comment hasChildren">
        <p><a href="https://news.ycombinator.com/user?id=lifthrasiir" target="_blank">lifthrasiir</a>   <span class="timeago" data-date="2021-08-18 10:14:05 &#43;0000 UTC">2021-08-18 10:14:05 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            But you can essentially perform DoS attack to human checkers, effectively rendering the entire system grind to a halt. The entire system is too reliant on the performance of NeuralHash which can be defaced in many ways. [1]<p>(Added later:) I should note that the DoS attack is only possible with the preimage attack and not the second preimage attack as the issue seemingly suggests, because you need the original CSAM to perform the second preimage attack. But given the second preimage attack is <i>this</i> easy, I don&#x27;t have any hope for the preimage resistance anyway.<p>(Added much later:) And I realized that Apple did think of this possibility and only stores blinded hashes in the device, so the preimage attack doesn&#x27;t really work as is. But it seems that the hash output is only 96 bits long according to the repository, so this attack might still be possible albeit with much higher computational cost.<p>[1] To be fair, I don&#x27;t think that Apple&#x27;s claim of 1&#x2F;1,000,000,000,000 false positive rate refers to that of the algorithm. Apple probably tweaked the threshold for manual checking to match that target rate, knowing NeuralHash&#x27;s false positive rate <i>under the normal circumstances</i>. Of course we know that there is no such thing like the normal circumstances.
        </div>
        <div class="children">
            
                


    
    <div class="comment hasChildren">
        <p><a href="https://news.ycombinator.com/user?id=shapefrog" target="_blank">shapefrog</a>   <span class="timeago" data-date="2021-08-18 12:00:15 &#43;0000 UTC">2021-08-18 12:00:15 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            I have seen it suggested that everyone should flood the system with flagged images to overwhelm it in protest to this move by apple.<p>Sounds pretty stupid to me to fill your phone with kiddie porn in protest, but you do you internet people.
        </div>
        <div class="children">
            
                


    
    <div class="comment hasChildren">
        <p><a href="https://news.ycombinator.com/user?id=mannerheim" target="_blank">mannerheim</a>   <span class="timeago" data-date="2021-08-18 12:58:39 &#43;0000 UTC">2021-08-18 12:58:39 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            You don&#x27;t need to do that, just use images that collide with the hashes.
        </div>
        <div class="children">
            
                


    
    <div class="comment ">
        <p><a href="https://news.ycombinator.com/user?id=" target="_blank"></a>   <span class="timeago" data-date="2021-08-18 14:12:43 &#43;0000 UTC">2021-08-18 14:12:43 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            
        </div>
        <div class="children">
            
        </div>
    </div>


            
                


    
    <div class="comment hasChildren">
        <p><a href="https://news.ycombinator.com/user?id=robertoandred" target="_blank">robertoandred</a>   <span class="timeago" data-date="2021-08-18 14:43:39 &#43;0000 UTC">2021-08-18 14:43:39 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            How will you know something collides?
        </div>
        <div class="children">
            
                


    
    <div class="comment hasChildren">
        <p><a href="https://news.ycombinator.com/user?id=mannerheim" target="_blank">mannerheim</a>   <span class="timeago" data-date="2021-08-18 15:17:35 &#43;0000 UTC">2021-08-18 15:17:35 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            &gt; Apple’s method of detecting known CSAM is designed with user privacy in mind. Instead of scanning 
images in the cloud, the system performs on-device matching using a database of known CSAM image 
hashes provided by NCMEC and other child-safety organizations. Apple further transforms this database 
into an unreadable set of hashes, which is securely stored on users’ devices.<p><a href="https:&#x2F;&#x2F;www.apple.com&#x2F;child-safety&#x2F;pdf&#x2F;CSAM_Detection_Technical_Summary.pdf" rel="nofollow">https:&#x2F;&#x2F;www.apple.com&#x2F;child-safety&#x2F;pdf&#x2F;CSAM_Detection_Techni...</a>
        </div>
        <div class="children">
            
                


    
    <div class="comment hasChildren">
        <p><a href="https://news.ycombinator.com/user?id=robertoandred" target="_blank">robertoandred</a>   <span class="timeago" data-date="2021-08-18 16:11:01 &#43;0000 UTC">2021-08-18 16:11:01 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            Yeah, so how would you know something would collide with the hash?
        </div>
        <div class="children">
            
                


    
    <div class="comment hasChildren">
        <p><a href="https://news.ycombinator.com/user?id=mannerheim" target="_blank">mannerheim</a>   <span class="timeago" data-date="2021-08-18 16:17:48 &#43;0000 UTC">2021-08-18 16:17:48 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            Because the hashes are stored on the user&#x27;s device?
        </div>
        <div class="children">
            
                


    
    <div class="comment hasChildren">
        <p><a href="https://news.ycombinator.com/user?id=robertoandred" target="_blank">robertoandred</a>   <span class="timeago" data-date="2021-08-18 16:18:52 &#43;0000 UTC">2021-08-18 16:18:52 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            No, encrypted and blinded hashes are stored. You can’t extract them.
        </div>
        <div class="children">
            
                


    
    <div class="comment ">
        <p><a href="https://news.ycombinator.com/user?id=fastball" target="_blank">fastball</a>   <span class="timeago" data-date="2021-08-19 04:19:05 &#43;0000 UTC">2021-08-19 04:19:05 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            Right, but your device is checking against the hashes, no?<p>So at some point you generate an image which triggers a subroutine and you know that image collides.
        </div>
        <div class="children">
            
        </div>
    </div>


            
                


    
    <div class="comment hasChildren">
        <p><a href="https://news.ycombinator.com/user?id=cyanite" target="_blank">cyanite</a>   <span class="timeago" data-date="2021-08-18 20:11:30 &#43;0000 UTC">2021-08-18 20:11:30 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            This also means that the device doesn’t know if a given image is a match.
        </div>
        <div class="children">
            
                


    
    <div class="comment ">
        <p><a href="https://news.ycombinator.com/user?id=robertoandred" target="_blank">robertoandred</a>   <span class="timeago" data-date="2021-08-18 20:39:39 &#43;0000 UTC">2021-08-18 20:39:39 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            Correct, only the server does.
        </div>
        <div class="children">
            
        </div>
    </div>


            
        </div>
    </div>


            
        </div>
    </div>


            
        </div>
    </div>


            
        </div>
    </div>


            
        </div>
    </div>


            
                


    
    <div class="comment ">
        <p><a href="https://news.ycombinator.com/user?id=HALtheWise" target="_blank">HALtheWise</a>   <span class="timeago" data-date="2021-08-18 19:53:46 &#43;0000 UTC">2021-08-18 19:53:46 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            Presumably, someone on the internet will download a dump _actual_ CP and run it through NeuralHash, then post the resulting hashes to GitHub. Doing so will be illegal for them because they are downloading CP, but it should be perfectly legal for you to then download the hashes they posted, randomly select one, and generate an innocuous image that matches it. The fact that Apple isn&#x27;t posting hashes doesn&#x27;t actually make it all that hard to get your hands on them.
        </div>
        <div class="children">
            
        </div>
    </div>


            
                


    
    <div class="comment hasChildren">
        <p><a href="https://news.ycombinator.com/user?id=falcolas" target="_blank">falcolas</a>   <span class="timeago" data-date="2021-08-18 14:52:45 &#43;0000 UTC">2021-08-18 14:52:45 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            Because the algorithm and list will be on your phone, and can (has, per TFA) be extracted.
        </div>
        <div class="children">
            
                


    
    <div class="comment hasChildren">
        <p><a href="https://news.ycombinator.com/user?id=robertoandred" target="_blank">robertoandred</a>   <span class="timeago" data-date="2021-08-18 14:54:38 &#43;0000 UTC">2021-08-18 14:54:38 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            You cannot extract or reverse the CSAM hashes. They&#x27;ve been encrypted and blinded using server-side-only keys. If TFA said that, it&#x27;s lying.
        </div>
        <div class="children">
            
                


    
    <div class="comment hasChildren">
        <p><a href="https://news.ycombinator.com/user?id=lifthrasiir" target="_blank">lifthrasiir</a>   <span class="timeago" data-date="2021-08-18 16:00:36 &#43;0000 UTC">2021-08-18 16:00:36 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            After more reading of the whitepaper I think you are right. As I understand it, given the image hash H0 and CSAM hashes H[1]...H[n] (some might be duplicates in disguise) the algorithm proceeds like this:<p>- The device generates a secret X and divides it into X[1]...X[m] with the secret sharing algorithm. m is some large number and any k (but no less) copies out of X[i] are enough to reconstruct X.<p>- The device stores blinded hashes f(H[1])...f(H[n]). The function f itself is not known to the client.<p>- The image hash H0 is compressed with another function g to the range between 1 and n.<p>- The downscaled image data (for the human check) is encrypted with X and appended with (probably) random X[i].<p>- The result is then encrypted again with a key derived from f(H0) and sent to the server with an associated data g(H0).<p>- The server tries to decrypt it with a key derived from f(H[g(H0)]). This is only possible when H[g(H0)] = H0, i.e. H0 represents some known CSAM.<p>- You can only decrypt the second layer with at least k copies of X[i] then.<p>At this stage Apple can still learn the number of CSAM images less than k. The fix is described in an overly technical document and I can&#x27;t exactly follow, but supposedly the client can inject an appropriate amount of synthetic data where only the first layer can be always decrypted and the second layer is bogus (including the presumed X[i]).<p>---<p>Assuming this scheme is correctly implemented, the only attack I can imagine is the timing attack. As I understand a malicious client can choose not to send false data. This will affect the number of items that pass the first layer of encryption, so the client can possibly learn the number of actual matches by adjusting the number of synthetic data since the server can only proceed to the next step with at least k such items.<p>This attack seems technically possible, but is probably infeasible to perform (remember that we already need 2^95 oracle operations, which is only vaguely possible even in the local device). Maybe the technical report actually has a solution for this, but for now I can only guess.
        </div>
        <div class="children">
            
                


    
    <div class="comment ">
        <p><a href="https://news.ycombinator.com/user?id=falcolas" target="_blank">falcolas</a>   <span class="timeago" data-date="2021-08-18 16:41:59 &#43;0000 UTC">2021-08-18 16:41:59 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            That synopsis disagrees with Apple&#x27;s own descriptions - or rather it goes into the secondary checks, which confuses the issue that the initial hash checks are indeed performed on-device:<p>&gt; Apple’s method of detecting known CSAM is designed with user privacy in mind. Instead of scanning images in the cloud, the system performs on-device matching using a database of known CSAM image hashes provided by NCMEC and other child-safety organizations. Apple further transforms this database into an unreadable set of hashes, which is securely stored on users’ devices.<p><a href="https:&#x2F;&#x2F;www.apple.com&#x2F;child-safety&#x2F;pdf&#x2F;CSAM_Detection_Technical_Summary.pdf" rel="nofollow">https:&#x2F;&#x2F;www.apple.com&#x2F;child-safety&#x2F;pdf&#x2F;CSAM_Detection_Techni...</a>
        </div>
        <div class="children">
            
        </div>
    </div>


            
        </div>
    </div>


            
                


    
    <div class="comment hasChildren">
        <p><a href="https://news.ycombinator.com/user?id=falcolas" target="_blank">falcolas</a>   <span class="timeago" data-date="2021-08-18 14:58:25 &#43;0000 UTC">2021-08-18 14:58:25 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            One does not need to reverse the CSAM hashes to find a collision with a hash. If the evaluation is being done on the phone, including identifying a hash match, the hashes must also be on the phone.
        </div>
        <div class="children">
            
                


    
    <div class="comment hasChildren">
        <p><a href="https://news.ycombinator.com/user?id=robertoandred" target="_blank">robertoandred</a>   <span class="timeago" data-date="2021-08-18 15:17:05 &#43;0000 UTC">2021-08-18 15:17:05 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            No, matches are not verified on the phone. On the phone, your image hash is used to look up an encrypted&#x2F;blinded (via the server&#x27;s secret key) CSAM hash. Then your image data (the hash and visual derivative) is encrypted with that encrypted&#x2F;blinded hash. This encrypted payload, along with a part of your image&#x27;s hash, is sent to Apple. Then on the server, Apple uses that part of your image&#x27;s hash and their secret key to create a decryption key for the payload. If your image hash matches the CSAM hash, the decryption key would unlock the payload.<p>In addition, they payload is protected at another layer by your user key. Only with enough mash matches can Apple put together the user decryption key and open the very innards of your image&#x27;s payload containing the full hash and visual derivative.
        </div>
        <div class="children">
            
                


    
    <div class="comment ">
        <p><a href="https://news.ycombinator.com/user?id=falcolas" target="_blank">falcolas</a>   <span class="timeago" data-date="2021-08-18 15:27:00 &#43;0000 UTC">2021-08-18 15:27:00 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            To quote a sibling comment, who looked into the horses&#x27; mouth:<p>&gt; Apple’s method of detecting known CSAM is designed with user privacy in mind. Instead of scanning images in the cloud, the system performs on-device matching using a database of known CSAM image hashes provided by NCMEC and other child-safety organizations. Apple further transforms this database into an unreadable set of hashes, which is securely stored on users’ devices.<p><a href="https:&#x2F;&#x2F;www.apple.com&#x2F;child-safety&#x2F;pdf&#x2F;CSAM_Detection_Technical_Summary.pdf" rel="nofollow">https:&#x2F;&#x2F;www.apple.com&#x2F;child-safety&#x2F;pdf&#x2F;CSAM_Detection_Techni...</a>
        </div>
        <div class="children">
            
        </div>
    </div>


            
        </div>
    </div>


            
                


    
    <div class="comment hasChildren">
        <p><a href="https://news.ycombinator.com/user?id=mannerheim" target="_blank">mannerheim</a>   <span class="timeago" data-date="2021-08-18 15:02:40 &#43;0000 UTC">2021-08-18 15:02:40 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            I believe the hash comparisons are made on Apple&#x27;s end. Then the only way to get hashes will be a data breach on Apple&#x27;s end (unlikely but not impossible) or generating it from known CSAM material.
        </div>
        <div class="children">
            
                


    
    <div class="comment hasChildren">
        <p><a href="https://news.ycombinator.com/user?id=falcolas" target="_blank">falcolas</a>   <span class="timeago" data-date="2021-08-18 15:04:51 &#43;0000 UTC">2021-08-18 15:04:51 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            That&#x27;s not what Apple&#x27;s plans state. The comparisons are done on phone, and are only escalated to Apple if there are more than N hash matches, at which point they are supposedly reviewed by Apple employees&#x2F;contractors.<p>Otherwise, they&#x27;d just keep doing it on the material that&#x27;s actually uploaded.
        </div>
        <div class="children">
            
                


    
    <div class="comment hasChildren">
        <p><a href="https://news.ycombinator.com/user?id=mannerheim" target="_blank">mannerheim</a>   <span class="timeago" data-date="2021-08-18 15:16:08 &#43;0000 UTC">2021-08-18 15:16:08 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            Ah, never mind, you&#x27;re right:<p>&gt; Apple’s method of detecting known CSAM is designed with user privacy in mind. Instead of scanning 
images in the cloud, the system performs on-device matching using a database of known CSAM image 
hashes provided by NCMEC and other child-safety organizations. Apple further transforms this database 
into an unreadable set of hashes, which is securely stored on users’ devices.<p><a href="https:&#x2F;&#x2F;www.apple.com&#x2F;child-safety&#x2F;pdf&#x2F;CSAM_Detection_Technical_Summary.pdf" rel="nofollow">https:&#x2F;&#x2F;www.apple.com&#x2F;child-safety&#x2F;pdf&#x2F;CSAM_Detection_Techni...</a>
        </div>
        <div class="children">
            
                


    
    <div class="comment ">
        <p><a href="https://news.ycombinator.com/user?id=cyanite" target="_blank">cyanite</a>   <span class="timeago" data-date="2021-08-18 20:14:07 &#43;0000 UTC">2021-08-18 20:14:07 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            He is not right, though. The system used will not reveal matches to the device, only to the server and only if the threshold is reached.
        </div>
        <div class="children">
            
        </div>
    </div>


            
        </div>
    </div>


            
                


    
    <div class="comment ">
        <p><a href="https://news.ycombinator.com/user?id=cyanite" target="_blank">cyanite</a>   <span class="timeago" data-date="2021-08-18 20:13:27 &#43;0000 UTC">2021-08-18 20:13:27 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            &gt; That&#x27;s not what Apple&#x27;s plans state. The comparisons are done on phone<p>Yes but as stated in the technical description, this match is against a blinded table, so the device doesn’t learn if it’s a match or not.
        </div>
        <div class="children">
            
        </div>
    </div>


            
        </div>
    </div>


            
        </div>
    </div>


            
        </div>
    </div>


            
        </div>
    </div>


            
        </div>
    </div>


            
        </div>
    </div>


            
                


    
    <div class="comment ">
        <p><a href="https://news.ycombinator.com/user?id=" target="_blank"></a>   <span class="timeago" data-date="2021-08-18 14:13:17 &#43;0000 UTC">2021-08-18 14:13:17 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            
        </div>
        <div class="children">
            
        </div>
    </div>


            
        </div>
    </div>


            
                


    
    <div class="comment hasChildren">
        <p><a href="https://news.ycombinator.com/user?id=floatingatoll" target="_blank">floatingatoll</a>   <span class="timeago" data-date="2021-08-18 14:16:57 &#43;0000 UTC">2021-08-18 14:16:57 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            It’s incredibly stupid because your Apple ID will get terminated for abusing Apple services.
        </div>
        <div class="children">
            
                


    
    <div class="comment hasChildren">
        <p><a href="https://news.ycombinator.com/user?id=falcolas" target="_blank">falcolas</a>   <span class="timeago" data-date="2021-08-18 14:53:49 &#43;0000 UTC">2021-08-18 14:53:49 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            There are applications which automatically save images sent to you to your camera roll (such as Whatsapp, IIRC). How can Apple prove you put them there intentionally?<p>Granted, they most likely won&#x27;t care, but it&#x27;s a legitimate attack vector.
        </div>
        <div class="children">
            
                


    
    <div class="comment ">
        <p><a href="https://news.ycombinator.com/user?id=floatingatoll" target="_blank">floatingatoll</a>   <span class="timeago" data-date="2021-08-18 15:11:48 &#43;0000 UTC">2021-08-18 15:11:48 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            You’re right that it’s a valid attack upon the people Apple pays to review matched images before invoking law enforcement, but no harm comes to the recipient in that model, unless they receive real legitimate CSAM and don’t report it to the authorities themselves.<p>Attempted entrapment and abuse of computing systems, which is an uncomfortable way to phrase the WhatsApp scenario, would be quite sufficient cause for a discovery warrant to have WhatsApp reveal the sender’s identity to Apple. Doesn’t mean they’d be found guilty, but WhatsApp will fold a lot sooner than Apple, especially if the warrant is sealed by the court to prevent the sender from deleting any CSAM in their possession.<p>A hacker would say that’s all contrived nonsense and anyways it’s just SWATting, that’s no big deal. A judge would say that’s a reasonable balance of protecting the sender from being dragged through the mud in the press before being indicted and permitting the abused party (Apple) to pursue a conviction and damages.<p>I am not your lawyer, this is not legal advice, etc.
        </div>
        <div class="children">
            
        </div>
    </div>


            
        </div>
    </div>


            
        </div>
    </div>


            
        </div>
    </div>


            
        </div>
    </div>


            
                


    
    <div class="comment hasChildren">
        <p><a href="https://news.ycombinator.com/user?id=Majromax" target="_blank">Majromax</a>   <span class="timeago" data-date="2021-08-18 12:45:59 &#43;0000 UTC">2021-08-18 12:45:59 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            &gt; The fact that you can randomly manipulate random noise until it matches the hash of an arbitrary image is not surprising.<p>It is, actually.  Remember that hashes are supposed to be many-bit digests of the original; it should take O(2^256) work to find a message with a chosen 256-bit hash and O(2^128) work to find a &quot;birthday attack&quot; collision.  Finding any collision at all with NeuralHash so soon after its release is very surprising, suggesting the algorithm is not very strong.<p>SHAttered is a big deal because it is a fully working attack model, but the writing was on the wall for SHA-1 after the collisions were found in reduced-round variations of the hash.  Attacks against an algorithm only get better with time, never worse.<p>Moreover, the break of NeuralHash may be even stronger than the SHAttered attack.  The latter modifies two documents to produce a collision, but the NeuralHash collision here may be a preimage attack.  It&#x27;s not clear if the attacker crafted both images to produce the collision or just the second one.
        </div>
        <div class="children">
            
                


    
    <div class="comment hasChildren">
        <p><a href="https://news.ycombinator.com/user?id=Ajedi32" target="_blank">Ajedi32</a>   <span class="timeago" data-date="2021-08-18 13:50:19 &#43;0000 UTC">2021-08-18 13:50:19 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            NeuralHash is a perceptual hash, not a cryptographically secure hash. Perceptual hashes have trivially findable second preimages <i>by design</i>, as the entire point is for two different images which appear visually similar to return the same result.<p>It&#x27;s not particularly surprising to me that a perceptual hash might also have collisions that don&#x27;t look similar to the human eye, though if Apple ever claimed otherwise this counterexample is solid proof that they&#x27;re wrong.
        </div>
        <div class="children">
            
                


    
    <div class="comment ">
        <p><a href="https://news.ycombinator.com/user?id=cyanite" target="_blank">cyanite</a>   <span class="timeago" data-date="2021-08-18 20:15:06 &#43;0000 UTC">2021-08-18 20:15:06 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            The problem is that you’d need the original NeuralHash, which isn’t stored on the device. The device only has a blinded version.
        </div>
        <div class="children">
            
        </div>
    </div>


            
        </div>
    </div>


            
        </div>
    </div>


            
                


    
    <div class="comment hasChildren">
        <p><a href="https://news.ycombinator.com/user?id=rdli" target="_blank">rdli</a>   <span class="timeago" data-date="2021-08-18 10:15:22 &#43;0000 UTC">2021-08-18 10:15:22 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            Are you pinning your hopes that a false positive like this will be appropriately caught because of an army of faceless, low wage workers who stare at CSAM cases all day will immediately flag?
        </div>
        <div class="children">
            
                


    
    <div class="comment hasChildren">
        <p><a href="https://news.ycombinator.com/user?id=rootusrootus" target="_blank">rootusrootus</a>   <span class="timeago" data-date="2021-08-18 14:31:11 &#43;0000 UTC">2021-08-18 14:31:11 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            Apple has pretty deep pockets.  Think how much that judgement is going to be when they find themselves in court for letting someone get raided over gray images.<p>Not that it&#x27;s going to happen, since it would also require NCMEC to think the images match, but whatever.  Attack me!  Attack me!  I want to retire.
        </div>
        <div class="children">
            
                


    
    <div class="comment ">
        <p><a href="https://news.ycombinator.com/user?id=hda2" target="_blank">hda2</a>   <span class="timeago" data-date="2021-08-19 02:41:08 &#43;0000 UTC">2021-08-19 02:41:08 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            &gt; Apple has pretty deep pockets.<p>For now, sure. What happens when their money runs short? What about the other tech companies that will inevitably be forced to deploy this shit? Will they also have Apple&#x27;s pretty deep pockets?<p>Blind faith in this system will not magically fix how flawed it is nor the abuse and harm it will allow. This is going to hurt a lot of innocent people.
        </div>
        <div class="children">
            
        </div>
    </div>


            
                


    
    <div class="comment ">
        <p><a href="https://news.ycombinator.com/user?id=brokenmachine" target="_blank">brokenmachine</a>   <span class="timeago" data-date="2021-08-19 03:49:31 &#43;0000 UTC">2021-08-19 03:49:31 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            <i>&gt;Attack me! Attack me! I want to retire.</i><p>If you post your whatsapp address, I&#x27;m sure someone will oblige.
        </div>
        <div class="children">
            
        </div>
    </div>


            
        </div>
    </div>


            
        </div>
    </div>


            
                


    
    <div class="comment hasChildren">
        <p><a href="https://news.ycombinator.com/user?id=icelancer" target="_blank">icelancer</a>   <span class="timeago" data-date="2021-08-18 10:45:26 &#43;0000 UTC">2021-08-18 10:45:26 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            &gt; Of course, grey noise will never pass for CSAM and will fail that step.<p>Never? You sure that one or more human operators will <i>never</i> make this mistake, dooming someone&#x27;s life &#x2F; causing them immense pain?
        </div>
        <div class="children">
            
                


    
    <div class="comment hasChildren">
        <p><a href="https://news.ycombinator.com/user?id=shapefrog" target="_blank">shapefrog</a>   <span class="timeago" data-date="2021-08-18 12:03:39 &#43;0000 UTC">2021-08-18 12:03:39 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            I can guarantee nobody will see the inside of a courtroom, on charges of possession and distribution of child porn for possessing multiple images of grey noise (unless there is some steganography going on).
        </div>
        <div class="children">
            
                


    
    <div class="comment hasChildren">
        <p><a href="https://news.ycombinator.com/user?id=falcolas" target="_blank">falcolas</a>   <span class="timeago" data-date="2021-08-18 14:56:59 &#43;0000 UTC">2021-08-18 14:56:59 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            One does not need to go to court to have their life ruined by accusations. Ironically, there&#x27;s quite a few examples of this over the years for alleged CSAM.<p>One example that sticks out in my mind is a pair of grandparents who photographed their grandchildren playing in the back yard. A photo tech flagged their photo, they were arrested, and it took their lawyer going through the hoops to get a review of the photo for the charges to be dropped.
        </div>
        <div class="children">
            
                


    
    <div class="comment hasChildren">
        <p><a href="https://news.ycombinator.com/user?id=shapefrog" target="_blank">shapefrog</a>   <span class="timeago" data-date="2021-08-18 15:38:44 &#43;0000 UTC">2021-08-18 15:38:44 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            Sure, there are always cases - but was their photo of a grey blob that matched a hash but is clearly a grey blob or of a naked child?<p>If the photo was a grey blob and they had to go through a judicial review for someone to look at the photo and confirm &#x27;yes that is a grey blob&#x27; then color me wrong.
        </div>
        <div class="children">
            
                


    
    <div class="comment hasChildren">
        <p><a href="https://news.ycombinator.com/user?id=falcolas" target="_blank">falcolas</a>   <span class="timeago" data-date="2021-08-18 15:43:05 &#43;0000 UTC">2021-08-18 15:43:05 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            I&#x27;d view a &quot;grey blob&quot; to be the MVP of hash collisions. I doubt that this will end with grey blobs - I see it ending with common images (memes would be great for this) being invisibly altered to collide with a CSAM hash.
        </div>
        <div class="children">
            
                


    
    <div class="comment hasChildren">
        <p><a href="https://news.ycombinator.com/user?id=shapefrog" target="_blank">shapefrog</a>   <span class="timeago" data-date="2021-08-18 15:54:11 &#43;0000 UTC">2021-08-18 15:54:11 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            If you need a judicial review to confim that a slightly altered Bernie in Coat and Gloves meme is not the same image as the picture of a child being raped that they have on file then we have way bigger problems.
        </div>
        <div class="children">
            
                


    
    <div class="comment hasChildren">
        <p><a href="https://news.ycombinator.com/user?id=falcolas" target="_blank">falcolas</a>   <span class="timeago" data-date="2021-08-18 16:36:54 &#43;0000 UTC">2021-08-18 16:36:54 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            Here&#x27;s the thing with CSAM - it&#x27;s illegal to view and transmit. So nobody, until the police have confiscated your devices, will actually be able to verify that it is a &quot;child being raped.&quot;<p>They&#x27;ll view visual hashes, look at descriptions, and so forth, but nobody from Apple will actually be looking at them, because then <i>they</i> are guilty of viewing and transmitting CSAM.<p>I noted in another comment, even the prosecutors and defense lawyers in the case typically only get a description of the content, they don&#x27;t see it themselves.
        </div>
        <div class="children">
            
                


    
    <div class="comment ">
        <p><a href="https://news.ycombinator.com/user?id=shapefrog" target="_blank">shapefrog</a>   <span class="timeago" data-date="2021-08-18 17:15:51 &#43;0000 UTC">2021-08-18 17:15:51 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            This is just not true. Human review is conducted. Apple will conduct human review, facebook conduct human review, NCMEC will conduct human review, law enforcement will conduct human review, lawyers and judges will conduct human review.<p>Over the years there have been countless articles etc about how fucked up being a reviewer of content flagged at all the tech companies is. <a href="https:&#x2F;&#x2F;www.vice.com&#x2F;en&#x2F;article&#x2F;a35xk5&#x2F;facebook-moderators-are-suing-for-trauma-ptsd" rel="nofollow">https:&#x2F;&#x2F;www.vice.com&#x2F;en&#x2F;article&#x2F;a35xk5&#x2F;facebook-moderators-a...</a>
        </div>
        <div class="children">
            
        </div>
    </div>


            
                


    
    <div class="comment hasChildren">
        <p><a href="https://news.ycombinator.com/user?id=hannasanarion" target="_blank">hannasanarion</a>   <span class="timeago" data-date="2021-08-18 18:58:16 &#43;0000 UTC">2021-08-18 18:58:16 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            Where did you get this idea, scooby doo?<p>It is not illegal to be an unwilling recipient of illegal material. If a package shows up at your door with a bomb, you&#x27;re not gonna be thrown in jail for having a bomb.
        </div>
        <div class="children">
            
                


    
    <div class="comment hasChildren">
        <p><a href="https://news.ycombinator.com/user?id=hnfong" target="_blank">hnfong</a>   <span class="timeago" data-date="2021-08-18 19:11:55 &#43;0000 UTC">2021-08-18 19:11:55 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            In theory, sure.<p>At the very least, you&#x27;d be one of the primary suspects, and if you somehow got a bad lawyer, all bets are off.<p><a href="https:&#x2F;&#x2F;hongkongfp.com&#x2F;2021&#x2F;08&#x2F;12&#x2F;judges-criticise-hong-kongs-justice-dept-over-prosecution-of-possibly-innocent-man&#x2F;" rel="nofollow">https:&#x2F;&#x2F;hongkongfp.com&#x2F;2021&#x2F;08&#x2F;12&#x2F;judges-criticise-hong-kong...</a>
        </div>
        <div class="children">
            
                


    
    <div class="comment ">
        <p><a href="https://news.ycombinator.com/user?id=hannasanarion" target="_blank">hannasanarion</a>   <span class="timeago" data-date="2021-08-18 19:25:51 &#43;0000 UTC">2021-08-18 19:25:51 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            Okay, and when a cursory look at the bomb actually reveals it to be a fisher-price toy, what then?<p>What is the scenario where a grey blob gets on your phone that sets off CSAM alerts, an investigator looks at it and sees only a grey blob, and then still decides to alert the authorities even though it&#x27;s just a grey blob, and the authorities still decide to arrest you even though it&#x27;s just a grey blob, and the DA still decides to prosecute you even though it&#x27;s just a grey blob, and a jury still decides to convict you, <i>even though it&#x27;s still just a grey blob</i>?<p>You&#x27;re the one who&#x27;s off in theory-land imagining that every person in the entire justice system is just as stupid as this algorithm is.
        </div>
        <div class="children">
            
        </div>
    </div>


            
        </div>
    </div>


            
                


    
    <div class="comment hasChildren">
        <p><a href="https://news.ycombinator.com/user?id=int_19h" target="_blank">int_19h</a>   <span class="timeago" data-date="2021-08-19 00:56:44 &#43;0000 UTC">2021-08-19 00:56:44 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            Possession of CSAM is a strict liability crime in most jurisdictions.
        </div>
        <div class="children">
            
                


    
    <div class="comment ">
        <p><a href="https://news.ycombinator.com/user?id=hannasanarion" target="_blank">hannasanarion</a>   <span class="timeago" data-date="2021-08-19 16:13:53 &#43;0000 UTC">2021-08-19 16:13:53 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            That is simply not true. There is no American jurisdiction where child pornography is a strict liability crime.<p>On this topic, the Supreme Court has ruled in Dickerson v US that, in all cases, to avoid First Amendment conflicts, all child pornography statutes must be interpreted with at least a &quot;reckless disregard&quot; standard.<p>Here is a typical criminal definition, from Minnesota, where a defendant recently tried to argue that the statute was strict liability and therefore unconstitutional, and that argument was rejected by the courts because it is clearly written to require knowledge and intent:<p>&gt; Subd. 4. Possession prohibited. (a) A person who possesses a pornographic work or a computer disk or computer or other electronic, magnetic, or optical storage system ․ containing a pornographic work, <i>knowing or with reason to know its content and character</i>, is guilty of a felony․
        </div>
        <div class="children">
            
        </div>
    </div>


            
        </div>
    </div>


            
        </div>
    </div>


            
        </div>
    </div>


            
        </div>
    </div>


            
        </div>
    </div>


            
        </div>
    </div>


            
        </div>
    </div>


            
                


    
    <div class="comment hasChildren">
        <p><a href="https://news.ycombinator.com/user?id=notRobot" target="_blank">notRobot</a>   <span class="timeago" data-date="2021-08-18 12:45:51 &#43;0000 UTC">2021-08-18 12:45:51 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            Many people never see the inside of a courtroom when false or unproven rape accusations are made against them, but their lives still get ruined because of the negative publicity.
        </div>
        <div class="children">
            
                


    
    <div class="comment ">
        <p><a href="https://news.ycombinator.com/user?id=bscphil" target="_blank">bscphil</a>   <span class="timeago" data-date="2021-08-18 13:55:31 &#43;0000 UTC">2021-08-18 13:55:31 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            Those cases are not comparable, because the whole reason they have that impact is that the accusations are usually made publicly (because the whole point is to harm the reputation of one&#x27;s rapist and warn others, should a conviction prove to be impossible), while CSAM review goes through a neural hash privately on your phone, then privately and anonymously through an Apple reviewer, then is privately reviewed at NCMEC (who - I think - have access to the full size image), and only then is turned over to law enforcement (which should also have access to the full image).<p>It only becomes public knowledge if law enforcement then chooses to charge you - and if all that happens on the basis of an obvious adversarial net image, the result is a publicity shitshow for Apple and you become a civil rights hero after your lawyer (even an underpaid overworked public defender should be able to handle this one) demonstrates this.<p>As others have stated in this thread, I think the real failure case is not someone&#x27;s life getting ruined by claims of CSAM possession somehow resulting from a bad hash match, but the fact that planted material (or sent via message) can now easily ruin your life because it gets automatically reported; you can&#x27;t simply delete it and move on any more.
        </div>
        <div class="children">
            
        </div>
    </div>


            
                


    
    <div class="comment ">
        <p><a href="https://news.ycombinator.com/user?id=rootusrootus" target="_blank">rootusrootus</a>   <span class="timeago" data-date="2021-08-18 14:34:33 &#43;0000 UTC">2021-08-18 14:34:33 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            We give way, way too much weight in the legal system to eye witness and victim statements, in absence of any corroborating evidence.  That&#x27;s a problem.<p>But not really comparable, IMO.  You won&#x27;t even know you got investigated until after the original images have been shipped off to NCMEC for verification.
        </div>
        <div class="children">
            
        </div>
    </div>


            
                


    
    <div class="comment ">
        <p><a href="https://news.ycombinator.com/user?id=shapefrog" target="_blank">shapefrog</a>   <span class="timeago" data-date="2021-08-18 13:18:47 &#43;0000 UTC">2021-08-18 13:18:47 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            Are you suggesting that perhaps less people should report rape accusations, because it might be awkward for the accused to get negative publicity? Thats messed up.
        </div>
        <div class="children">
            
        </div>
    </div>


            
        </div>
    </div>


            
                


    
    <div class="comment hasChildren">
        <p><a href="https://news.ycombinator.com/user?id=dannyw" target="_blank">dannyw</a>   <span class="timeago" data-date="2021-08-18 16:13:50 &#43;0000 UTC">2021-08-18 16:13:50 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            What if it is legal pornography of 21 year olds but disturbed to collide with CSAM? You are aware even defence lawyers are not allowed to look at alleged CSAM material in court right?
        </div>
        <div class="children">
            
                


    
    <div class="comment hasChildren">
        <p><a href="https://news.ycombinator.com/user?id=shapefrog" target="_blank">shapefrog</a>   <span class="timeago" data-date="2021-08-18 16:23:52 &#43;0000 UTC">2021-08-18 16:23:52 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            The process would not trigger any action. The NCMEC, who can look at the material, and are the people to whom the matter is reported, would compare the flagged image with the source material and reject it as not matching known CSAM.<p>What if the legal porn of a 21 year old that triggered the collision match looked really really really close? So close that a human can not distinguish between the image of a 12 year old being raped that they have in their database and your image? Well then you might have a problem, legal and otherwise.<p>&gt; defence lawyers are not allowed to look at alleged CSAM material in court right<p>I know this is not true in many countries, but cant speak for your country.
        </div>
        <div class="children">
            
                


    
    <div class="comment hasChildren">
        <p><a href="https://news.ycombinator.com/user?id=dannyw" target="_blank">dannyw</a>   <span class="timeago" data-date="2021-08-18 16:42:19 &#43;0000 UTC">2021-08-18 16:42:19 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            You are aware that a lot of CSAM are close ups of say pussies for example, and human anatomy can look very similar?<p>I&#x27;m not talking about images of rape here. I&#x27;m taking about images that you&#x27;d see on a regular porn site, of adults and their body parts.<p>You are also aware that CSAM covers anywhere from 0 to 17.99 years of age, and the legal obligation to report exists equally for the whole spectrum?<p>So let&#x27;s say I download a close up pussy collection of 31 images of what I believe to be consenting 20 year olds, and what are consenting 20 year olds.<p>But they are actually planted by an attacker (let&#x27;s say an oppressive regime who doesn&#x27;t like me) and disturbed to match CSAM, that is, pussy close ups of 17 year olds. They are all just pussy pics. They will look the same.<p>Should I go to jail?<p>Do I have a non zero chance of going to jail? Yes.
        </div>
        <div class="children">
            
                


    
    <div class="comment ">
        <p><a href="https://news.ycombinator.com/user?id=shapefrog" target="_blank">shapefrog</a>   <span class="timeago" data-date="2021-08-18 17:27:08 &#43;0000 UTC">2021-08-18 17:27:08 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            If you have content that has a matching hash value and is identical by all computational and human inspection to content that has been identified as CSAM from which the hash was generate then you have a problem.<p>Without getting into the metaphysics of what is an image, at that point, you basically have a large collection of child porn.<p>Your hypothetical oppressive regime has gone to a lot of trouble planting not illegal evidence on your device. It would be much more effective to just put actual child porn on your device, which you would need to have to conduct the attack in the first place.
        </div>
        <div class="children">
            
        </div>
    </div>


            
                


    
    <div class="comment hasChildren">
        <p><a href="https://news.ycombinator.com/user?id=cyanite" target="_blank">cyanite</a>   <span class="timeago" data-date="2021-08-18 20:16:54 &#43;0000 UTC">2021-08-18 20:16:54 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            &gt; You are aware that a lot of CSAM are close ups of say pussies for example, and human anatomy can look very similar?<p>I doubt images that look quite generic will make it into those hash sets, though.
        </div>
        <div class="children">
            
                


    
    <div class="comment ">
        <p><a href="https://news.ycombinator.com/user?id=brokenmachine" target="_blank">brokenmachine</a>   <span class="timeago" data-date="2021-08-19 03:47:51 &#43;0000 UTC">2021-08-19 03:47:51 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            What is the factual basis for you to doubt that?
        </div>
        <div class="children">
            
        </div>
    </div>


            
                


    
    <div class="comment ">
        <p><a href="https://news.ycombinator.com/user?id=FabHK" target="_blank">FabHK</a>   <span class="timeago" data-date="2021-08-18 21:35:49 &#43;0000 UTC">2021-08-18 21:35:49 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            Nearly impossible to verify, though, by construction.
        </div>
        <div class="children">
            
        </div>
    </div>


            
        </div>
    </div>


            
        </div>
    </div>


            
        </div>
    </div>


            
        </div>
    </div>


            
                


    
    <div class="comment hasChildren">
        <p><a href="https://news.ycombinator.com/user?id=icelancer" target="_blank">icelancer</a>   <span class="timeago" data-date="2021-08-18 21:46:14 &#43;0000 UTC">2021-08-18 21:46:14 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            &gt; I can guarantee nobody will see the inside of a courtroom<p>This wasn&#x27;t the question I asked.
        </div>
        <div class="children">
            
                


    
    <div class="comment hasChildren">
        <p><a href="https://news.ycombinator.com/user?id=shapefrog" target="_blank">shapefrog</a>   <span class="timeago" data-date="2021-08-18 23:08:11 &#43;0000 UTC">2021-08-18 23:08:11 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            &gt; grey noise will never pass for CSAM
&gt; You sure that one or more human operators will never make this mistake.<p>Yes I can say 100% that no human operator will ever classify a grey image for a child being raped. Happy to put money on it.
        </div>
        <div class="children">
            
                


    
    <div class="comment ">
        <p><a href="https://news.ycombinator.com/user?id=meowster" target="_blank">meowster</a>   <span class="timeago" data-date="2021-08-18 23:58:20 &#43;0000 UTC">2021-08-18 23:58:20 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            It&#x27;s possible that the operator accidently clicks the wrong button.<p>When dealing with a monotonous task that the operator is probably getting PTSD from, I think the chance is greater than 0%.<p>Articles about content moderators and PTSD:<p><a href="https:&#x2F;&#x2F;www.businessinsider.com&#x2F;youtube-content-moderators-accenture-ptsd-statements-acknowledgements-mental-health-2020-1" rel="nofollow">https:&#x2F;&#x2F;www.businessinsider.com&#x2F;youtube-content-moderators-a...</a><p><a href="https:&#x2F;&#x2F;www.bbc.com&#x2F;news&#x2F;technology-52642633" rel="nofollow">https:&#x2F;&#x2F;www.bbc.com&#x2F;news&#x2F;technology-52642633</a><p><a href="https:&#x2F;&#x2F;www.theverge.com&#x2F;2019&#x2F;2&#x2F;25&#x2F;18229714&#x2F;cognizant-facebook-content-moderator-interviews-trauma-working-conditions-arizona" rel="nofollow">https:&#x2F;&#x2F;www.theverge.com&#x2F;2019&#x2F;2&#x2F;25&#x2F;18229714&#x2F;cognizant-facebo...</a>
        </div>
        <div class="children">
            
        </div>
    </div>


            
        </div>
    </div>


            
        </div>
    </div>


            
        </div>
    </div>


            
        </div>
    </div>


            
                


    
    <div class="comment ">
        <p><a href="https://news.ycombinator.com/user?id=SXX" target="_blank">SXX</a>   <span class="timeago" data-date="2021-08-18 14:35:56 &#43;0000 UTC">2021-08-18 14:35:56 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            &gt; The real challenge is generating a real image that could be mistaken for CSAM at low res + is actually benign (or else just send CSAM directly) + matches the hash of real CSAM.<p>Why do you have an idea that image have to be benign? Almost everyone watch porn and it&#x27;s will be so much easier to find collisions by manipulating actual porn images which are not CSAM.<p>Also this way you&#x27;ll more likely to trigged false-positive from Apple staff since they aren&#x27;t suppose to see how actual CSAM looks like.
        </div>
        <div class="children">
            
        </div>
    </div>


            
                


    
    <div class="comment hasChildren">
        <p><a href="https://news.ycombinator.com/user?id=bo1024" target="_blank">bo1024</a>   <span class="timeago" data-date="2021-08-18 12:26:04 &#43;0000 UTC">2021-08-18 12:26:04 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            &gt; The fact that you can randomly manipulate random noise until it matches the hash of an arbitrary image is not surprising.<p>Strongly disagree. (1) The primary feature of any decent hash function is that this should not happen. (2) Any preimage attack opens the way for further manipulations like you describe.
        </div>
        <div class="children">
            
                


    
    <div class="comment hasChildren">
        <p><a href="https://news.ycombinator.com/user?id=brokensegue" target="_blank">brokensegue</a>   <span class="timeago" data-date="2021-08-18 14:37:02 &#43;0000 UTC">2021-08-18 14:37:02 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            cryptographic hashes are different from image fingerprints
        </div>
        <div class="children">
            
                


    
    <div class="comment ">
        <p><a href="https://news.ycombinator.com/user?id=bo1024" target="_blank">bo1024</a>   <span class="timeago" data-date="2021-08-18 14:47:04 &#43;0000 UTC">2021-08-18 14:47:04 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            That&#x27;s true, one way to put it is that traditionally non-cryptographic hashes are supposed to prevent accidental collisions, while cryptographic ones should prevent even collisions on purpose.<p>But hashing is used in many places that could be vulnerable to an attack, so I think the distinction is blurry. People used MD5 for lots of things but are moving away for this reason, even though they&#x27;re not in cryptographic settings.
        </div>
        <div class="children">
            
        </div>
    </div>


            
        </div>
    </div>


            
        </div>
    </div>


            
                


    
    <div class="comment ">
        <p><a href="https://news.ycombinator.com/user?id=nabakin" target="_blank">nabakin</a>   <span class="timeago" data-date="2021-08-18 10:27:58 &#43;0000 UTC">2021-08-18 10:27:58 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            I don&#x27;t think that is far away either. I won&#x27;t be surprised if that is achieved within the day, if not sooner.<p>Also, generating images that look the same as the original and yet produce a different hash.
        </div>
        <div class="children">
            
        </div>
    </div>


            
                


    
    <div class="comment ">
        <p><a href="https://news.ycombinator.com/user?id=varispeed" target="_blank">varispeed</a>   <span class="timeago" data-date="2021-08-18 11:29:57 &#43;0000 UTC">2021-08-18 11:29:57 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            &gt; Apple&#x27;s scheme includes operators manually verifying a low-res version of each image<p>The reviewer, likely on a minimum wage, will report images just in case. Nobody would like to be dragged through the mud because they didn&#x27;t report something they thought it is innocent.
        </div>
        <div class="children">
            
        </div>
    </div>


            
        </div>
    </div>


        
            


    
    <div class="comment hasChildren">
        <p><a href="https://news.ycombinator.com/user?id=Edd314159" target="_blank">Edd314159</a>   <span class="timeago" data-date="2021-08-18 11:46:48 &#43;0000 UTC">2021-08-18 11:46:48 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            I think I am in dire need of some education here and so I have questions:<p>* Is this a problem with Apple&#x27;s CSAM discriminator engine or with the fact that it&#x27;s happening on-device?<p>* Would this attack not be possible if scanning was instead happening in the cloud, using the same model?<p>* Are other services (Google Photos, Facebook, etc.) that store photos in the cloud not doing something similar to uploaded photos, with models that may be similarly vulnerable to this attack?<p>I know that an argument against on-device scanning is that people don&#x27;t like to feel like the device that they own is acting against them - like it&#x27;s snitching on them. I can understand and actually sympathise with that argument, it feels wrong.<p>But we have known for a long time that computer vision can be fooled with adversarial images. What is special about this particular example? Is it only because it&#x27;s specifically tricking the Apple CSAM system, which is currently a hotly-debated topic, or is there something particularly bad here, something that is not true with other CSAM &quot;detectors&quot;?<p>I genuinely don&#x27;t know enough about this subject to comment with anything other than questions.
        </div>
        <div class="children">
            
                


    
    <div class="comment hasChildren">
        <p><a href="https://news.ycombinator.com/user?id=bo1024" target="_blank">bo1024</a>   <span class="timeago" data-date="2021-08-18 12:32:18 &#43;0000 UTC">2021-08-18 12:32:18 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            Not complete answers but background: apple’s system works by having your device create a hash of each image you have. The hash (a short hexadecimal string) is compared to a list of known CP image hashes, and if it matches, then your image is uploaded to Apple for further investigation.<p>A devastating scenario for such a system is if an attacker knows how to look at a hash and generate some image that matches the hash, allowing them to trigger false positives any time. That appears to be what we are witnessing.
        </div>
        <div class="children">
            
                


    
    <div class="comment hasChildren">
        <p><a href="https://news.ycombinator.com/user?id=Edd314159" target="_blank">Edd314159</a>   <span class="timeago" data-date="2021-08-18 12:56:35 &#43;0000 UTC">2021-08-18 12:56:35 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            &gt; A devastating scenario for such a system is if an attacker knows how to look at a hash and generate some image that matches the hash, allowing them to trigger false positives any time.<p>This is my understanding too. But is this not also true for other (cloud-based) CSAM scanning systems? Why is Apple&#x27;s special in this regard?
        </div>
        <div class="children">
            
                


    
    <div class="comment hasChildren">
        <p><a href="https://news.ycombinator.com/user?id=nonbirithm" target="_blank">nonbirithm</a>   <span class="timeago" data-date="2021-08-18 16:14:56 &#43;0000 UTC">2021-08-18 16:14:56 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            They aren&#x27;t.<p>Apple could have saved themselves so much backlash and not have caused the outrage to be focused exclusively on them if they hadn&#x27;t tried to be novel with their method of hashing, and had just announced that they were about to do exactly what all the other tech companies had already been doing for years - server side scanning.<p>Apple would still be accused of walking back on its claims of protecting users&#x27; privacy, but for a different reason - by trying to conform. Instead of wasting all the debate on how Apple and <i>only</i> Apple is violating everyone&#x27;s privacy with its on-device scanning mechanism, which was without precedent, this could have been an educational experience for many people about how little privacy is valued in the cloud in general, no matter who you choose to give your data to, because there <i>is</i> precedent for such privacy violations that take place on the server.<p>Apple could have been just one of the companies in a long line of others whose data management policies would have received significant renewed attention as a result of this. Instead, everyone is focused on criticizing Apple.<p>There is a significant problem with people&#x27;s perception of &quot;privacy&quot; in tech if merely moving the scan on-device causes this much backlash while those same people stayed silent during the times that Google and Facebook and the rest adopted the very same technique on the server in the past decade. Maybe if Apple had done the same, they would have been able to get away with it.
        </div>
        <div class="children">
            
                


    
    <div class="comment ">
        <p><a href="https://news.ycombinator.com/user?id=cyanite" target="_blank">cyanite</a>   <span class="timeago" data-date="2021-08-18 20:20:05 &#43;0000 UTC">2021-08-18 20:20:05 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            Perception aside, Apple’s system is somewhat better for privacy, since Apple needs to access much less data server side.
        </div>
        <div class="children">
            
        </div>
    </div>


            
        </div>
    </div>


            
                


    
    <div class="comment ">
        <p><a href="https://news.ycombinator.com/user?id=floatingatoll" target="_blank">floatingatoll</a>   <span class="timeago" data-date="2021-08-18 14:35:25 &#43;0000 UTC">2021-08-18 14:35:25 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            There’s a lot of overlap between Apple product buyers and “fight the thoughtcrime slippery slope” hackers. The CSAM abusers are presumably (if they’re not stupid) also fanning the flames on that slippery slope perception, because it’s to their benefit if the hackers defeat Apple.
        </div>
        <div class="children">
            
        </div>
    </div>


            
                


    
    <div class="comment hasChildren">
        <p><a href="https://news.ycombinator.com/user?id=bo1024" target="_blank">bo1024</a>   <span class="timeago" data-date="2021-08-18 14:48:26 &#43;0000 UTC">2021-08-18 14:48:26 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            I don&#x27;t know. I don&#x27;t know what other systems use, I just know what I&#x27;ve read recently about Apple&#x27;s.
        </div>
        <div class="children">
            
                


    
    <div class="comment ">
        <p><a href="https://news.ycombinator.com/user?id=cyanite" target="_blank">cyanite</a>   <span class="timeago" data-date="2021-08-18 20:20:39 &#43;0000 UTC">2021-08-18 20:20:39 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            This pretty much sums this entire drama up, I think.
        </div>
        <div class="children">
            
        </div>
    </div>


            
        </div>
    </div>


            
        </div>
    </div>


            
                


    
    <div class="comment hasChildren">
        <p><a href="https://news.ycombinator.com/user?id=rollinggoron" target="_blank">rollinggoron</a>   <span class="timeago" data-date="2021-08-18 14:17:27 &#43;0000 UTC">2021-08-18 14:17:27 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            But how does this exact attack and scenario not also apply to Google, Facebook, Microsoft etc... who are also doing the same thing on their clouds servers?
        </div>
        <div class="children">
            
                


    
    <div class="comment hasChildren">
        <p><a href="https://news.ycombinator.com/user?id=bo1024" target="_blank">bo1024</a>   <span class="timeago" data-date="2021-08-18 14:52:02 &#43;0000 UTC">2021-08-18 14:52:02 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            I don&#x27;t know what those companies do, hopefully someone who does know will chime in and answer.
        </div>
        <div class="children">
            
                


    
    <div class="comment ">
        <p><a href="https://news.ycombinator.com/user?id=cyanite" target="_blank">cyanite</a>   <span class="timeago" data-date="2021-08-18 20:18:58 &#43;0000 UTC">2021-08-18 20:18:58 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            They essentially do the same, just in the cloud meaning they access all images directly.
        </div>
        <div class="children">
            
        </div>
    </div>


            
        </div>
    </div>


            
        </div>
    </div>


            
                


    
    <div class="comment ">
        <p><a href="https://news.ycombinator.com/user?id=cyanite" target="_blank">cyanite</a>   <span class="timeago" data-date="2021-08-18 20:18:28 &#43;0000 UTC">2021-08-18 20:18:28 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            That’s an oversimplified and misleading description of how the system works, but ok. I recommend reading the technical description, or even the paper linked from that: <a href="https:&#x2F;&#x2F;www.apple.com&#x2F;child-safety&#x2F;pdf&#x2F;CSAM_Detection_Technical_Summary.pdf" rel="nofollow">https:&#x2F;&#x2F;www.apple.com&#x2F;child-safety&#x2F;pdf&#x2F;CSAM_Detection_Techni...</a>
        </div>
        <div class="children">
            
        </div>
    </div>


            
                


    
    <div class="comment ">
        <p><a href="https://news.ycombinator.com/user?id=fortenforge" target="_blank">fortenforge</a>   <span class="timeago" data-date="2021-08-18 18:18:31 &#43;0000 UTC">2021-08-18 18:18:31 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            No, I don&#x27;t think that&#x27;s what we&#x27;re witnessing. No one has yet demonstrated that they can take a hash alone and produce an image that matches the hash. It&#x27;s true that that would be bad, but right now you still need the original colliding image.
        </div>
        <div class="children">
            
        </div>
    </div>


            
                


    
    <div class="comment hasChildren">
        <p><a href="https://news.ycombinator.com/user?id=magpi3" target="_blank">magpi3</a>   <span class="timeago" data-date="2021-08-18 12:39:01 &#43;0000 UTC">2021-08-18 12:39:01 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            But how would the attacker get the generated image on a person&#x27;s phone?
        </div>
        <div class="children">
            
                


    
    <div class="comment ">
        <p><a href="https://news.ycombinator.com/user?id=bo1024" target="_blank">bo1024</a>   <span class="timeago" data-date="2021-08-18 14:50:57 &#43;0000 UTC">2021-08-18 14:50:57 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            I didn&#x27;t mean to suggest a targeted attack. If the goal is to just overwhelm Apple&#x27;s system, the attacker doesn&#x27;t need to target a particular phone, they just need to distribute lots of colliding images to some phones. Even their own phones, since the colliding images wouldn&#x27;t be illegal.
        </div>
        <div class="children">
            
        </div>
    </div>


            
                


    
    <div class="comment hasChildren">
        <p><a href="https://news.ycombinator.com/user?id=Koffiepoeder" target="_blank">Koffiepoeder</a>   <span class="timeago" data-date="2021-08-18 12:45:58 &#43;0000 UTC">2021-08-18 12:45:58 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            E.g. send them a whatsapp message that looks innocent
        </div>
        <div class="children">
            
                


    
    <div class="comment hasChildren">
        <p><a href="https://news.ycombinator.com/user?id=endisneigh" target="_blank">endisneigh</a>   <span class="timeago" data-date="2021-08-18 13:24:24 &#43;0000 UTC">2021-08-18 13:24:24 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            They can see the image - why would they import a random image into their library from someone they don’t know?
        </div>
        <div class="children">
            
                


    
    <div class="comment hasChildren">
        <p><a href="https://news.ycombinator.com/user?id=kemayo" target="_blank">kemayo</a>   <span class="timeago" data-date="2021-08-18 13:58:16 &#43;0000 UTC">2021-08-18 13:58:16 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            WhatsApp has a <i>really weird</i> default behavior: it imports all images you&#x27;re sent into your photo library.<p>This is a smart thing to disable, even outside this recent discussion of CSAM.<p><a href="https:&#x2F;&#x2F;faq.whatsapp.com&#x2F;android&#x2F;how-to-stop-saving-whatsapp-media-to-your-phones-gallery&#x2F;?lang=en" rel="nofollow">https:&#x2F;&#x2F;faq.whatsapp.com&#x2F;android&#x2F;how-to-stop-saving-whatsapp...</a>
        </div>
        <div class="children">
            
                


    
    <div class="comment hasChildren">
        <p><a href="https://news.ycombinator.com/user?id=endisneigh" target="_blank">endisneigh</a>   <span class="timeago" data-date="2021-08-18 14:23:05 &#43;0000 UTC">2021-08-18 14:23:05 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            In that scenario this attack is unnecessary. Someone could just send you legitimate child pornography and then immediately tell the authorities that you possess said things.
        </div>
        <div class="children">
            
                


    
    <div class="comment hasChildren">
        <p><a href="https://news.ycombinator.com/user?id=kemayo" target="_blank">kemayo</a>   <span class="timeago" data-date="2021-08-18 14:25:49 &#43;0000 UTC">2021-08-18 14:25:49 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            Yup! It&#x27;s a legitimately crazy default.<p>Edit: Though, to be fair, the specific hash-collision scenario would be that someone could send you something that doesn&#x27;t <i>look</i> like CSAM and so you wouldn&#x27;t reflexively delete it.
        </div>
        <div class="children">
            
                


    
    <div class="comment hasChildren">
        <p><a href="https://news.ycombinator.com/user?id=endisneigh" target="_blank">endisneigh</a>   <span class="timeago" data-date="2021-08-18 14:28:13 &#43;0000 UTC">2021-08-18 14:28:13 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            If it doesn’t look like it wouldn’t a human reviewer disregard it once it gets to that point?<p>Personally I don’t really see the issue.
        </div>
        <div class="children">
            
                


    
    <div class="comment hasChildren">
        <p><a href="https://news.ycombinator.com/user?id=MisterSandman" target="_blank">MisterSandman</a>   <span class="timeago" data-date="2021-08-18 14:57:39 &#43;0000 UTC">2021-08-18 14:57:39 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            We don&#x27;t know how the human review is going to work. Countries with fewer resources are going to find it easier to just arrest&#x2F;detain any suspects, instead of spending time and money on figuring out which reports are true.<p>All you have to be is accused of CP for your life to be destroyed. It doesn&#x27;t matter if you did it or not.
        </div>
        <div class="children">
            
                


    
    <div class="comment ">
        <p><a href="https://news.ycombinator.com/user?id=endisneigh" target="_blank">endisneigh</a>   <span class="timeago" data-date="2021-08-18 17:20:35 &#43;0000 UTC">2021-08-18 17:20:35 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            What you’re describing is possible even if you didn’t receive anything.<p>If the government wants to get you they don’t need this Apple scanning tech, or anything at all really
        </div>
        <div class="children">
            
        </div>
    </div>


            
        </div>
    </div>


            
                


    
    <div class="comment ">
        <p><a href="https://news.ycombinator.com/user?id=kemayo" target="_blank">kemayo</a>   <span class="timeago" data-date="2021-08-18 14:31:11 &#43;0000 UTC">2021-08-18 14:31:11 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            Yeah, you&#x27;d need a <i>very specific</i> set of processes for it to really be a problem.
        </div>
        <div class="children">
            
        </div>
    </div>


            
        </div>
    </div>


            
        </div>
    </div>


            
        </div>
    </div>


            
        </div>
    </div>


            
                


    
    <div class="comment ">
        <p><a href="https://news.ycombinator.com/user?id=Kliment" target="_blank">Kliment</a>   <span class="timeago" data-date="2021-08-18 20:35:56 &#43;0000 UTC">2021-08-18 20:35:56 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            Whatsapp imports received images into camera roll by default, and icloud sync is on by default. So you just need to get into a WA group the victim is a member of, and have the victim use default settings.
        </div>
        <div class="children">
            
        </div>
    </div>


            
                


    
    <div class="comment ">
        <p><a href="https://news.ycombinator.com/user?id=markus92" target="_blank">markus92</a>   <span class="timeago" data-date="2021-08-18 13:48:22 &#43;0000 UTC">2021-08-18 13:48:22 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            Auto import could be turned on?
        </div>
        <div class="children">
            
        </div>
    </div>


            
        </div>
    </div>


            
        </div>
    </div>


            
                


    
    <div class="comment ">
        <p><a href="https://news.ycombinator.com/user?id=bumblebritches5" target="_blank">bumblebritches5</a>   <span class="timeago" data-date="2021-08-18 15:32:27 &#43;0000 UTC">2021-08-18 15:32:27 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            Seriously?<p>Look into Pegasus and Candiru
        </div>
        <div class="children">
            
        </div>
    </div>


            
        </div>
    </div>


            
        </div>
    </div>


            
                


    
    <div class="comment ">
        <p><a href="https://news.ycombinator.com/user?id=gitgud" target="_blank">gitgud</a>   <span class="timeago" data-date="2021-08-19 00:59:45 &#43;0000 UTC">2021-08-19 00:59:45 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            Basically you are right. There&#x27;s nothing that special about hash collisions with image recognition.<p>I think this is blowing up because it&#x27;s cathartic to see a technology you disagree with get undermined and basically broken by the community...
        </div>
        <div class="children">
            
        </div>
    </div>


            
        </div>
    </div>


        
            


    
    <div class="comment hasChildren">
        <p><a href="https://news.ycombinator.com/user?id=vesinisa" target="_blank">vesinisa</a>   <span class="timeago" data-date="2021-08-18 10:16:48 &#43;0000 UTC">2021-08-18 10:16:48 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            Now this offers Apple a very delicate opportunity to back out of the whole scanning controversy due to technological vulnerabilities.
        </div>
        <div class="children">
            
                


    
    <div class="comment ">
        <p><a href="https://news.ycombinator.com/user?id=notquitehuman" target="_blank">notquitehuman</a>   <span class="timeago" data-date="2021-08-18 13:56:41 &#43;0000 UTC">2021-08-18 13:56:41 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            There’s no going back now. They revealed themselves to be enthusiastic participants in a creeping global surveillance system. They’d have to fire every single executive who passed on an opportunity to tank this thing before they get the opportunity to make the case that they’ve changed.
        </div>
        <div class="children">
            
        </div>
    </div>


            
                


    
    <div class="comment ">
        <p><a href="https://news.ycombinator.com/user?id=nonbirithm" target="_blank">nonbirithm</a>   <span class="timeago" data-date="2021-08-18 19:39:07 &#43;0000 UTC">2021-08-18 19:39:07 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            The idea that because the state of the art right now is flawed that Apple should give up misses the point.<p>Even if it is flawed, in a few years Apple will just release a new version or a different technique that works more effectively for its stated purpose. Other companies will silently improve the server-side scanning techniques they already use after the public outcry started by Apple blows over.<p>So long as companies feel the need to protect themselves from liability for hosting certain kinds of data, be it criminal or political or moral or anything else, there are no <i>societal</i> checks in place to stop them from doing so.<p>This is not a technological issue like so many people are trying to frame it as; it is a policy issue.
        </div>
        <div class="children">
            
        </div>
    </div>


            
                


    
    <div class="comment ">
        <p><a href="https://news.ycombinator.com/user?id=kleene_op" target="_blank">kleene_op</a>   <span class="timeago" data-date="2021-08-18 12:11:09 &#43;0000 UTC">2021-08-18 12:11:09 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            10 bucks they don&#x27;t.
        </div>
        <div class="children">
            
        </div>
    </div>


            
                


    
    <div class="comment ">
        <p><a href="https://news.ycombinator.com/user?id=rootusrootus" target="_blank">rootusrootus</a>   <span class="timeago" data-date="2021-08-18 14:38:31 &#43;0000 UTC">2021-08-18 14:38:31 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            The only way they will back out is if this misunderstanding of perceptual hashes results in significant blowback from average customers.  Apple is certainly not going to be surprised that a perceptual hash collision is technically easy.
        </div>
        <div class="children">
            
        </div>
    </div>


            
                


    
    <div class="comment ">
        <p><a href="https://news.ycombinator.com/user?id=Croftengea" target="_blank">Croftengea</a>   <span class="timeago" data-date="2021-08-18 13:20:42 &#43;0000 UTC">2021-08-18 13:20:42 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            This! Apple almost never admit their mistakes (&quot;You&#x27;re holding it wrong&quot;), so the technical argument would allow leaving this feature in an indefinite and perhaps opt-in beta.
        </div>
        <div class="children">
            
        </div>
    </div>


            
        </div>
    </div>


        
            


    
    <div class="comment hasChildren">
        <p><a href="https://news.ycombinator.com/user?id=SXX" target="_blank">SXX</a>   <span class="timeago" data-date="2021-08-18 14:59:52 &#43;0000 UTC">2021-08-18 14:59:52 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            Some people here in comments believe that whoever gonna check reported material on Apple side will never ever flag false-positive. We already know that NCMEC database itself don&#x27;t exclusively contain child porn, but also some other photos that closely related ot CSAM. Even if those photos don&#x27;t have actual CSAM on them. But let&#x27;s ignore this fact.<p>Do people who believe in behevolent Apple understand that CSAM don&#x27;t all have some big red &quot;CHILD PORN&quot; sign on it? No demonic feel included. Like any porn we can suppose that many of such images might not even have any faces or literally anything that make them different from images of 18yo person.<p>When you think well about it brute-forcing actual porn or some jailbait images doesn&#x27;t sound that impossible. All you need is a lot of totally legal porn and some compute power. Both we have in abundance.
        </div>
        <div class="children">
            
                


    
    <div class="comment hasChildren">
        <p><a href="https://news.ycombinator.com/user?id=dannyw" target="_blank">dannyw</a>   <span class="timeago" data-date="2021-08-18 16:16:42 &#43;0000 UTC">2021-08-18 16:16:42 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            I guarantee you out of millions of alleged CSAM images hapzardly added by local police and intetnet reports, thousands are legal porn of consenting adults.
        </div>
        <div class="children">
            
                


    
    <div class="comment ">
        <p><a href="https://news.ycombinator.com/user?id=SXX" target="_blank">SXX</a>   <span class="timeago" data-date="2021-08-18 18:51:32 &#43;0000 UTC">2021-08-18 18:51:32 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            I totally sure it&#x27;s very much possible, but even if this database only exclusively contained CSAM it&#x27;s still very  much possible for human to match false-positive since Apple can only show their snoops your photos and likely they will be compressed to some 360x360 or whatever.
        </div>
        <div class="children">
            
        </div>
    </div>


            
        </div>
    </div>


            
                


    
    <div class="comment ">
        <p><a href="https://news.ycombinator.com/user?id=int_19h" target="_blank">int_19h</a>   <span class="timeago" data-date="2021-08-19 00:59:14 &#43;0000 UTC">2021-08-19 00:59:14 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            Quite frankly, I don&#x27;t even care whether the reviewers are 100% perfect or not. I did not consent to having some random stranger out there reviewing my <i>private</i> photos. Even if it doesn&#x27;t actually have any legal consequences, it&#x27;s still unacceptable.
        </div>
        <div class="children">
            
        </div>
    </div>


            
        </div>
    </div>


        
            


    
    <div class="comment hasChildren">
        <p><a href="https://news.ycombinator.com/user?id=UncleMeat" target="_blank">UncleMeat</a>   <span class="timeago" data-date="2021-08-18 11:52:10 &#43;0000 UTC">2021-08-18 11:52:10 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            Why is this meaningfully different than, say, what Google Photos has been doing for years?<p>If you can get rooting malware on the target device then you could<p>1. Produce actual CSAM rather than a hash collision<p>2. Produce lots of it<p>3. Sync it with Google Photos<p>This attack has been available for many years and does not need convoluted steps like hash collisions if you have the means to control somebody&#x27;s phone with a RAT.
        </div>
        <div class="children">
            
                


    
    <div class="comment hasChildren">
        <p><a href="https://news.ycombinator.com/user?id=UseStrict" target="_blank">UseStrict</a>   <span class="timeago" data-date="2021-08-18 13:56:24 &#43;0000 UTC">2021-08-18 13:56:24 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            The _initial_ implementation of client-side scanning has the same initial result, but vastly different futures. Up until now it could be (foolishly or otherwise) assumed that Apple had your personal privacy in mind. Now they have demonstrated a willingness to compromise your local device privacy. Right now it&#x27;s iCloud only, but I&#x27;m sure it&#x27;s a boolean configuration away somewhere to make it scan everything, regardless of whether it&#x27;s marked for iCloud upload or not. And I don&#x27;t think Apple is strong enough to withstand a gagged demand to flip that boolean.<p>On their cloud services (Apple, Google, MS, etc) they can only get to content that a user has implicitly agreed to share (whether or not they understand the ramifications is another conversation). On your device, there&#x27;s no more separation.
        </div>
        <div class="children">
            
                


    
    <div class="comment ">
        <p><a href="https://news.ycombinator.com/user?id=UncleMeat" target="_blank">UncleMeat</a>   <span class="timeago" data-date="2021-08-18 14:33:38 &#43;0000 UTC">2021-08-18 14:33:38 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            But the discussion here is about malicious actors fraudulently inserting files that trigger alarms. If they have control over your device, &quot;content that a user has implicitly agreed to share&quot; is not a meaningful category.
        </div>
        <div class="children">
            
        </div>
    </div>


            
                


    
    <div class="comment ">
        <p><a href="https://news.ycombinator.com/user?id=cyanite" target="_blank">cyanite</a>   <span class="timeago" data-date="2021-08-18 20:23:18 &#43;0000 UTC">2021-08-18 20:23:18 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            How have they demonstrated that they will compromise your privacy? They compromise it <i>less</i> than the systems scanning on the cloud, since like this most of the match is only known to the device and not the cloud.<p>If you don’t trust Apple to not lie, then the entire discussion becomes a bit moot, I think, since then they could pretty much do anything, with or without this system.
        </div>
        <div class="children">
            
        </div>
    </div>


            
        </div>
    </div>


            
                


    
    <div class="comment hasChildren">
        <p><a href="https://news.ycombinator.com/user?id=grishka" target="_blank">grishka</a>   <span class="timeago" data-date="2021-08-18 14:17:11 &#43;0000 UTC">2021-08-18 14:17:11 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            The big difference is that cloud providers do the scanning on their own infrastructure. If you don&#x27;t want something scanned, you don&#x27;t upload it to the could, that simple. But here, your own device is snitching on you.
        </div>
        <div class="children">
            
                


    
    <div class="comment hasChildren">
        <p><a href="https://news.ycombinator.com/user?id=UncleMeat" target="_blank">UncleMeat</a>   <span class="timeago" data-date="2021-08-18 14:35:14 &#43;0000 UTC">2021-08-18 14:35:14 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            But the discussion at hand is about malicious actors causing innocent people to trip the alarms. Given the clear capabilities of tools like Pegasus, &quot;just don&#x27;t enable cloud syncing&quot; is obviously not sufficient to protect against a malicious actor who wants to plant illegal content on your device and trip the alarms.
        </div>
        <div class="children">
            
                


    
    <div class="comment ">
        <p><a href="https://news.ycombinator.com/user?id=hannasanarion" target="_blank">hannasanarion</a>   <span class="timeago" data-date="2021-08-18 19:02:15 &#43;0000 UTC">2021-08-18 19:02:15 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            OK, and so what? &quot;Tripping the alarms&quot; in this case means somebody looks at the pictures that were put on your device without your knowledge, sees that they&#x27;re all grey blobs, and flags it as a false alarm, case closed.
        </div>
        <div class="children">
            
        </div>
    </div>


            
        </div>
    </div>


            
                


    
    <div class="comment ">
        <p><a href="https://news.ycombinator.com/user?id=cyanite" target="_blank">cyanite</a>   <span class="timeago" data-date="2021-08-18 20:24:05 &#43;0000 UTC">2021-08-18 20:24:05 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            No. Only pictures being uploaded to iCloud Photo Library are being scanned.
        </div>
        <div class="children">
            
        </div>
    </div>


            
        </div>
    </div>


            
                


    
    <div class="comment ">
        <p><a href="https://news.ycombinator.com/user?id=dang" target="_blank">dang</a>   <span class="timeago" data-date="2021-08-18 18:48:55 &#43;0000 UTC">2021-08-18 18:48:55 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            (We detached this subthread from <a href="https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=28219296" rel="nofollow">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=28219296</a>.)
        </div>
        <div class="children">
            
        </div>
    </div>


            
                


    
    <div class="comment hasChildren">
        <p><a href="https://news.ycombinator.com/user?id=short_sells_poo" target="_blank">short_sells_poo</a>   <span class="timeago" data-date="2021-08-18 12:08:22 &#43;0000 UTC">2021-08-18 12:08:22 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            Google was not standing on a pedestal preaching privacy. In contrast Apple was trying to appear as the privacy conscious hardware&#x2F;software vendor. To now implement such a blatantly obvious stepping stone to dragnet surveillance of actual devices is such a hypocritical move that it beggars belief.<p>Ignoring the whataboutism in your question, we know that privacy once lost is practically impossible to get back. Once the genie is out of the bottle and Apple is doing on device scanning, what&#x27;s to stop 3 letter agencies and governments around the world to start demanding ever more access? Because that&#x27;s exactly what&#x27;s going to happen. &quot;Oh it&#x27;s not a big deal, we just need slightly more access than we have now.&quot;<p>10 years down the line, we&#x27;d have people&#x27;s phones datamining every piece of info they have and silently reporting to an unknowable set of entities. All in the name of fighting crime.<p>There needs to be pushback on this crap. Every single one of these attempts must be met with absolute and unconditional refusal. Mere inaction means things will inevitably and invariably get worse over time.
        </div>
        <div class="children">
            
                


    
    <div class="comment hasChildren">
        <p><a href="https://news.ycombinator.com/user?id=UncleMeat" target="_blank">UncleMeat</a>   <span class="timeago" data-date="2021-08-18 12:41:18 &#43;0000 UTC">2021-08-18 12:41:18 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            It just seems to me that there are two very different conversations happening at the same time, with people swapping back and forth between them<p>1. There can be false positives or other mechanisms for innocent people to get flagged.<p>2. It is bad to do this sort of check on the local disk.<p>The discussion at hand started as entirely #1. But now you&#x27;ve swapped to #2, talking about government spying on local files. It makes it very difficult to have a conversation because any pushback against arguments made for one point is assumed to be pushback against arguments made for the other point.
        </div>
        <div class="children">
            
                


    
    <div class="comment hasChildren">
        <p><a href="https://news.ycombinator.com/user?id=stetrain" target="_blank">stetrain</a>   <span class="timeago" data-date="2021-08-18 13:52:15 &#43;0000 UTC">2021-08-18 13:52:15 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            This exactly.<p>I am mostly convinced based on the technical details that have (slowly) come out from Apple that they have made this system sufficiently inconvenient to use as a direct surveillance system by a malicious government.<p>Yes a government could secretly order Apple to make changes to the system, but they could also order them to just give them all of your iCloud photos and backups, or send data directly from the Camera or Messages app, or any number of things that would be easier and more useful for the government. If you don&#x27;t trust your government don&#x27;t use a mobile device on a public cell network or store your data on servers.<p>But all of that said there is a still a line being crossed on principle and precedent for scanning images locally on device and then reporting out when something &quot;bad&quot; is found.<p>Apple thinks this is more private than just directly rifling through your photos in iCloud, but I can draw a line between what is on my device and what I send to Apple&#x27;s servers and be comfortable with that.
        </div>
        <div class="children">
            
                


    
    <div class="comment hasChildren">
        <p><a href="https://news.ycombinator.com/user?id=short_sells_poo" target="_blank">short_sells_poo</a>   <span class="timeago" data-date="2021-08-18 14:40:14 &#43;0000 UTC">2021-08-18 14:40:14 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            Yes of course it is more private than outright scanning all photos but how does that relate to the ground state of private photos simply not being scanned at all?<p>And please do not bring in whataboutist arguments like &quot;but other cloud providers are already doing it&quot;.<p>I&#x27;m honestly taken aback by how many people on HN are completely OK with what Apple is pulling here.<p>In my mind, it&#x27;s irrelevant that the current system is &quot;sufficiently inconvenient to use as a dragnet surveillance system&quot;, because it&#x27;s the first step towards one that is convenient to use and if we extrapolate all the other similar efforts, we know full well what is going to happen.
        </div>
        <div class="children">
            
                


    
    <div class="comment hasChildren">
        <p><a href="https://news.ycombinator.com/user?id=stetrain" target="_blank">stetrain</a>   <span class="timeago" data-date="2021-08-18 14:46:16 &#43;0000 UTC">2021-08-18 14:46:16 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            I don&#x27;t think &quot;All user data on device and in the cloud is not scannable for CSAM or retrievable with a warrant&quot; is a tenable position in the current US political landscape, and even less so in other countries.<p>And my point is that if the government wanted a dragnet they could just legislate or secretly order one. Just like they have done in various forms over the last 20 years. And Apple might not even be allowed to tell us it is happening.
        </div>
        <div class="children">
            
                


    
    <div class="comment hasChildren">
        <p><a href="https://news.ycombinator.com/user?id=short_sells_poo" target="_blank">short_sells_poo</a>   <span class="timeago" data-date="2021-08-18 15:22:57 &#43;0000 UTC">2021-08-18 15:22:57 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            &gt; &quot;All user data on device and in the cloud is not scannable for CSAM or retrievable with a warrant&quot;
Who said anything about warrants? As far as I know the proposed system is proactive and requires no warrants at all.<p>More to the point, anyone remotely sophisticated can just encrypt CSAM into a binary blob and plaster it all over the cloud providers servers.<p>Ie, this system will possibly catch some small time perverts at the cost of even more potential of government misuse of the current technical landscape.<p>I consider it a similar situation to governments trying to legislate backdoors into encryption. A remotely sophisticated baddie will just ignore the laws, and all it does is add risks for innocents.<p>&gt; And my point is that if the government wanted a dragnet they could just legislate or secretly order one. Just like they have done in various forms over the last 20 years. And Apple might not even be allowed to tell us it is happening.<p>I don&#x27;t understand how is this an argument against the pushback at all. So just because it could be worse, we should just throw our hands in the air and say &quot;Oh it&#x27;s all fine, it could be worse so the (so far) mild intrusion into privacy is nothing to worry about.&quot;<p>The thing is, these things seem to happen step by step so the outrage is minimized. Insert your favorite &quot;frog being boiled slowly&quot; anecdote here. You don&#x27;t push back on the mild stuff, and before you realize things have gotten so much worse.
        </div>
        <div class="children">
            
                


    
    <div class="comment hasChildren">
        <p><a href="https://news.ycombinator.com/user?id=stetrain" target="_blank">stetrain</a>   <span class="timeago" data-date="2021-08-18 15:36:24 &#43;0000 UTC">2021-08-18 15:36:24 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            I haven’t ever said this is a good thing and that we should like it.<p>I’m saying if the concern is that a government orders Apple to change it and do something different, then that’s a government problem and maybe we should try fixing that.
        </div>
        <div class="children">
            
                


    
    <div class="comment hasChildren">
        <p><a href="https://news.ycombinator.com/user?id=short_sells_poo" target="_blank">short_sells_poo</a>   <span class="timeago" data-date="2021-08-18 16:15:55 &#43;0000 UTC">2021-08-18 16:15:55 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            &gt; then that’s a government problem and maybe we should try fixing that.<p>But why even give governments hints that people are generally OK with their devices being scanned?<p>We can argue about the technicalities of how abusable or resilient the current implementation is. But we can agree that it&#x27;s a step towards losing privacy, yes? We didn&#x27;t have scanning of iDevices before, now we do.<p>Because in my mind it&#x27;s not a long shot to argue that once it becomes normalized that Apple can scan people&#x27;s phones for CSAM when uploading to iCloud, it&#x27;s just a small extra step to scan all pictures. The capability is basically in place already, it&#x27;s literally removing a filter.<p>And then the next small step is not just CSAM but any fingerprint submitted by LE. And so it goes.<p>Governments can&#x27;t legally compel Apple to implement this capability. But if the capability is already there, Apple can be compelled to turn over the information collected. Again, they can&#x27;t do that if the capability and information doesn&#x27;t exist.<p>E.g. if Apple can&#x27;t decrypt data on your phone because they designed it such, then they can&#x27;t be forced, even with a warrant, to backdoor your phone. They can legally refuse to add such capabilities.
        </div>
        <div class="children">
            
                


    
    <div class="comment ">
        <p><a href="https://news.ycombinator.com/user?id=stetrain" target="_blank">stetrain</a>   <span class="timeago" data-date="2021-08-18 16:55:56 &#43;0000 UTC">2021-08-18 16:55:56 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            &gt; We can argue about the technicalities of how abusable or resilient the current implementation is. But we can agree that it&#x27;s a step towards losing privacy, yes? We didn&#x27;t have scanning of iDevices before, now we do.<p>Yes, that was the intention of my original comment. The “slippery slope” is one of principle and precedent, more than this specific technical implementation.
        </div>
        <div class="children">
            
        </div>
    </div>


            
        </div>
    </div>


            
        </div>
    </div>


            
        </div>
    </div>


            
        </div>
    </div>


            
        </div>
    </div>


            
        </div>
    </div>


            
                


    
    <div class="comment hasChildren">
        <p><a href="https://news.ycombinator.com/user?id=short_sells_poo" target="_blank">short_sells_poo</a>   <span class="timeago" data-date="2021-08-18 14:35:36 &#43;0000 UTC">2021-08-18 14:35:36 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            I&#x27;d argue they are both important points that are tightly interconnected.<p>#2 is bad on it&#x27;s own and in my mind it shouldn&#x27;t even get to the merits of discussing #1 because at that point the debate is already lost in favor of surveillance.<p>But beyond that, #1 is highly problematic, doubly so given the fact that government surveillance is basically a non-decreasing function.
        </div>
        <div class="children">
            
                


    
    <div class="comment ">
        <p><a href="https://news.ycombinator.com/user?id=cyanite" target="_blank">cyanite</a>   <span class="timeago" data-date="2021-08-18 20:26:09 &#43;0000 UTC">2021-08-18 20:26:09 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            I’d argue that #2 is good, since it offers much more privacy than scanning in the cloud. This is based on reading and understanding the technical summary and paper linked from there.
        </div>
        <div class="children">
            
        </div>
    </div>


            
        </div>
    </div>


            
        </div>
    </div>


            
        </div>
    </div>


            
        </div>
    </div>


        
            


    
    <div class="comment hasChildren">
        <p><a href="https://news.ycombinator.com/user?id=HumblyTossed" target="_blank">HumblyTossed</a>   <span class="timeago" data-date="2021-08-18 14:03:48 &#43;0000 UTC">2021-08-18 14:03:48 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            This can also be used to make Apple&#x27;s system useless, no?  If enough (millions?) of people were to, say, go to a web site and save generated gray-blobs to their phones, it would create enough false-positives to kill this system, right?  Maybe game it and have everyone convert their various profile pics to these images.
        </div>
        <div class="children">
            
                


    
    <div class="comment ">
        <p><a href="https://news.ycombinator.com/user?id=floatingatoll" target="_blank">floatingatoll</a>   <span class="timeago" data-date="2021-08-18 14:24:52 &#43;0000 UTC">2021-08-18 14:24:52 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            No, because Apple will simply start killing Apple IDs and blocking hardware by serial number for abusive behavior towards Apple when y’all do that, and that’s a very expensive problem to overcome.<p>“You tried to exploit our production systems and we’re ending our customer relationship with you over it” is a classic Apple move and no one outside of the Hackintosh community realizes that their devices include crypto-signed attestations of their serial number during Apple service sign-ins.
        </div>
        <div class="children">
            
        </div>
    </div>


            
                


    
    <div class="comment hasChildren">
        <p><a href="https://news.ycombinator.com/user?id=robertoandred" target="_blank">robertoandred</a>   <span class="timeago" data-date="2021-08-18 14:45:23 &#43;0000 UTC">2021-08-18 14:45:23 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            And how would anyone know that those gray blobs match child porn?
        </div>
        <div class="children">
            
                


    
    <div class="comment hasChildren">
        <p><a href="https://news.ycombinator.com/user?id=jug" target="_blank">jug</a>   <span class="timeago" data-date="2021-08-18 16:01:26 &#43;0000 UTC">2021-08-18 16:01:26 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            Exactly, that’s the scary abuse scenario where people could send a whole load of this stuff to e.g a political enemy.
        </div>
        <div class="children">
            
                


    
    <div class="comment hasChildren">
        <p><a href="https://news.ycombinator.com/user?id=robertoandred" target="_blank">robertoandred</a>   <span class="timeago" data-date="2021-08-18 16:18:15 &#43;0000 UTC">2021-08-18 16:18:15 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            What? Why do you think gray blobs would hurt anyone?
        </div>
        <div class="children">
            
                


    
    <div class="comment hasChildren">
        <p><a href="https://news.ycombinator.com/user?id=dannyw" target="_blank">dannyw</a>   <span class="timeago" data-date="2021-08-18 16:25:02 &#43;0000 UTC">2021-08-18 16:25:02 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            No one is going to send gray blobs, they will be finding legal porn (like pussy close ups, tongue pics, whatever) and then disturbing it to trigger a CSAM hit.<p>The low res derivative will match, perhaps even closely, because pussy closeups look similar to an apple employee when its grayscale 64 by 64 pixels (remember: it&#x27;s illegal for Apple to transmit CSAM, so it must be so visually degraded to the point where it&#x27;s arguably not visual).<p>The victim will get raided, be considered a paedophile by their workplace, media, and family, and perhaps even go into jail.<p>The attacker in this case can be users of Pegasus unhappy with a journalist.
        </div>
        <div class="children">
            
                


    
    <div class="comment ">
        <p><a href="https://news.ycombinator.com/user?id=robertoandred" target="_blank">robertoandred</a>   <span class="timeago" data-date="2021-08-18 17:51:25 &#43;0000 UTC">2021-08-18 17:51:25 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            And disturbing it enough to match CSAM would make it obvious it’s been tampered with. And how would the attacker obtain the CSAM hashes they’d need to match?
        </div>
        <div class="children">
            
        </div>
    </div>


            
        </div>
    </div>


            
        </div>
    </div>


            
        </div>
    </div>


            
        </div>
    </div>


            
        </div>
    </div>


        
            


    
    <div class="comment hasChildren">
        <p><a href="https://news.ycombinator.com/user?id=sierpinsky" target="_blank">sierpinsky</a>   <span class="timeago" data-date="2021-08-18 17:53:44 &#43;0000 UTC">2021-08-18 17:53:44 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            &quot;According to media reports, the cloud computing industry does not take full advantage of the existing CSAM screening toolsto detect images or videos in cloud computing storage. For instance, big industry players, such as Apple, do not scan their cloud storage. In 2019, Amazon provided only eight reports to the NCMEC, despite handling cloud storage services with millions of uploads and downloads every second. Others, such as Dropbox, Google and Microsoft perform scans for illegal images, but &#x27;only when someone shares them, not when they are uploaded&#x27;.&quot; [1]<p>So I guess the question is what exactly &quot;others&quot; are doing, &#x27;only when someone shares them, not when they are uploaded&#x27;. The whole discussion seems to center around what Apple intends to do on-device, ignoring what others are already doing in the cloud. Isn&#x27;t this strange?<p>[1] <a href="https:&#x2F;&#x2F;www.europarl.europa.eu&#x2F;RegData&#x2F;etudes&#x2F;BRIE&#x2F;2020&#x2F;659360&#x2F;EPRS_BRI(2020)659360_EN.pdf" rel="nofollow">https:&#x2F;&#x2F;www.europarl.europa.eu&#x2F;RegData&#x2F;etudes&#x2F;BRIE&#x2F;2020&#x2F;6593...</a>
        </div>
        <div class="children">
            
                


    
    <div class="comment ">
        <p><a href="https://news.ycombinator.com/user?id=eightysixfour" target="_blank">eightysixfour</a>   <span class="timeago" data-date="2021-08-18 23:07:52 &#43;0000 UTC">2021-08-18 23:07:52 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            It is a shift in trust. If my things are scanned on my local device, I now must trust that Apple:<p>* Will not be compelled to change the list of targeted material by government coercion.<p>* Will not upload the &quot;vouchers&quot; unless the material is uploaded to iCloud.<p>* Will implement these things in such a way that hackers cannot maliciously cause someone to be wrongly implicated in a crime.<p>* Will implement these things in such a way that hackers cannot use the tools Apple has created to seek other information (such as state sponsored hacking groups looking for political dissidents).<p>And for many of us, we do not believe that these things are a given. Here&#x27;s a fictional scenario to help bring it home:<p>Let&#x27;s say a device is created that can, with decent accuracy, detect drugs in the air. Storage unit rental companies start to install them in all of their storage units to reduce the risk that they are storing illegal substances, and they notify the police if the sensors go off.<p>One storage unit rental company feels like this is an invasion of privacy, so they install the device in your house but promise to only check the results if you move your things into the storage unit.<p>This sounds crazy, right?
        </div>
        <div class="children">
            
        </div>
    </div>


            
                


    
    <div class="comment ">
        <p><a href="https://news.ycombinator.com/user?id=cyanite" target="_blank">cyanite</a>   <span class="timeago" data-date="2021-08-18 20:59:20 &#43;0000 UTC">2021-08-18 20:59:20 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            &gt; The whole discussion seems to center around what Apple intends to do on-device, ignoring what others are already doing in the cloud. Isn&#x27;t this strange?<p>Very strange. Especially when this on-device technique means that Apple needs to access far less data than when doing it on the cloud.
        </div>
        <div class="children">
            
        </div>
    </div>


            
        </div>
    </div>


        
            


    
    <div class="comment hasChildren">
        <p><a href="https://news.ycombinator.com/user?id=magpi3" target="_blank">magpi3</a>   <span class="timeago" data-date="2021-08-18 12:44:49 &#43;0000 UTC">2021-08-18 12:44:49 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            Can someone ELI5? I understand that a person can now generate an image with the same hash as an illegal image (such as child porn), but I don&#x27;t understand how they can get it on someone&#x27;s phone and I don&#x27;t understand why someone would get in trouble for an image, when finally examined, that is clearly not child pornography.
        </div>
        <div class="children">
            
                


    
    <div class="comment hasChildren">
        <p><a href="https://news.ycombinator.com/user?id=bitneuker" target="_blank">bitneuker</a>   <span class="timeago" data-date="2021-08-18 13:03:28 &#43;0000 UTC">2021-08-18 13:03:28 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            A vulnerability by itself is not that dangerous, but in combination with a sophisticated attack, or another vulnerability can be disastrous. State actors have the resources to exploit a number of unknown bugs in combination with this collision to have Apple&#x27;s systems flag persons of interest.<p>This, combined with human error during the manual review process might result in someone getting reported. Seeing as twitter (and other social media sites) jump on the bandwagon whenever someone gets accused of being a pedophile, this might destroy someones life.<p>The entire story might seem a bit to far fetched, but based on past events, you never know how bad something &#x27;simple&#x27; as a hash collision can be.
        </div>
        <div class="children">
            
                


    
    <div class="comment hasChildren">
        <p><a href="https://news.ycombinator.com/user?id=dannyw" target="_blank">dannyw</a>   <span class="timeago" data-date="2021-08-18 16:28:57 &#43;0000 UTC">2021-08-18 16:28:57 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            No one is going to send gray blobs, they will be finding legal porn (like pussy close ups, tongue pics, whatever) and then disturbing it to trigger a CSAM hit.<p>The low res derivative will match, perhaps even closely, because pussy closeups look similar to an apple employee when its grayscale 64 by 64 pixels (remember: it&#x27;s illegal for Apple to transmit CSAM, so it must be so visually degraded to the point where it&#x27;s arguably not visual).<p>The victim will get raided, be considered a paedophile by their workplace, media, and family, and perhaps even go into jail.<p>The attacker in this case can be users of Pegasus unhappy with a journalist.
        </div>
        <div class="children">
            
                


    
    <div class="comment ">
        <p><a href="https://news.ycombinator.com/user?id=FabHK" target="_blank">FabHK</a>   <span class="timeago" data-date="2021-08-18 21:29:07 &#43;0000 UTC">2021-08-18 21:29:07 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            Ok, so you posit an attacker could find&#x2F;generate 30+ pictures that are<p>1. accepted by innocent user,<p>2. flagged as known CSAM by NeuralHash,<p>2b. also flagged by the second algorithm Apple will run over flagged images server side as known CSAM,<p>3. apparently CSAM in the &quot;visual derivative&quot;.<p>That strikes me as a rather remote scenario, but worth investigating. Having said that, if it&#x27;s a 3-letter adversary using Pegasus unhappy with a journalist, couldn&#x27;t they just put actual CSAM onto the journalist&#x27;s phone? And couldn&#x27;t they have done that for many years?
        </div>
        <div class="children">
            
        </div>
    </div>


            
                


    
    <div class="comment ">
        <p><a href="https://news.ycombinator.com/user?id=cyanite" target="_blank">cyanite</a>   <span class="timeago" data-date="2021-08-18 20:27:17 &#43;0000 UTC">2021-08-18 20:27:17 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            There is a lot of speculation about things that haven’t happened in that comment.
        </div>
        <div class="children">
            
        </div>
    </div>


            
        </div>
    </div>


            
                


    
    <div class="comment hasChildren">
        <p><a href="https://news.ycombinator.com/user?id=rootusrootus" target="_blank">rootusrootus</a>   <span class="timeago" data-date="2021-08-18 14:39:51 &#43;0000 UTC">2021-08-18 14:39:51 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            &gt; State actors have the resources<p>You can end the conversation right there.  If you are up against a state actor, you have already lost.
        </div>
        <div class="children">
            
                


    
    <div class="comment hasChildren">
        <p><a href="https://news.ycombinator.com/user?id=dannyw" target="_blank">dannyw</a>   <span class="timeago" data-date="2021-08-18 16:33:07 &#43;0000 UTC">2021-08-18 16:33:07 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            Incorrect. A Chinese state actor can&#x27;t just go around imprisoning journalists they don&#x27;t like in America, but they can now do this through planting child porngoraphy via remote malware (Pegasus) and watch their enemies get arrested by the US Feds.
        </div>
        <div class="children">
            
                


    
    <div class="comment ">
        <p><a href="https://news.ycombinator.com/user?id=rootusrootus" target="_blank">rootusrootus</a>   <span class="timeago" data-date="2021-08-18 20:12:22 &#43;0000 UTC">2021-08-18 20:12:22 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            Disagree.  It isn&#x27;t just jurisdiction.  It is resource access.  If the Chinese gov&#x27;t were coming after little old me right now, I&#x27;d be properly worried, even though I&#x27;m safely within the boundaries of the US.
        </div>
        <div class="children">
            
        </div>
    </div>


            
        </div>
    </div>


            
                


    
    <div class="comment hasChildren">
        <p><a href="https://news.ycombinator.com/user?id=bitneuker" target="_blank">bitneuker</a>   <span class="timeago" data-date="2021-08-18 16:23:52 &#43;0000 UTC">2021-08-18 16:23:52 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            Very true, just wanted to paint a picture on how this could be abused.
        </div>
        <div class="children">
            
                


    
    <div class="comment ">
        <p><a href="https://news.ycombinator.com/user?id=dannyw" target="_blank">dannyw</a>   <span class="timeago" data-date="2021-08-18 16:34:10 &#43;0000 UTC">2021-08-18 16:34:10 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            It&#x27;s not true at all. You&#x27;re assuming state actors are in the same jurisdiction. This isn&#x27;t always the case - think an oppressive authoritian regime wanting to get an American journalist arrested for child pornography.<p>It&#x27;s always possible before, but client-side CSAM detection and alerting has weaponised this.<p>Previously, you always had to somehow alert an unfriendly jurisdiction. Now, you just use malware like Pegasus to drop CSAM, whether real or disturbed from legal porn, and watch as Apple tips off the Feds on your enemies.
        </div>
        <div class="children">
            
        </div>
    </div>


            
        </div>
    </div>


            
        </div>
    </div>


            
                


    
    <div class="comment hasChildren">
        <p><a href="https://news.ycombinator.com/user?id=floatingatoll" target="_blank">floatingatoll</a>   <span class="timeago" data-date="2021-08-18 14:43:46 &#43;0000 UTC">2021-08-18 14:43:46 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            State actors will just Gitmo you, without all this wasteful effort on hashes. This system offers no benefit sufficient to make it worth their time if they want to cull you from the population somehow.
        </div>
        <div class="children">
            
                


    
    <div class="comment hasChildren">
        <p><a href="https://news.ycombinator.com/user?id=dannyw" target="_blank">dannyw</a>   <span class="timeago" data-date="2021-08-18 16:31:45 &#43;0000 UTC">2021-08-18 16:31:45 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            No, China can&#x27;t just Gitmo an American journalist on American soil.<p>But now China can send some legal pornography (eg closeup pussy pictures), disturbed to match a CSAM hit, to a journalist they don&#x27;t like and get them in jail.<p>Why can&#x27;t China do this before? Because previously, they&#x27;d still need to tip off authorities, which has an attribution trail and credibility barrier. Now, they can just use Pegasus to plant these images and then watch as Apple turns them into the Feds. Zero links to the attacker.
        </div>
        <div class="children">
            
                


    
    <div class="comment ">
        <p><a href="https://news.ycombinator.com/user?id=floatingatoll" target="_blank">floatingatoll</a>   <span class="timeago" data-date="2021-08-18 17:27:48 &#43;0000 UTC">2021-08-18 17:27:48 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            The scenario you describe has already been extant for the past ten years. Unreported zerodays could have been used at any time to inject a CSAM hit into someone&#x27;s camera roll, way back in time where they wouldn&#x27;t see it, in order to get them investigated. Their phone would have uploaded it to iCloud or Google Photos or Dropbox Whatever and the CSAM detections at each place would have fired off. No need for any of this fancy AI static nonsense.<p>I know of zero instances of this attack being executed on anyone, so apparently even though it&#x27;s been possible for years, it isn&#x27;t a material threat to any Apple customers today. If you have information to the contrary, please present it.<p>What <i>new</i> attacks are possible upon device owners when the CSAM scanning of iCloud uploads is shifted to the device, that were not <i>already</i> a viable attack at any time in the past decade?
        </div>
        <div class="children">
            
        </div>
    </div>


            
        </div>
    </div>


            
        </div>
    </div>


            
        </div>
    </div>


            
                


    
    <div class="comment hasChildren">
        <p><a href="https://news.ycombinator.com/user?id=vnchr" target="_blank">vnchr</a>   <span class="timeago" data-date="2021-08-18 12:55:30 &#43;0000 UTC">2021-08-18 12:55:30 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            I would think a Message with the attached photo from a burner phone&#x2F;account would be enough.
        </div>
        <div class="children">
            
                


    
    <div class="comment hasChildren">
        <p><a href="https://news.ycombinator.com/user?id=manmal" target="_blank">manmal</a>   <span class="timeago" data-date="2021-08-18 13:15:18 &#43;0000 UTC">2021-08-18 13:15:18 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            Currently, the image would have to be imported into the photos library, and iCloud upload must be enabled.
        </div>
        <div class="children">
            
                


    
    <div class="comment hasChildren">
        <p><a href="https://news.ycombinator.com/user?id=zug_zug" target="_blank">zug_zug</a>   <span class="timeago" data-date="2021-08-18 13:21:53 &#43;0000 UTC">2021-08-18 13:21:53 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            This conversely means that all illegal content can be freely texted and this system won&#x27;t even catch the distribution of CP unless those pictures are imported into the photos library and icloud updates enabled.<p>There&#x27;s a pretty good chance that it was inevitably going to get expanded to handle pictures arriving at the phone through other means.
        </div>
        <div class="children">
            
                


    
    <div class="comment ">
        <p><a href="https://news.ycombinator.com/user?id=cyanite" target="_blank">cyanite</a>   <span class="timeago" data-date="2021-08-18 20:28:10 &#43;0000 UTC">2021-08-18 20:28:10 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            We can take that discussion if and when that happens.
        </div>
        <div class="children">
            
        </div>
    </div>


            
        </div>
    </div>


            
                


    
    <div class="comment hasChildren">
        <p><a href="https://news.ycombinator.com/user?id=Invictus0" target="_blank">Invictus0</a>   <span class="timeago" data-date="2021-08-18 15:34:36 &#43;0000 UTC">2021-08-18 15:34:36 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            Whatsapp has a feature where all images are automatically saved to device as they are received. If automatic iCloud upload is on, then all the conditions are there for a person to innocently click on a spammy Whatsapp message, see a bunch of nonsense grayscale images, and continue on with their day--not realizing they are now being monitored for CSAM.
        </div>
        <div class="children">
            
                


    
    <div class="comment ">
        <p><a href="https://news.ycombinator.com/user?id=manmal" target="_blank">manmal</a>   <span class="timeago" data-date="2021-08-18 20:15:26 &#43;0000 UTC">2021-08-18 20:15:26 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            I agree, that will become a problem.
        </div>
        <div class="children">
            
        </div>
    </div>


            
        </div>
    </div>


            
                


    
    <div class="comment ">
        <p><a href="https://news.ycombinator.com/user?id=dannyw" target="_blank">dannyw</a>   <span class="timeago" data-date="2021-08-18 16:36:48 &#43;0000 UTC">2021-08-18 16:36:48 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            Pegasus says hi.
        </div>
        <div class="children">
            
        </div>
    </div>


            
        </div>
    </div>


            
        </div>
    </div>


            
                


    
    <div class="comment hasChildren">
        <p><a href="https://news.ycombinator.com/user?id=varispeed" target="_blank">varispeed</a>   <span class="timeago" data-date="2021-08-18 12:55:01 &#43;0000 UTC">2021-08-18 12:55:01 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            &gt; that is clearly not<p>For one, you can&#x27;t know if that&#x27;s true as the image could have been manipulated to appear as such. For example you wouldn&#x27;t know if a kind of steganography has been used to hide image in an image and that neuralhash picked on a hidden image.<p>&gt; but I don&#x27;t understand how they can get it on someone&#x27;s phone<p>There is many vectors. For example you can leave phone unattended and someone can snap a picture of an image or since a collision may look innocent to you, you would overlook it in an email etc...
        </div>
        <div class="children">
            
                


    
    <div class="comment hasChildren">
        <p><a href="https://news.ycombinator.com/user?id=mukesh610" target="_blank">mukesh610</a>   <span class="timeago" data-date="2021-08-18 13:23:37 &#43;0000 UTC">2021-08-18 13:23:37 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            &gt; For example you wouldn&#x27;t know if a kind of steganography has been used to hide image in an image and that neuralhash picked on a hidden image.<p>How would NeuralHash pick a &quot;hidden image&quot;? It only uses the pixels of the image to get the hash. Any hidden image in the metadata would not even be picked up and no amount of steganography can fool NeuralHash.<p>&gt; There is many vectors. For example you can leave phone unattended and someone can snap a picture of an image or since a collision may look innocent to you, you would overlook it in an email etc...<p>As iterated elsewhere in this thread, random gibberish pixels colliding with CSAM would definitely not be useful in incriminating anyone. The manual process would catch that. Also, if the manual process is overloaded, I&#x27;m pretty sure basic object recognition can filter out most of the colliding gibberish .
        </div>
        <div class="children">
            
                


    
    <div class="comment ">
        <p><a href="https://news.ycombinator.com/user?id=varispeed" target="_blank">varispeed</a>   <span class="timeago" data-date="2021-08-18 15:16:04 &#43;0000 UTC">2021-08-18 15:16:04 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            The steganography is not to fool the NeuralHash, but to fool the viewer.<p>The NeuralHash would &quot;see&quot; the planted image, but for the viewer it would appear innocent.<p>I am trying to say that a person reviewing image manually, without special tools will not be able to tell if the image is a false positive and would have to report everything.
        </div>
        <div class="children">
            
        </div>
    </div>


            
        </div>
    </div>


            
        </div>
    </div>


            
        </div>
    </div>


        
            


    
    <div class="comment hasChildren">
        <p><a href="https://news.ycombinator.com/user?id=reacharavindh" target="_blank">reacharavindh</a>   <span class="timeago" data-date="2021-08-18 12:11:44 &#43;0000 UTC">2021-08-18 12:11:44 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            I admit that I have not done enough research to have a strong opinion on this, but why is Apple taking this on themselves? As far as I can see, this outrage is because of &quot;scanning on iPhone&quot; that is wildly out of user&#x27;s control. Why can&#x27;t Apple be like others and say we scan the shit out of what you upload to iCloud(and it is in our Terms and Conditions to use iCloud)?<p>Almost all tech people know that iCloud (or its backups) are not encrypted, so they can decrypt it as they wish.. and those among us that are privacy conscious can comfortable turn iCloud OFF(or not sign up for one) and use the iPhone as a simple private device?
        </div>
        <div class="children">
            
                


    
    <div class="comment hasChildren">
        <p><a href="https://news.ycombinator.com/user?id=helen___keller" target="_blank">helen___keller</a>   <span class="timeago" data-date="2021-08-18 13:19:38 &#43;0000 UTC">2021-08-18 13:19:38 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            My understanding is that iCloud reports orders of magnitudes less CSAM than other cloud services at a similar scale. My guess is that apple wanted a way to report the CSAM they are currently storing without having to decrypt&#x2F;inspect every person&#x27;s personal data (which necessarily opens vectors for e.g. rogue employees doing bad things with people&#x27;s personal data)<p>Hence why this approach was stated as a privacy win by Apple. They catch the CSAM and they don&#x27;t have to look at your photos stored online.
        </div>
        <div class="children">
            
                


    
    <div class="comment ">
        <p><a href="https://news.ycombinator.com/user?id=cyanite" target="_blank">cyanite</a>   <span class="timeago" data-date="2021-08-18 20:29:44 &#43;0000 UTC">2021-08-18 20:29:44 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            Yes, and if you read (and understand) the technical implementation, it certainly does offer more privacy. But yeah for people who don’t, it often seems much worse.
        </div>
        <div class="children">
            
        </div>
    </div>


            
        </div>
    </div>


            
                


    
    <div class="comment hasChildren">
        <p><a href="https://news.ycombinator.com/user?id=jefftk" target="_blank">jefftk</a>   <span class="timeago" data-date="2021-08-18 12:19:27 &#43;0000 UTC">2021-08-18 12:19:27 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            The speculation that I&#x27;ve seen here is that Apple is rolling this out ahead of a future announcement that iCloud uploads will be encrypted.
        </div>
        <div class="children">
            
                


    
    <div class="comment hasChildren">
        <p><a href="https://news.ycombinator.com/user?id=bitneuker" target="_blank">bitneuker</a>   <span class="timeago" data-date="2021-08-18 13:09:22 &#43;0000 UTC">2021-08-18 13:09:22 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            Are they not? I would&#x27;ve thought that with Apple advertising privacy and security they would at least encrypt iCloud uploads.
        </div>
        <div class="children">
            
                


    
    <div class="comment ">
        <p><a href="https://news.ycombinator.com/user?id=reacharavindh" target="_blank">reacharavindh</a>   <span class="timeago" data-date="2021-08-19 09:44:15 &#43;0000 UTC">2021-08-19 09:44:15 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            Apple has the decryption keys to iCloud... It is one of those things that many people assume the other way given Apple&#x27;s marketing and PR stands.<p>I for one am aware of this with iCloud and am still using iCloud for convenience (and by choice). If I were in need of better privacy, I can always use the iPhone without iCloud, and it will work. After this &quot;in phone&quot; watchdog implementation, that will not be the case. I will assume that Apple is constantly watching all my unencrypted content in the worst case, on behalf of state actors and intelligence agencies.<p>We have seen Apple give in to the Chinese government spying because it is the law there. US government could easily ask for this and also apply a gag order preventing Apple from being able to tell the user. The best situation is to lack such a tool.
        </div>
        <div class="children">
            
        </div>
    </div>


            
                


    
    <div class="comment ">
        <p><a href="https://news.ycombinator.com/user?id=davidcbc" target="_blank">davidcbc</a>   <span class="timeago" data-date="2021-08-18 13:42:08 &#43;0000 UTC">2021-08-18 13:42:08 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            iCloud uploads are encrypted in transit and on their servers, but they are not E2E encrypted. Apple has the decryption keys
        </div>
        <div class="children">
            
        </div>
    </div>


            
        </div>
    </div>


            
                


    
    <div class="comment hasChildren">
        <p><a href="https://news.ycombinator.com/user?id=zug_zug" target="_blank">zug_zug</a>   <span class="timeago" data-date="2021-08-18 13:18:09 &#43;0000 UTC">2021-08-18 13:18:09 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            If so they flubbed it.<p>If they are e2e encrypted they can&#x27;t be distributed&#x2F;shared without giving a key of some sort. Why not just scan them at time of distribution, and treat undistributed files the same as any local hard-drive (i.e. not their problem).
        </div>
        <div class="children">
            
                


    
    <div class="comment ">
        <p><a href="https://news.ycombinator.com/user?id=cyanite" target="_blank">cyanite</a>   <span class="timeago" data-date="2021-08-18 20:30:28 &#43;0000 UTC">2021-08-18 20:30:28 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            This system could still work with an e2e encrypted cloud library. However, it’s currently not e2e.
        </div>
        <div class="children">
            
        </div>
    </div>


            
        </div>
    </div>


            
        </div>
    </div>


            
        </div>
    </div>


        
            


    
    <div class="comment hasChildren">
        <p><a href="https://news.ycombinator.com/user?id=neximo64" target="_blank">neximo64</a>   <span class="timeago" data-date="2021-08-18 10:26:59 &#43;0000 UTC">2021-08-18 10:26:59 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            Maybe the best idea is for a sufficient number of people to replicate the hashes with collisions from the CSAM database and make copies of photos with nothing in them like this one and just let Apple deal with it. Maybe it can have text too.
        </div>
        <div class="children">
            
                


    
    <div class="comment ">
        <p><a href="https://news.ycombinator.com/user?id=cyanite" target="_blank">cyanite</a>   <span class="timeago" data-date="2021-08-18 21:09:41 &#43;0000 UTC">2021-08-18 21:09:41 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            Best for whom? (Also, we don’t know the exact hash list Apple will use).
        </div>
        <div class="children">
            
        </div>
    </div>


            
        </div>
    </div>


        
            


    
    <div class="comment ">
        <p><a href="https://news.ycombinator.com/user?id=jakear" target="_blank">jakear</a>   <span class="timeago" data-date="2021-08-18 15:32:18 &#43;0000 UTC">2021-08-18 15:32:18 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            Baseless speculation: this is less about ongoing protection against keeping CSAM on their servers and more about a one-time sting operation requested by some sort of acronym’d official. The basic idea being: have a bunch of catfish agents send out these known CSAM images to folks through anonymous channels (sourcing the targets would likely be unfortunately trivial), expect that some of them will save the images and be synced to iCloud, coerce Apple into letting them see who has the material, sting them.<p>To this end, I would not be at all surprised to see that in some not-too-distant future Apple issues a big ol’ public apology and removes this feature. The operation will of course be long complete by then.<p>Just writing this out here so I have a “I told you so” link for if&#x2F;when that time comes :)
        </div>
        <div class="children">
            
        </div>
    </div>


        
            


    
    <div class="comment ">
        <p><a href="https://news.ycombinator.com/user?id=azinman2" target="_blank">azinman2</a>   <span class="timeago" data-date="2021-08-18 16:38:33 &#43;0000 UTC">2021-08-18 16:38:33 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            It’s not enough to have collisions — you’ll have to have collisions against ANOTHER hidden model for the same photos, and then have it pass a human looking at it. People are getting unnecessarily angry over this.
        </div>
        <div class="children">
            
        </div>
    </div>


        
            


    
    <div class="comment hasChildren">
        <p><a href="https://news.ycombinator.com/user?id=avsteele" target="_blank">avsteele</a>   <span class="timeago" data-date="2021-08-18 12:08:31 &#43;0000 UTC">2021-08-18 12:08:31 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            Apple claimed &#x27;one in a  trillion&#x27; chance of a collision.  This is a great example of why you should not trust such an assessment.
        </div>
        <div class="children">
            
                


    
    <div class="comment ">
        <p><a href="https://news.ycombinator.com/user?id=manmal" target="_blank">manmal</a>   <span class="timeago" data-date="2021-08-18 13:17:34 &#43;0000 UTC">2021-08-18 13:17:34 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            That number is based around their plan to only trigger a search if a certain number of CSAM hashes were detected on a phone. A single collision wouldn&#x27;t trigger such a search. A sufficiently motivated person can of course get access to a phone and put plenty hash-colliding images in the library.
        </div>
        <div class="children">
            
        </div>
    </div>


            
                


    
    <div class="comment ">
        <p><a href="https://news.ycombinator.com/user?id=arsome" target="_blank">arsome</a>   <span class="timeago" data-date="2021-08-18 12:57:22 &#43;0000 UTC">2021-08-18 12:57:22 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            To be fair, log_2(1 trillion) is only about 40 bits.
        </div>
        <div class="children">
            
        </div>
    </div>


            
                


    
    <div class="comment ">
        <p><a href="https://news.ycombinator.com/user?id=cyanite" target="_blank">cyanite</a>   <span class="timeago" data-date="2021-08-18 20:34:10 &#43;0000 UTC">2021-08-18 20:34:10 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            That’s a different scenario. That’s for it happening by chance, not on purpose. At any rate, the “matching” images are reviewed (or a “visual derivative” is).
        </div>
        <div class="children">
            
        </div>
    </div>


            
        </div>
    </div>


        
            


    
    <div class="comment hasChildren">
        <p><a href="https://news.ycombinator.com/user?id=newArray" target="_blank">newArray</a>   <span class="timeago" data-date="2021-08-18 15:15:05 &#43;0000 UTC">2021-08-18 15:15:05 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            Hash collisions happen by design in the perceptual hash, its supposed to give equal hashes for small changes after all.<p>Something I find interesting is the necessary consequences of the property of small edits resulting in the same hash. We can show that this is impossible to absolutely achieve, or in other words there must exist an image such that changing a single pixel will change the hash.<p>Proof: Start with 2 images, A and B, of equal dimension, and with different perceptual hashes h(A) and h(B). Transform one pixel of A into the corresponding pixel of B and recompute h(A). At some point, after a single pixel change, h(A) = h(B), this is guaranteed to happen before or at A = B. Now A and the previous version of A have are 1 pixel apart, but have different hashes. QED<p>We can also ATTEMPT to create an image A with a specified hash matching h(A_initial) but which is visually similar to a target image B. Again start with A and B, different images with same dimensions. Transform a random pixel of A towards a pixel of B, but discard the change if h(A) changes from h(A_initial). Since we have so many degrees of freedom for our edit at any point (each channel of each pixel) and the perceptual hash invariant is in our favor, it may be possible to maneuver A close enough to B to fool a person, and keep h(A) = h(A_initial).<p>If this is possible one could transform a given CSAM image into a harmless meme while not changing the hash, spread the crafted image, and get tons of iCloud accounts flagged.
        </div>
        <div class="children">
            
                


    
    <div class="comment ">
        <p><a href="https://news.ycombinator.com/user?id=mrits" target="_blank">mrits</a>   <span class="timeago" data-date="2021-08-18 15:18:04 &#43;0000 UTC">2021-08-18 15:18:04 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            The best part of this is we could all be flagged in a database for things like this and not even realize it. Who knows what kind of review process the governments are actually keeping around these
        </div>
        <div class="children">
            
        </div>
    </div>


            
        </div>
    </div>


        
            


    
    <div class="comment hasChildren">
        <p><a href="https://news.ycombinator.com/user?id=deadalus" target="_blank">deadalus</a>   <span class="timeago" data-date="2021-08-18 09:52:59 &#43;0000 UTC">2021-08-18 09:52:59 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            Expectation : Political rivals and enemies of powerful people will be taken out because c-ild pornography will be found in their phone. Pegasus can already monitor and exfiltrate every ounce of data right now, it won&#x27;t be that hard to insert compromising images on the infected device.<p>Any news about &quot;c-ild porn&quot; being found on someone&#x27;s phone is suspect now. This has been done before :<p>1) <a href="https:&#x2F;&#x2F;www.deccanchronicle.com&#x2F;technology&#x2F;in-other-news&#x2F;120117&#x2F;beware-hackers-are-planting-child-porn-on-peoples-computers.html" rel="nofollow">https:&#x2F;&#x2F;www.deccanchronicle.com&#x2F;technology&#x2F;in-other-news&#x2F;120...</a><p>2) <a href="https:&#x2F;&#x2F;www.independent.co.uk&#x2F;news&#x2F;uk&#x2F;crime&#x2F;handyman-planted-child-porn-colleague-s-computer-2037439.html" rel="nofollow">https:&#x2F;&#x2F;www.independent.co.uk&#x2F;news&#x2F;uk&#x2F;crime&#x2F;handyman-planted...</a><p>3) <a href="https:&#x2F;&#x2F;www.theatlantic.com&#x2F;notes&#x2F;2015&#x2F;09&#x2F;how-easily-can-hackers-plant-evidence-contd&#x2F;405133&#x2F;" rel="nofollow">https:&#x2F;&#x2F;www.theatlantic.com&#x2F;notes&#x2F;2015&#x2F;09&#x2F;how-easily-can-hac...</a><p>4) <a href="https:&#x2F;&#x2F;www.nytimes.com&#x2F;2016&#x2F;12&#x2F;09&#x2F;world&#x2F;europe&#x2F;vladimir-putin-russia-fake-news-hacking-cybersecurity.html" rel="nofollow">https:&#x2F;&#x2F;www.nytimes.com&#x2F;2016&#x2F;12&#x2F;09&#x2F;world&#x2F;europe&#x2F;vladimir-put...</a><p>5) <a href="https:&#x2F;&#x2F;www.cnet.com&#x2F;tech&#x2F;services-and-software&#x2F;a-child-porn-planting-virus-threat-or-bad-defense&#x2F;" rel="nofollow">https:&#x2F;&#x2F;www.cnet.com&#x2F;tech&#x2F;services-and-software&#x2F;a-child-porn...</a>
        </div>
        <div class="children">
            
                


    
    <div class="comment hasChildren">
        <p><a href="https://news.ycombinator.com/user?id=bko" target="_blank">bko</a>   <span class="timeago" data-date="2021-08-18 10:21:08 &#43;0000 UTC">2021-08-18 10:21:08 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            Isn&#x27;t it weird how it is weaponized against political enemies but the one person everyone knows did engage in exploitation was protected for decades?
        </div>
        <div class="children">
            
                


    
    <div class="comment hasChildren">
        <p><a href="https://news.ycombinator.com/user?id=dijit" target="_blank">dijit</a>   <span class="timeago" data-date="2021-08-18 10:31:51 &#43;0000 UTC">2021-08-18 10:31:51 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            I don&#x27;t think that&#x27;s weird.<p>Those people are not political enemies, they&#x27;re allies for whatever regime and their support is important.<p>Better to keep someone powerful in your pocket than to oust them and let someone uncorruptible or uncompromising assume the power vacuum.
        </div>
        <div class="children">
            
                


    
    <div class="comment hasChildren">
        <p><a href="https://news.ycombinator.com/user?id=LeoNatan25" target="_blank">LeoNatan25</a>   <span class="timeago" data-date="2021-08-18 10:50:14 &#43;0000 UTC">2021-08-18 10:50:14 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            No one is “ uncorruptible or uncompromising”. They just have to start looking from scratch.
        </div>
        <div class="children">
            
                


    
    <div class="comment ">
        <p><a href="https://news.ycombinator.com/user?id=dijit" target="_blank">dijit</a>   <span class="timeago" data-date="2021-08-18 10:55:11 &#43;0000 UTC">2021-08-18 10:55:11 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            There are many people who are incorruptible or uncompromising, we usually dislike them.<p>Maybe &quot;not sympathetic&quot; would prevent me from being nerd sniped. :)
        </div>
        <div class="children">
            
        </div>
    </div>


            
        </div>
    </div>


            
        </div>
    </div>


            
                


    
    <div class="comment ">
        <p><a href="https://news.ycombinator.com/user?id=newsclues" target="_blank">newsclues</a>   <span class="timeago" data-date="2021-08-18 11:27:55 &#43;0000 UTC">2021-08-18 11:27:55 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            Selective prosecution.
        </div>
        <div class="children">
            
        </div>
    </div>


            
                


    
    <div class="comment ">
        <p><a href="https://news.ycombinator.com/user?id=draw_down" target="_blank">draw_down</a>   <span class="timeago" data-date="2021-08-18 10:28:04 &#43;0000 UTC">2021-08-18 10:28:04 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            More par for the course, I’d say.
        </div>
        <div class="children">
            
        </div>
    </div>


            
        </div>
    </div>


            
                


    
    <div class="comment ">
        <p><a href="https://news.ycombinator.com/user?id=raxxorrax" target="_blank">raxxorrax</a>   <span class="timeago" data-date="2021-08-18 12:47:05 &#43;0000 UTC">2021-08-18 12:47:05 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            That you felt the need to kill the &#x27;h&#x27; in child is telling enough. Let&#x27;s face it, security policy from the last 20 years was created by idiots. Plain borderline mentally inhibited idiots. All the surviellance introduced so much mistrust in some states that no terrorist could have ever dreamed of. There is no rational cost-benefit analysis, only propaganda about some Ivan being allegedly worse. Yes, great metric...<p>Sorry for the rant.
        </div>
        <div class="children">
            
        </div>
    </div>


            
                


    
    <div class="comment hasChildren">
        <p><a href="https://news.ycombinator.com/user?id=Rd6n6" target="_blank">Rd6n6</a>   <span class="timeago" data-date="2021-08-18 12:23:33 &#43;0000 UTC">2021-08-18 12:23:33 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            That image planting virus concept is the scariest thing. Could you trigger this system and get the police alerted with spam mms with images or spam email with images, or targeted ads that get the gray blob images into your tmp files? Or does it have to be an actual script downloading images, say, one per week in secret until it triggers?
        </div>
        <div class="children">
            
                


    
    <div class="comment hasChildren">
        <p><a href="https://news.ycombinator.com/user?id=robertoandred" target="_blank">robertoandred</a>   <span class="timeago" data-date="2021-08-18 15:04:00 &#43;0000 UTC">2021-08-18 15:04:00 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            No, you couldn&#x27;t. This is only checking images you upload to your iCloud photo library.<p>Why would you save tons of gray blobs to your photo library? Why would gray blobs look like child porn? Why would Apple reviewers think gray blobs are child porn? Why would the NCMEC think gray blobs are child porn? Why would law enforcement spend time arresting someone for gray blobs?
        </div>
        <div class="children">
            
                


    
    <div class="comment hasChildren">
        <p><a href="https://news.ycombinator.com/user?id=nullc" target="_blank">nullc</a>   <span class="timeago" data-date="2021-08-18 16:40:20 &#43;0000 UTC">2021-08-18 16:40:20 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            The grey blob is a proof of concept.  The existence of the original image is proof that not all images which produce the target hash are grey blobs.<p>Since the grey blob exists, I believe it is fully possible to construct natural(-ish) images that have a selected hash.<p>So, you should perhaps instead imagine attackers that modify lawful nude images to have matching hashes with child porn images.<p>With that in mind, most of your questions are answered, except perhaps for &quot;Why would the NCMEC think&quot;-- and the answer would be &quot;because its porn and the computer says its child porn&quot;.<p>Of course, an attacker could just as well use REAL child porn. But there are logistic advantages in having to do less handling of unlawful material, and it&#x27;s less likely that the target will notice.  Imagine: if the target already has nude images on their host the attacker might use those as the templates.  I doubt most people would notice if their nude image collection was replaced with noisier versions of the same stuff.  And even if they did notice, &quot;someone is trying to frame me for child porn&quot; wouldn&#x27;t be their first guess. :)
        </div>
        <div class="children">
            
                


    
    <div class="comment hasChildren">
        <p><a href="https://news.ycombinator.com/user?id=hannasanarion" target="_blank">hannasanarion</a>   <span class="timeago" data-date="2021-08-18 18:48:45 &#43;0000 UTC">2021-08-18 18:48:45 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            The other thread demonstrates a similar attack with pictures of dogs.<p>The same questions still apply. Why would you save tons of pictures of dogs to your photo library? Why would pictures of dogs look like child porn? Why would Apple reviewers think pictures of dogs are child porn? Why would the NCMEC think pictures of dogs are child porn? Why would law enforcement spend time arresting someone for pictures of dogs?<p>What is the scenario where someone can be harmed reputationally or criminally because of a hash collision attack, where the same attack could not be performed more easily and causing more damage by actually using real CSAM images?
        </div>
        <div class="children">
            
                


    
    <div class="comment hasChildren">
        <p><a href="https://news.ycombinator.com/user?id=nullc" target="_blank">nullc</a>   <span class="timeago" data-date="2021-08-18 20:54:50 &#43;0000 UTC">2021-08-18 20:54:50 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            I&#x27;m really surprised to see a fellow HN poster making such a stark failure to generalize.<p>You see that they can do this with pictures of dogs.  What makes you think they can&#x27;t do exactly the same with pictures of crotches?<p>Presumably you don&#x27;t believe there is some kind inherent dog-nature that makes dog images more likely to undermine the hash.  :)  People are using pictures of dogs because they are a tasteful safe-for-work example, not because anyone actually imagines using images of dogs.<p>An actual attack would use ordinary nude images, probably ones selected so that if you were primed to expect child porn you&#x27;d believe it if you didn&#x27;t spend a while studying the image.<p>&gt; where the same attack could not be performed more easily and causing more damage by actually using real CSAM images?<p>Actual child porn images are more likely to get deleted by the target and&#x2F;or reported by the target.  The attacker also takes on some additional risk that their possession of the images is a strict liability crime.  This means that if the planting party gets found with the real child porn they&#x27;ll be in trouble vs with the hash-matching-legal-images they&#x27;ll only be in trouble if they get caught planting it (and potentially only exposed to a civil lawsuit, rather than a felony, depending on how they were planting it).<p>Personally, I agree that the second-preimage-images are not the most interesting attack!  But they are a weakness that makes the system even more dangerous.  We could debate how much more dangerous they make it.
        </div>
        <div class="children">
            
                


    
    <div class="comment hasChildren">
        <p><a href="https://news.ycombinator.com/user?id=hannasanarion" target="_blank">hannasanarion</a>   <span class="timeago" data-date="2021-08-18 21:54:01 &#43;0000 UTC">2021-08-18 21:54:01 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            What kind of non-child-porn imagery is so similar to child porn that it fools the NCMEC investigator who is looking at it side-by-side with its maliciously-collisioned hash match, but is at the same time so so <i>dissimilar</i> from real child porn that, as you say, the target won&#x27;t report its sudden appearance on their device and the the sender won&#x27;t be at legal risk? Your scenario requires the image (not the hash, but the regular human-perceptible image) to be both unremarkably innocent and incontrovertibly damning at the same time.
        </div>
        <div class="children">
            
                


    
    <div class="comment hasChildren">
        <p><a href="https://news.ycombinator.com/user?id=nullc" target="_blank">nullc</a>   <span class="timeago" data-date="2021-08-19 00:17:51 &#43;0000 UTC">2021-08-19 00:17:51 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            You&#x27;re assuming that there is a side-by-side comparison with a hash matching image.  I think that is an extremely big assumption which assumes facts not in evidence at all.<p>As far as what kind of image would be believed to be child porn without a side-by-side with the supposed match: ordinary porn. Without context plenty would be hard to distinguish, especially with the popularity of waxed smooth bodies and explicit close up shots.<p>Prosecution over &quot;child porn&quot; that isn&#x27;t is already a proved thing, with instances where the government was happily trotting out &#x27;experts&#x27; to claim that the images were clearly a child based on physical characteristics only to be rebuffed by the actual actress taking the stand. ( <a href="https:&#x2F;&#x2F;www.crimeandfederalism.com&#x2F;page&#x2F;61&#x2F;" rel="nofollow">https:&#x2F;&#x2F;www.crimeandfederalism.com&#x2F;page&#x2F;61&#x2F;</a> )
        </div>
        <div class="children">
            
                


    
    <div class="comment ">
        <p><a href="https://news.ycombinator.com/user?id=hannasanarion" target="_blank">hannasanarion</a>   <span class="timeago" data-date="2021-08-19 15:55:27 &#43;0000 UTC">2021-08-19 15:55:27 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            You still haven&#x27;t explained how an image can be at the same time so benign that the user doesn&#x27;t delete it from their phone and cloud service and the sender is in no legal danger, and yet so obviously pornographic that a jury will be convinced beyond any reasonable doubt that it&#x27;s child porn.<p>The entire point of this process is to catch only images from a known catalogue of existing images, of course there will be opportunity for side-by-side comparison. And if the side-by-side comparison can convince a jury that the images are the same, then the result is literally identical to if you had just sent the original image, the hash checking service hasn&#x27;t impacted the attack whatsoever.<p>The idea that new images of nudity could be caught in it is because of a mixup by the press after it was announced, because people confused it with a <i>different</i> parental control feature that detects nudity in images taken by your kids&#x27; phones. This is not the same thing.
        </div>
        <div class="children">
            
        </div>
    </div>


            
        </div>
    </div>


            
        </div>
    </div>


            
        </div>
    </div>


            
        </div>
    </div>


            
                


    
    <div class="comment ">
        <p><a href="https://news.ycombinator.com/user?id=fouric" target="_blank">fouric</a>   <span class="timeago" data-date="2021-08-18 17:50:32 &#43;0000 UTC">2021-08-18 17:50:32 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            &gt; Since the grey blob exists, I believe it is fully possible to construct natural(-ish) images that have a selected hash.<p>At the very least, there are large classes of images that, while not realistic photographs, people will willingly download, that should be very easy to craft collisions for:<p>Memes. In particular, given the distortions of surreal and deep-fried memes, it should be pretty easy to craft collisions there.
        </div>
        <div class="children">
            
        </div>
    </div>


            
                


    
    <div class="comment ">
        <p><a href="https://news.ycombinator.com/user?id=nullc" target="_blank">nullc</a>   <span class="timeago" data-date="2021-08-18 20:56:00 &#43;0000 UTC">2021-08-18 20:56:00 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            Update: The latest results now have second-preimages that are other photos, not just noise. <a href="https:&#x2F;&#x2F;github.com&#x2F;AsuharietYgvar&#x2F;AppleNeuralHash2ONNX&#x2F;issues&#x2F;1#issuecomment-901360955" rel="nofollow">https:&#x2F;&#x2F;github.com&#x2F;AsuharietYgvar&#x2F;AppleNeuralHash2ONNX&#x2F;issue...</a>   They still look pretty bad, but attacks only get better!
        </div>
        <div class="children">
            
        </div>
    </div>


            
                


    
    <div class="comment hasChildren">
        <p><a href="https://news.ycombinator.com/user?id=robertoandred" target="_blank">robertoandred</a>   <span class="timeago" data-date="2021-08-18 17:49:27 &#43;0000 UTC">2021-08-18 17:49:27 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            And where would they get the CSAM hash they need to match?
        </div>
        <div class="children">
            
                


    
    <div class="comment ">
        <p><a href="https://news.ycombinator.com/user?id=nullc" target="_blank">nullc</a>   <span class="timeago" data-date="2021-08-18 20:46:08 &#43;0000 UTC">2021-08-18 20:46:08 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            The bulk of the CSAM hash database comes widely circulated images, available on sleeze dark web websites (and historically on sketchy porn sites just on the web).<p>So someone need only find a collection of likely included images and compute their hashes and publish it.<p>The attack works just as well even if the attacker isn&#x27;t sure that every hash is in the database, they just need to know enough likely-matches such that they get enough hits.<p>Consider it this way, some attacker spends a couple hours searching for child porn and finds some stuff.  ... what kind of failure would the NCMEC database be if it <i>didn&#x27;t</i> include that material?
        </div>
        <div class="children">
            
        </div>
    </div>


            
        </div>
    </div>


            
        </div>
    </div>


            
                


    
    <div class="comment ">
        <p><a href="https://news.ycombinator.com/user?id=" target="_blank"></a>   <span class="timeago" data-date="2021-08-18 15:36:39 &#43;0000 UTC">2021-08-18 15:36:39 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            
        </div>
        <div class="children">
            
        </div>
    </div>


            
        </div>
    </div>


            
        </div>
    </div>


            
                


    
    <div class="comment ">
        <p><a href="https://news.ycombinator.com/user?id=Crontab" target="_blank">Crontab</a>   <span class="timeago" data-date="2021-08-18 11:56:03 &#43;0000 UTC">2021-08-18 11:56:03 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            I remember reading the evil handyman story (link 2) a while ago and it scared me into using FDE. (Of course, FDE doesn&#x27;t solve malware and things like that.)
        </div>
        <div class="children">
            
        </div>
    </div>


            
                


    
    <div class="comment hasChildren">
        <p><a href="https://news.ycombinator.com/user?id=snarf21" target="_blank">snarf21</a>   <span class="timeago" data-date="2021-08-18 12:54:53 &#43;0000 UTC">2021-08-18 12:54:53 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            I&#x27;m against this change from Apple but weren&#x27;t they already doing this scanning in iCloud? Couldn&#x27;t someone use it the same way against a &quot;political rival and enemies of powerful people&quot;?
        </div>
        <div class="children">
            
                


    
    <div class="comment ">
        <p><a href="https://news.ycombinator.com/user?id=cyanite" target="_blank">cyanite</a>   <span class="timeago" data-date="2021-08-18 20:35:28 &#43;0000 UTC">2021-08-18 20:35:28 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            All major cloud photo library providers do this scan server side, yes. This way offers more privacy for the end user… but evidently most people don’t realize that.
        </div>
        <div class="children">
            
        </div>
    </div>


            
        </div>
    </div>


            
                


    
    <div class="comment ">
        <p><a href="https://news.ycombinator.com/user?id=" target="_blank"></a>   <span class="timeago" data-date="2021-08-18 12:54:14 &#43;0000 UTC">2021-08-18 12:54:14 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            
        </div>
        <div class="children">
            
        </div>
    </div>


            
                


    
    <div class="comment ">
        <p><a href="https://news.ycombinator.com/user?id=coldtea" target="_blank">coldtea</a>   <span class="timeago" data-date="2021-08-18 12:15:08 &#43;0000 UTC">2021-08-18 12:15:08 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            You think such a thing can happen to the best of all possible worlds, where the powerful always abide by the law and never have politicians in their pockets, and the state only serves and never defames, threatens, or blackmails people?
        </div>
        <div class="children">
            
        </div>
    </div>


            
                


    
    <div class="comment ">
        <p><a href="https://news.ycombinator.com/user?id=hanklazard" target="_blank">hanklazard</a>   <span class="timeago" data-date="2021-08-18 11:40:17 &#43;0000 UTC">2021-08-18 11:40:17 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            And as an extra horrible side-effect, the use of these kinds of attacks in the political arena will “vindicate” believers of the QAnon conspiracy.
        </div>
        <div class="children">
            
        </div>
    </div>


            
        </div>
    </div>


        
            


    
    <div class="comment hasChildren">
        <p><a href="https://news.ycombinator.com/user?id=pkulak" target="_blank">pkulak</a>   <span class="timeago" data-date="2021-08-18 16:26:36 &#43;0000 UTC">2021-08-18 16:26:36 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            If your first thought, like mine, was &quot;Who cares? The whole point is that there will be plenty of false positives.&quot;, this is pre-image:<p><a href="https:&#x2F;&#x2F;github.com&#x2F;AsuharietYgvar&#x2F;AppleNeuralHash2ONNX&#x2F;issues&#x2F;1#issuecomment-901016864" rel="nofollow">https:&#x2F;&#x2F;github.com&#x2F;AsuharietYgvar&#x2F;AppleNeuralHash2ONNX&#x2F;issue...</a>
        </div>
        <div class="children">
            
                


    
    <div class="comment ">
        <p><a href="https://news.ycombinator.com/user?id=fogof" target="_blank">fogof</a>   <span class="timeago" data-date="2021-08-18 19:25:21 &#43;0000 UTC">2021-08-18 19:25:21 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            That was, indeed, my first thought. Can you explain why reposting the link that the OP posted is supposed to change my mind, or how it contributes to the conversation?
        </div>
        <div class="children">
            
        </div>
    </div>


            
                


    
    <div class="comment ">
        <p><a href="https://news.ycombinator.com/user?id=fortenforge" target="_blank">fortenforge</a>   <span class="timeago" data-date="2021-08-18 18:26:37 &#43;0000 UTC">2021-08-18 18:26:37 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            I don&#x27;t think that&#x27;s true. This was a second pre-image attack.
        </div>
        <div class="children">
            
        </div>
    </div>


            
        </div>
    </div>


        
            


    
    <div class="comment hasChildren">
        <p><a href="https://news.ycombinator.com/user?id=yayr" target="_blank">yayr</a>   <span class="timeago" data-date="2021-08-18 11:54:01 &#43;0000 UTC">2021-08-18 11:54:01 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            How long did it take now to make the Apple algorithm ultimately useless or even harmful?<p>Apple announcement of neural hashing: 5.8.2021.<p>Generic algorithm to generate a different matching image: 8.8.2021.<p>one script was already released 10 days ago here <a href="https:&#x2F;&#x2F;gist.github.com&#x2F;unrealwill&#x2F;c480371c3a4bf3abb29856c29197c0be" rel="nofollow">https:&#x2F;&#x2F;gist.github.com&#x2F;unrealwill&#x2F;c480371c3a4bf3abb29856c29...</a>
        </div>
        <div class="children">
            
                


    
    <div class="comment hasChildren">
        <p><a href="https://news.ycombinator.com/user?id=cyanite" target="_blank">cyanite</a>   <span class="timeago" data-date="2021-08-18 21:11:52 &#43;0000 UTC">2021-08-18 21:11:52 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            None of this makes the system useless or harmful. Also, it’s not Apple’s algorithm. The actual hash list Apple will use is not accessible to the device.
        </div>
        <div class="children">
            
                


    
    <div class="comment hasChildren">
        <p><a href="https://news.ycombinator.com/user?id=yayr" target="_blank">yayr</a>   <span class="timeago" data-date="2021-08-18 21:29:48 &#43;0000 UTC">2021-08-18 21:29:48 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            If anybody with enough motivation can modify any existing harmless image to have the same neural hash as a &quot;tracked database&quot; image this will create too many false positives. Too many false positives make the algorithm useless.<p>If someone with even more motivation and the means to put those images onto your device via social engineering, exploits or maybe even features and you become the target of a criminal investigation in any jurisdiction you just happen to be at the moment, this makes the algorithm harmful.
        </div>
        <div class="children">
            
                


    
    <div class="comment ">
        <p><a href="https://news.ycombinator.com/user?id=FabHK" target="_blank">FabHK</a>   <span class="timeago" data-date="2021-08-19 00:08:15 &#43;0000 UTC">2021-08-19 00:08:15 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            &gt; this will create too many false positives. Too many false positives make the algorithm useless<p>Apple can (and will) run a second algorithm server side to filter out further false positives.<p>&gt; If someone with even more motivation and the means to put those images onto your device via social engineering, exploits or maybe even features<p>Such an attacker could just plant CSAM directly. The hash collision has no bearing on it. If, however, it is hash collision you&#x27;re worried about, they&#x27;d be caught during the manual review.
        </div>
        <div class="children">
            
        </div>
    </div>


            
        </div>
    </div>


            
        </div>
    </div>


            
        </div>
    </div>


        
            


    
    <div class="comment ">
        <p><a href="https://news.ycombinator.com/user?id=" target="_blank"></a>   <span class="timeago" data-date="2021-08-18 14:30:38 &#43;0000 UTC">2021-08-18 14:30:38 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            
        </div>
        <div class="children">
            
        </div>
    </div>


        
            


    
    <div class="comment hasChildren">
        <p><a href="https://news.ycombinator.com/user?id=yeldarb" target="_blank">yeldarb</a>   <span class="timeago" data-date="2021-08-18 16:02:35 &#43;0000 UTC">2021-08-18 16:02:35 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            I was curious how big of a deal this would be in a real-world attack (eg could someone use this to DDoS Apple&#x27;s human reviewers?) so I ran the colliding images through another feature extraction model, OpenAI&#x27;s CLIP, to see if they also fooled it.<p>They don&#x27;t; I think it&#x27;d be much harder to create an image that both matches the NeuralHash of a CSAM image and also fools a generic model like CLIP as a sanity check for being a CSAM image.<p>I wrote up my findings here: <a href="https:&#x2F;&#x2F;blog.roboflow.com&#x2F;apples-csam-neuralhash-collision&#x2F;" rel="nofollow">https:&#x2F;&#x2F;blog.roboflow.com&#x2F;apples-csam-neuralhash-collision&#x2F;</a>
        </div>
        <div class="children">
            
                


    
    <div class="comment ">
        <p><a href="https://news.ycombinator.com/user?id=dannyw" target="_blank">dannyw</a>   <span class="timeago" data-date="2021-08-18 16:23:05 &#43;0000 UTC">2021-08-18 16:23:05 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            I heard that a lot of content on the CSAM database don&#x27;t individually look like child porn. For example, a vaginal close up that is of a 17 year old, but could look like a vaginal close up of a 19 year old.<p>Remember, anything under 18 is treated the exact same way from a legal perspective, and once discovered Apple must report.<p>So there is an adversarial attack in that you find legal porn of say 18 year old pussy closeups, disturb it to match CSAM with the neural hash, and send it to your enemies.<p>The Apple employee will verify a match. The Feds will raid your target and imprison them, and reputationally tarnish them for life.<p>Now think about how Pegasus can remotely modify your photos. And think about Julian Assange&#x27;s sexual assault allegations that were admitted to be fabrications by the accuser.
        </div>
        <div class="children">
            
        </div>
    </div>


            
        </div>
    </div>


        
            


    
    <div class="comment ">
        <p><a href="https://news.ycombinator.com/user?id=proactivesvcs" target="_blank">proactivesvcs</a>   <span class="timeago" data-date="2021-08-18 14:48:14 &#43;0000 UTC">2021-08-18 14:48:14 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            Many of us have been at the sharp end of a large company&#x27;s supposed &quot;human review&quot; process when it comes to account lockouts, invalid abuse reports and invalid accusations of breach of terms of service. It simply does not work for too many and leaves us with little or no redress; it is a completely untenable fallback.<p>When you&#x27;re promised that this will never happen to you because there are humans in the process remember that they&#x27;re one or more of underpaid, poorly-trained, poorly-treated, under-resourced or just flat-out terrible at their job.<p>This may well happen to you and you&#x27;ll have no way back.
        </div>
        <div class="children">
            
        </div>
    </div>


        
            


    
    <div class="comment ">
        <p><a href="https://news.ycombinator.com/user?id=nine_k" target="_blank">nine_k</a>   <span class="timeago" data-date="2021-08-18 14:57:07 &#43;0000 UTC">2021-08-18 14:57:07 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            I think this is a great, high-profile  example of how keeping things open helped quickly identify a critical problem.<p>AFAICT the hash function &#x2F; perceptual model is openly available, or, at least, easy to find online.
        </div>
        <div class="children">
            
        </div>
    </div>


        
            


    
    <div class="comment hasChildren">
        <p><a href="https://news.ycombinator.com/user?id=bengale" target="_blank">bengale</a>   <span class="timeago" data-date="2021-08-18 15:31:00 &#43;0000 UTC">2021-08-18 15:31:00 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            I don&#x27;t understand why this is a concern. Someone would need 30 or so child porn images to generate these. Then they&#x27;d need to create these grey blobs and send them to the target. The target would need to save them in their photo library for some reason so they sync to iCloud. Then when all this triggers the review they&#x27;ll see they are grey blobs and nothing else will happen?
        </div>
        <div class="children">
            
                


    
    <div class="comment hasChildren">
        <p><a href="https://news.ycombinator.com/user?id=teitoklien" target="_blank">teitoklien</a>   <span class="timeago" data-date="2021-08-18 15:38:57 &#43;0000 UTC">2021-08-18 15:38:57 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            Its astonishing , 
How society went from keeping personal photos private , to thinking its ok , to let a random apple employee scan their photos for suspicion and debate on how human reviewers from a faceless corporation can easily stop these mistakes there.<p>I’d trust the algorithm more than the human reviewing it , considering the algorithm already shows flaws and loopholes , the human part of it does not bring any confidence.<p>This isnt facebook , this is your private photos , it shouldnt have reached till this stage , in the first place
        </div>
        <div class="children">
            
                


    
    <div class="comment hasChildren">
        <p><a href="https://news.ycombinator.com/user?id=bengale" target="_blank">bengale</a>   <span class="timeago" data-date="2021-08-18 16:02:40 &#43;0000 UTC">2021-08-18 16:02:40 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            But they won&#x27;t be my private photos, the only way it gets to the point where a human see&#x27;s them is that I have a bunch of child porn uploaded into iCloud, or someone puts a bunch of these grey blob images into my library. Neither of those are my images. There is no chance 30+ of my personal images have hash collisions with this database of child porn.<p>&gt; How society went from keeping personal photos private<p>Also, didn&#x27;t the local photo development shops used to call the police quite frequently when dodgy images were sent in to be developed? I remember it happening once at our local supermarket.
        </div>
        <div class="children">
            
                


    
    <div class="comment hasChildren">
        <p><a href="https://news.ycombinator.com/user?id=teitoklien" target="_blank">teitoklien</a>   <span class="timeago" data-date="2021-08-18 16:12:38 &#43;0000 UTC">2021-08-18 16:12:38 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            Hash collisions dont need grey images 
You can take a perfectly normal image , play around with a range of its individual pixels, to get collisions too.<p>So someone might forward you your personal pics from your meeting with them and its pixels might get edited by the messenger app you use to save the picture into your photos to specifically collide its hash with a csam image, while to you the image will look perfectly normal<p>This is one example , lets say you 100% trust every developer of every app that you download on apple’s phone.<p>Fine, but that’s already a lot of trust on a lot of people about something that can ruin your life.<p>Do you now trust every single image which might get auto downloaded to your gallery by a malicious actor , what if a random anon person messages you with 100 such colliding images , enough to cross threshold to get the authorities knocking your door ,<p>Is this “feature” really worth it , to have the inconvenience of authorities knocking your door and going through legal trouble ?<p>For an iphone ?
        </div>
        <div class="children">
            
                


    
    <div class="comment hasChildren">
        <p><a href="https://news.ycombinator.com/user?id=bengale" target="_blank">bengale</a>   <span class="timeago" data-date="2021-08-18 16:16:31 &#43;0000 UTC">2021-08-18 16:16:31 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            &gt; So someone might forward you your personal pics from your meeting with them and its pixels might get edited by the messenger app you use to save the picture into your photos to specifically collide its hash with a csam image, while to you the image will look perfectly normal<p>Ok, but then that image is already not private, if I&#x27;ve been sending it to someone that could do that.<p>&gt; This is one example , lets say you 100% trust every developer of every app that you download on apple’s phone.<p>I don&#x27;t have to, they have to ask me before storing images in my photo library. I&#x27;m not going to give some fart generator app access to my library.<p>&gt; Do you now trust every single image which might get auto downloaded to your gallery by a malicious actor , what if a random anon person messages you with 100 such colliding images , enough to cross threshold to get the authorities knocking your door<p>Beyond the fact that nothing writes images to my library automatically. If they sent me 100 images that are grey blobs or slightly manipulated normal images they dont get past the check anyway so no police. If they send me 100 CSAM images I&#x27;ll be on the phone to the police anyway.
        </div>
        <div class="children">
            
                


    
    <div class="comment hasChildren">
        <p><a href="https://news.ycombinator.com/user?id=teitoklien" target="_blank">teitoklien</a>   <span class="timeago" data-date="2021-08-18 16:24:52 &#43;0000 UTC">2021-08-18 16:24:52 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            Great on you , 
But i dont see my parents disabling the auto feature , nor do my friends.<p>Youre right indeed , you can protect yourself from this , but all the measures you mentioned are settings which are non-default,<p>A lot of apps annoyingly turn it on by default , while HN users can turn it off , I doubt my grandparents will go through the same effort or understand it.<p>Question is , why should they ? 
Their phone was supposed to help them , not be a snitch and a snitch with flaws at that.<p>Why should my non-tech friends train to protect themselves from “their” phones.<p>Human society has always tried to place co-existence and trust on their fore frontier , their ideal wish for society.<p>This makes us and our devices act as snitches to each other.<p>Snitch for whom ? 
Apple ?<p>But yes youre right , that if its normal image it will stop at review level , (altho thats on the condition you trust apple employees who are reviewing them)<p>I just still dont think its a good idea to even reach till this step.<p>This just makes our computers feel more hostile
        </div>
        <div class="children">
            
                


    
    <div class="comment hasChildren">
        <p><a href="https://news.ycombinator.com/user?id=bengale" target="_blank">bengale</a>   <span class="timeago" data-date="2021-08-18 16:32:39 &#43;0000 UTC">2021-08-18 16:32:39 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            Ok so your fear is someone will target your grandparents with child porn?<p>If this was such a major problem why has no-one been targeted like this in the last decade when everyone else was scanning for it?
        </div>
        <div class="children">
            
                


    
    <div class="comment hasChildren">
        <p><a href="https://news.ycombinator.com/user?id=teitoklien" target="_blank">teitoklien</a>   <span class="timeago" data-date="2021-08-18 16:37:16 &#43;0000 UTC">2021-08-18 16:37:16 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            Here’s an example of exactly that<p><a href="https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=28221907" rel="nofollow">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=28221907</a>
        </div>
        <div class="children">
            
                


    
    <div class="comment hasChildren">
        <p><a href="https://news.ycombinator.com/user?id=bengale" target="_blank">bengale</a>   <span class="timeago" data-date="2021-08-18 16:53:13 &#43;0000 UTC">2021-08-18 16:53:13 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            It&#x27;s not an example though. Putting aside a lack of news report we&#x27;ll assume it&#x27;s true, it&#x27;s got nothing to do with cloud scanning for images. A picture of someones children wouldn&#x27;t trigger these systems at all.
        </div>
        <div class="children">
            
                


    
    <div class="comment hasChildren">
        <p><a href="https://news.ycombinator.com/user?id=teitoklien" target="_blank">teitoklien</a>   <span class="timeago" data-date="2021-08-18 17:00:46 &#43;0000 UTC">2021-08-18 17:00:46 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            &gt; its got nothing to do with cloud scanning for images<p>Everyone else are cloud services scanning for images , cloud or on-device wont change the math, math stays same and here the math makes mistake<p>&gt; A picture of someone’s children wouldn’t trigger these systems at all<p>The main post under which we are conversing , is about someone having generated a collision to trigger this system.
        </div>
        <div class="children">
            
                


    
    <div class="comment ">
        <p><a href="https://news.ycombinator.com/user?id=bengale" target="_blank">bengale</a>   <span class="timeago" data-date="2021-08-18 17:27:53 &#43;0000 UTC">2021-08-18 17:27:53 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            I feel we&#x27;re circling the same issues here so I&#x27;ll leave it after this. But what you&#x27;re saying is the point I&#x27;m making, many clouds have been doing this for a decade and we&#x27;ve not seen the issues you&#x27;re worried about. The one example you gave was a photo tech, so human developing photos, making a mistake. Not someone being targeted with material that would trigger google photos or Facebook to flag them, not algorithms making mistakes with photos of someones kids, none of it.<p>The post we&#x27;re commenting under has created hash collisions with grey blob images, if you get 100 of these they won&#x27;t get past the review step anyway. It&#x27;d be a pointless attack.<p>Like most of the outrage around this system it mostly seems to boil down to FUD.
        </div>
        <div class="children">
            
        </div>
    </div>


            
        </div>
    </div>


            
        </div>
    </div>


            
        </div>
    </div>


            
        </div>
    </div>


            
        </div>
    </div>


            
        </div>
    </div>


            
        </div>
    </div>


            
        </div>
    </div>


            
        </div>
    </div>


            
        </div>
    </div>


        
            


    
    <div class="comment ">
        <p><a href="https://news.ycombinator.com/user?id=borplk" target="_blank">borplk</a>   <span class="timeago" data-date="2021-08-18 11:27:25 &#43;0000 UTC">2021-08-18 11:27:25 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            If you piss off the wrong crowd they&#x27;re going to drop a small bag with white powder in your photo gallery app if you know what I mean. Along with event logs and all.<p>Good luck!
        </div>
        <div class="children">
            
        </div>
    </div>


        
            


    
    <div class="comment hasChildren">
        <p><a href="https://news.ycombinator.com/user?id=newsclues" target="_blank">newsclues</a>   <span class="timeago" data-date="2021-08-18 11:50:51 &#43;0000 UTC">2021-08-18 11:50:51 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            First CP.<p>Then leaked or unauthorized nudes will get filtered.<p>Filters for terrorists, and terrorist imagery or symbols.<p>Start scanning for guns and drugs.<p>Drug dealers and criminals get added to the list.<p>How long before before it’s dissidents, political opponents, and minorities in dictatorships?<p>How long before Tim Cooks CP filters are used against LGBT groups abroad?
        </div>
        <div class="children">
            
                


    
    <div class="comment hasChildren">
        <p><a href="https://news.ycombinator.com/user?id=zug_zug" target="_blank">zug_zug</a>   <span class="timeago" data-date="2021-08-18 13:31:37 &#43;0000 UTC">2021-08-18 13:31:37 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            It&#x27;s funny how it&#x27;s always CP or terrorism we need to watch out for.... Never is it the politician who accidentally went to war with wrong country, or the tax return of the congressman, or a cop sleeping with somebody who&#x27;s under arrest, or EPA employee who took a bribe to declare a chemical safe, or mysteriously malfunctioning cameras in a prison the night of an alleged suicide.<p>Build surveillance to catch those people and I&#x27;ll listen.
        </div>
        <div class="children">
            
                


    
    <div class="comment ">
        <p><a href="https://news.ycombinator.com/user?id=personlurking" target="_blank">personlurking</a>   <span class="timeago" data-date="2021-08-18 17:39:53 &#43;0000 UTC">2021-08-18 17:39:53 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            &gt;It&#x27;s funny how it&#x27;s always CP or terrorism we need to watch out for<p>Change the name on the box to something else, but same message.<p><a href="https:&#x2F;&#x2F;i.imgur.com&#x2F;TAHgUPy.jpg" rel="nofollow">https:&#x2F;&#x2F;i.imgur.com&#x2F;TAHgUPy.jpg</a>
        </div>
        <div class="children">
            
        </div>
    </div>


            
                


    
    <div class="comment ">
        <p><a href="https://news.ycombinator.com/user?id=newsclues" target="_blank">newsclues</a>   <span class="timeago" data-date="2021-08-18 15:33:34 &#43;0000 UTC">2021-08-18 15:33:34 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            I am not surprised censorship is used selectively by the powerful against their enemies.
        </div>
        <div class="children">
            
        </div>
    </div>


            
        </div>
    </div>


            
                


    
    <div class="comment hasChildren">
        <p><a href="https://news.ycombinator.com/user?id=Crontab" target="_blank">Crontab</a>   <span class="timeago" data-date="2021-08-18 12:23:45 &#43;0000 UTC">2021-08-18 12:23:45 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            &gt; How long before before it’s dissidents, political opponents, and minorities in dictatorships?<p>Siri: &quot;You seem to be having unpatriotic thoughts. We are dispatching someone to help you.&quot;
        </div>
        <div class="children">
            
                


    
    <div class="comment ">
        <p><a href="https://news.ycombinator.com/user?id=jnsie" target="_blank">jnsie</a>   <span class="timeago" data-date="2021-08-18 13:35:26 &#43;0000 UTC">2021-08-18 13:35:26 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            By dispatching someone you presumably mean “ok, I found this on the web”?
        </div>
        <div class="children">
            
        </div>
    </div>


            
        </div>
    </div>


            
                


    
    <div class="comment hasChildren">
        <p><a href="https://news.ycombinator.com/user?id=robertoandred" target="_blank">robertoandred</a>   <span class="timeago" data-date="2021-08-18 14:46:19 &#43;0000 UTC">2021-08-18 14:46:19 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            Do you know how many billions of pictures of guns and drugs have ever been taken?
        </div>
        <div class="children">
            
                


    
    <div class="comment hasChildren">
        <p><a href="https://news.ycombinator.com/user?id=newsclues" target="_blank">newsclues</a>   <span class="timeago" data-date="2021-08-18 15:31:55 &#43;0000 UTC">2021-08-18 15:31:55 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            Sure!  Lots!<p>My Instagram account is mainly weed (legal in Canada) and Instagram censorship hits my content occasionally (because illegal cannabis selling on the platform is rampant).
        </div>
        <div class="children">
            
                


    
    <div class="comment hasChildren">
        <p><a href="https://news.ycombinator.com/user?id=robertoandred" target="_blank">robertoandred</a>   <span class="timeago" data-date="2021-08-18 16:17:26 &#43;0000 UTC">2021-08-18 16:17:26 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            And how many of those billions will Apple hash and check for?
        </div>
        <div class="children">
            
                


    
    <div class="comment hasChildren">
        <p><a href="https://news.ycombinator.com/user?id=newsclues" target="_blank">newsclues</a>   <span class="timeago" data-date="2021-08-18 16:22:20 &#43;0000 UTC">2021-08-18 16:22:20 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            Not zero
        </div>
        <div class="children">
            
                


    
    <div class="comment ">
        <p><a href="https://news.ycombinator.com/user?id=robertoandred" target="_blank">robertoandred</a>   <span class="timeago" data-date="2021-08-18 16:23:51 &#43;0000 UTC">2021-08-18 16:23:51 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            And how many of those pictures will you have in your library?
        </div>
        <div class="children">
            
        </div>
    </div>


            
        </div>
    </div>


            
        </div>
    </div>


            
        </div>
    </div>


            
        </div>
    </div>


            
                


    
    <div class="comment ">
        <p><a href="https://news.ycombinator.com/user?id=cyanite" target="_blank">cyanite</a>   <span class="timeago" data-date="2021-08-18 20:37:35 &#43;0000 UTC">2021-08-18 20:37:35 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            Maybe we can worry about things when and if they are implemented? If you don’t trust Apple it’s a moot discussion anyway.
        </div>
        <div class="children">
            
        </div>
    </div>


            
                


    
    <div class="comment hasChildren">
        <p><a href="https://news.ycombinator.com/user?id=plutonorm" target="_blank">plutonorm</a>   <span class="timeago" data-date="2021-08-18 13:34:38 &#43;0000 UTC">2021-08-18 13:34:38 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            This is the real danger here. Covid made them bold, it set them up as a judge of content passing through their systems. Now they are consolidating themselves in that role. A boot stamping on a human face for all eternity.
        </div>
        <div class="children">
            
                


    
    <div class="comment ">
        <p><a href="https://news.ycombinator.com/user?id=newsclues" target="_blank">newsclues</a>   <span class="timeago" data-date="2021-08-18 15:30:29 &#43;0000 UTC">2021-08-18 15:30:29 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            Fact checking to censoring misinformation then on device content scanning.  In less than 5 years we have made multiple leaps towards censorship.
        </div>
        <div class="children">
            
        </div>
    </div>


            
        </div>
    </div>


            
        </div>
    </div>


        
            


    
    <div class="comment ">
        <p><a href="https://news.ycombinator.com/user?id=rogers18445" target="_blank">rogers18445</a>   <span class="timeago" data-date="2021-08-18 13:46:09 &#43;0000 UTC">2021-08-18 13:46:09 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            The hash was supposed to not be altered by minor perturbations to the input. This requirement assures that the hash can never be cryptographically secure, collision attacks therefore are trivial the moment the &quot;security by obscurity&quot; aspect is broken.
        </div>
        <div class="children">
            
        </div>
    </div>


        
            


    
    <div class="comment ">
        <p><a href="https://news.ycombinator.com/user?id=orange_puff" target="_blank">orange_puff</a>   <span class="timeago" data-date="2021-08-18 13:14:41 &#43;0000 UTC">2021-08-18 13:14:41 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            Today HackerNews learns about adversarial machine learning, which every ML model is susceptible to.
        </div>
        <div class="children">
            
        </div>
    </div>


        
            


    
    <div class="comment hasChildren">
        <p><a href="https://news.ycombinator.com/user?id=kuu" target="_blank">kuu</a>   <span class="timeago" data-date="2021-08-18 09:51:58 &#43;0000 UTC">2021-08-18 09:51:58 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            I don&#x27;t understand the comment in the issue by an iPhone user. Can you see the hashes that the mobile generates for each image?? Why that is not &quot;obfuscated&quot; &#x2F; hidden from the user? I mean, I would expect something complicated to validate that you have a collision.
        </div>
        <div class="children">
            
                


    
    <div class="comment hasChildren">
        <p><a href="https://news.ycombinator.com/user?id=scandinavian" target="_blank">scandinavian</a>   <span class="timeago" data-date="2021-08-18 10:16:06 &#43;0000 UTC">2021-08-18 10:16:06 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            It is hidden for the user. Obfuscation doesn&#x27;t work. They probably just called the private API to generate a hash on a jailbroken phone. There&#x27;s even a link to another piece of software that can do just that, only on macOS.<p><a href="https:&#x2F;&#x2F;github.com&#x2F;KhaosT&#x2F;nhcalc" rel="nofollow">https:&#x2F;&#x2F;github.com&#x2F;KhaosT&#x2F;nhcalc</a>
        </div>
        <div class="children">
            
                


    
    <div class="comment ">
        <p><a href="https://news.ycombinator.com/user?id=cyanite" target="_blank">cyanite</a>   <span class="timeago" data-date="2021-08-18 20:39:16 &#43;0000 UTC">2021-08-18 20:39:16 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            The hash table is blinded on the device, and the device never knows if a given image is a hit or not. This is well documented.
        </div>
        <div class="children">
            
        </div>
    </div>


            
        </div>
    </div>


            
                


    
    <div class="comment ">
        <p><a href="https://news.ycombinator.com/user?id=jhugo" target="_blank">jhugo</a>   <span class="timeago" data-date="2021-08-18 10:22:16 &#43;0000 UTC">2021-08-18 10:22:16 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            You&#x27;re holding the iPhone in your hand. You can inspect everything it does, the only thing that varies is the level of difficulty of inspecting it. Obfuscation doesn&#x27;t work.
        </div>
        <div class="children">
            
        </div>
    </div>


            
                


    
    <div class="comment ">
        <p><a href="https://news.ycombinator.com/user?id=" target="_blank"></a>   <span class="timeago" data-date="2021-08-18 10:20:50 &#43;0000 UTC">2021-08-18 10:20:50 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            
        </div>
        <div class="children">
            
        </div>
    </div>


            
                


    
    <div class="comment hasChildren">
        <p><a href="https://news.ycombinator.com/user?id=nulld3v" target="_blank">nulld3v</a>   <span class="timeago" data-date="2021-08-18 09:55:56 &#43;0000 UTC">2021-08-18 09:55:56 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            They could have a jailbreak device.
        </div>
        <div class="children">
            
                


    
    <div class="comment ">
        <p><a href="https://news.ycombinator.com/user?id=eptcyka" target="_blank">eptcyka</a>   <span class="timeago" data-date="2021-08-18 10:02:57 &#43;0000 UTC">2021-08-18 10:02:57 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            They could just be a dev that work on the code and have toy apps to test it out.
        </div>
        <div class="children">
            
        </div>
    </div>


            
        </div>
    </div>


            
                


    
    <div class="comment hasChildren">
        <p><a href="https://news.ycombinator.com/user?id=jdlshore" target="_blank">jdlshore</a>   <span class="timeago" data-date="2021-08-18 18:57:00 &#43;0000 UTC">2021-08-18 18:57:00 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            You&#x27;re correct. The amount of misinformation in this thread (and in the other responses to you) is out of control.<p>The database of CSAM hashes is blinded and no one has the hashes. Without the hashes, this attack is useless.<p>It&#x27;s also mitigated by a LOT of checks and balances. First they have to know 30 hashes to target (they&#x27;re secret). They have to get 30 colliding images on your phone. The images have to be unnoticed by you (why not just infiltrate CSAM, then?) or sufficiently compelling that you don&#x27;t just delete them. Thirty images have to pass human review at Apple. At least one has to pass human review by law enforcement. Then, and only then, will you be arrested and face a threat.<p>Short version: If somebody wants to frame you for possessing CSAM, there are much easier ways. There is no new threat here. <a href="https:&#x2F;&#x2F;xkcd.com&#x2F;538&#x2F;" rel="nofollow">https:&#x2F;&#x2F;xkcd.com&#x2F;538&#x2F;</a>
        </div>
        <div class="children">
            
                


    
    <div class="comment ">
        <p><a href="https://news.ycombinator.com/user?id=cyanite" target="_blank">cyanite</a>   <span class="timeago" data-date="2021-08-18 20:40:23 &#43;0000 UTC">2021-08-18 20:40:23 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            &gt; The amount of misinformation in this thread (and in the other responses to you) is out of control.<p>True it’s not perfect here, but you should see Reddit, then :p. People there hardly even know what they are mad about.
        </div>
        <div class="children">
            
        </div>
    </div>


            
        </div>
    </div>


            
        </div>
    </div>


        
            


    
    <div class="comment hasChildren">
        <p><a href="https://news.ycombinator.com/user?id=visarga" target="_blank">visarga</a>   <span class="timeago" data-date="2021-08-18 19:32:45 &#43;0000 UTC">2021-08-18 19:32:45 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            Have you considered that Apple might have a second neural hashing network which is hidden and they let this one network be hacked to throw people off?
        </div>
        <div class="children">
            
                


    
    <div class="comment ">
        <p><a href="https://news.ycombinator.com/user?id=faeriechangling" target="_blank">faeriechangling</a>   <span class="timeago" data-date="2021-08-19 01:52:27 &#43;0000 UTC">2021-08-19 01:52:27 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            What a genius publicity move
        </div>
        <div class="children">
            
        </div>
    </div>


            
        </div>
    </div>


        
            


    
    <div class="comment hasChildren">
        <p><a href="https://news.ycombinator.com/user?id=raspasov" target="_blank">raspasov</a>   <span class="timeago" data-date="2021-08-18 13:48:47 &#43;0000 UTC">2021-08-18 13:48:47 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            I&#x27;m not in favor of assuming that everyone&#x27;s guilty until proven innocent.<p>But, as a side note...<p>I get the feeling that a lot of people assume that the CSAM hashes are going to be stored directly on everyone&#x27;s phone so it&#x27;s easy to get a hold of them and create images that match those hashes.<p>That does not seem to be the case. The actual CSAM hashes go through a &quot;blinding&quot; server-side step.<p><a href="https:&#x2F;&#x2F;www.apple.com&#x2F;child-safety&#x2F;pdf&#x2F;CSAM_Detection_Technical_Summary.pdf" rel="nofollow">https:&#x2F;&#x2F;www.apple.com&#x2F;child-safety&#x2F;pdf&#x2F;CSAM_Detection_Techni...</a>
        </div>
        <div class="children">
            
                


    
    <div class="comment ">
        <p><a href="https://news.ycombinator.com/user?id=cyanite" target="_blank">cyanite</a>   <span class="timeago" data-date="2021-08-18 20:38:28 &#43;0000 UTC">2021-08-18 20:38:28 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            It seems to me that the vast majority of people in forums don’t actually know how this system is implemented. Although people definitely seem more informed here than on Reddit.
        </div>
        <div class="children">
            
        </div>
    </div>


            
        </div>
    </div>


        
            


    
    <div class="comment ">
        <p><a href="https://news.ycombinator.com/user?id=maz1b" target="_blank">maz1b</a>   <span class="timeago" data-date="2021-08-18 16:56:03 &#43;0000 UTC">2021-08-18 16:56:03 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            No longer considering purchasing any more Apple devices or services for any team members in my company. This is unacceptable.
        </div>
        <div class="children">
            
        </div>
    </div>


        
            


    
    <div class="comment ">
        <p><a href="https://news.ycombinator.com/user?id=daralthus" target="_blank">daralthus</a>   <span class="timeago" data-date="2021-08-18 17:37:27 &#43;0000 UTC">2021-08-18 17:37:27 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            Putting aside the original intent what else can they scan for that&#x27;s worth doing in the aggregate with this tech?
        </div>
        <div class="children">
            
        </div>
    </div>


        
            


    
    <div class="comment hasChildren">
        <p><a href="https://news.ycombinator.com/user?id=nojito" target="_blank">nojito</a>   <span class="timeago" data-date="2021-08-18 12:56:02 &#43;0000 UTC">2021-08-18 12:56:02 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            Any matches are matched again server side to thwart this type of attack.<p>&gt;Once Apple&#x27;s iCloud Photos servers decrypt a set of positive match vouchers for an account that exceeded the match threshold, the visual derivatives of the positively matching images are referred for review by Apple. First, as an additional safeguard, the visual derivatives themselves are matched to the known CSAM database by a second, independent perceptual hash. This independent hash is chosen to reject the unlikely possibility that the match threshold was exceeded due to non-CSAM images that were adversarially perturbed to cause false NeuralHash matches against the on-device encrypted CSAM database. If the CSAM finding is confirmed by this independent hash, the visual derivatives are provided to Apple human reviewers for final confirmation.<p>They also fuzz this process by sending false positives I think?
        </div>
        <div class="children">
            
                


    
    <div class="comment hasChildren">
        <p><a href="https://news.ycombinator.com/user?id=notquitehuman" target="_blank">notquitehuman</a>   <span class="timeago" data-date="2021-08-18 14:12:20 &#43;0000 UTC">2021-08-18 14:12:20 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            It doesn’t matter what they do after “looking for something to report to law enforcement.” Nothing after that makes it less invasive.
        </div>
        <div class="children">
            
                


    
    <div class="comment ">
        <p><a href="https://news.ycombinator.com/user?id=floatingatoll" target="_blank">floatingatoll</a>   <span class="timeago" data-date="2021-08-18 14:39:01 &#43;0000 UTC">2021-08-18 14:39:01 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            Incorrect. The objections here are threefold:<p>1) This is invasive.<p>2) Slippery slope.<p>3) Gets innocents arrested.<p>Their solution mitigates #3 with review and mitigates #1 with blurring, and those mitigations occur and the point in the process where you claim it doesn’t matter what they do.<p>Please don’t reductively dismiss facts that don’t support your narrative. Saying that it doesn’t matter is wrong and serves only to boost your particular viewpoint on #2.
        </div>
        <div class="children">
            
        </div>
    </div>


            
                


    
    <div class="comment ">
        <p><a href="https://news.ycombinator.com/user?id=nojito" target="_blank">nojito</a>   <span class="timeago" data-date="2021-08-18 15:45:31 &#43;0000 UTC">2021-08-18 15:45:31 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            Disagree. What companies do now is much more invasive. (Indiscriminately scanning cloud storage)
        </div>
        <div class="children">
            
        </div>
    </div>


            
        </div>
    </div>


            
        </div>
    </div>


        
            


    
    <div class="comment ">
        <p><a href="https://news.ycombinator.com/user?id=bugmen0t" target="_blank">bugmen0t</a>   <span class="timeago" data-date="2021-08-18 12:57:36 &#43;0000 UTC">2021-08-18 12:57:36 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            Note that a hash collision is not the same as a pre-image attack. Though it seems to me that finding the latter is also feasible.
        </div>
        <div class="children">
            
        </div>
    </div>


        
            


    
    <div class="comment hasChildren">
        <p><a href="https://news.ycombinator.com/user?id=rollinggoron" target="_blank">rollinggoron</a>   <span class="timeago" data-date="2021-08-18 14:19:31 &#43;0000 UTC">2021-08-18 14:19:31 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            How is this scenario unique to Apple but not everyone else who does scanning? e.g. Google, Facebook, Microsoft etc...
        </div>
        <div class="children">
            
                


    
    <div class="comment hasChildren">
        <p><a href="https://news.ycombinator.com/user?id=steeleduncan" target="_blank">steeleduncan</a>   <span class="timeago" data-date="2021-08-18 14:32:57 &#43;0000 UTC">2021-08-18 14:32:57 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            If the entity doing the scanning has a copy of the original image they can verify it is illegal before calling the police. With Apple&#x27;s system they have to call the police on the basis of the image hash without verifying that anything illegal is on the phone.<p>You can whatsapp someone an innocent image doctored to have a hash collision with known CSAM. If they have default settings it will be saved to their photo reel, scanned by iOS and the police will be called.<p>Until the arresting police officer explains to them they are being arrested on suspicion of being a paedophile, they won&#x27;t even know this has happened.
        </div>
        <div class="children">
            
                


    
    <div class="comment ">
        <p><a href="https://news.ycombinator.com/user?id=ec109685" target="_blank">ec109685</a>   <span class="timeago" data-date="2021-08-18 16:13:46 &#43;0000 UTC">2021-08-18 16:13:46 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            A low res copy is available to the reviewer on Apple&#x27;s end to verify.
        </div>
        <div class="children">
            
        </div>
    </div>


            
                


    
    <div class="comment hasChildren">
        <p><a href="https://news.ycombinator.com/user?id=cyanite" target="_blank">cyanite</a>   <span class="timeago" data-date="2021-08-18 20:41:35 &#43;0000 UTC">2021-08-18 20:41:35 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            Apple doesn’t “call the police”, though, they contact the center for child abuse or whatever it’s called, who will then presumably verify the picture.
        </div>
        <div class="children">
            
                


    
    <div class="comment ">
        <p><a href="https://news.ycombinator.com/user?id=dragonwriter" target="_blank">dragonwriter</a>   <span class="timeago" data-date="2021-08-18 20:45:21 &#43;0000 UTC">2021-08-18 20:45:21 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            The National Center for Missing and Exploited Children, a “private” nonprofit created by the US government and primarily funded by the Department of Justice.
        </div>
        <div class="children">
            
        </div>
    </div>


            
        </div>
    </div>


            
        </div>
    </div>


            
                


    
    <div class="comment ">
        <p><a href="https://news.ycombinator.com/user?id=floatingatoll" target="_blank">floatingatoll</a>   <span class="timeago" data-date="2021-08-18 14:20:58 &#43;0000 UTC">2021-08-18 14:20:58 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            It’s not. The hacker community
just never knew or cared about CSAM scanning before Apple did it.
        </div>
        <div class="children">
            
        </div>
    </div>


            
        </div>
    </div>


        
            


    
    <div class="comment ">
        <p><a href="https://news.ycombinator.com/user?id=dgb99" target="_blank">dgb99</a>   <span class="timeago" data-date="2021-08-18 14:56:40 &#43;0000 UTC">2021-08-18 14:56:40 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            can someone upvote me please? I need to add my account to my keybase profile.
        </div>
        <div class="children">
            
        </div>
    </div>


        
            


    
    <div class="comment hasChildren">
        <p><a href="https://news.ycombinator.com/user?id=marcob95" target="_blank">marcob95</a>   <span class="timeago" data-date="2021-08-18 15:24:59 &#43;0000 UTC">2021-08-18 15:24:59 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            Can someone help me figure out why this is such a big deal? Thanks
        </div>
        <div class="children">
            
                


    
    <div class="comment ">
        <p><a href="https://news.ycombinator.com/user?id=cyanite" target="_blank">cyanite</a>   <span class="timeago" data-date="2021-08-18 20:43:28 &#43;0000 UTC">2021-08-18 20:43:28 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            I’d say that it’s not, and that many people think it is is because they haven’t fully read or understood the technical description of the system.<p>But what you should do is read the documents yourself.
        </div>
        <div class="children">
            
        </div>
    </div>


            
        </div>
    </div>


        
            


    
    <div class="comment ">
        <p><a href="https://news.ycombinator.com/user?id=a-dub" target="_blank">a-dub</a>   <span class="timeago" data-date="2021-08-18 15:16:22 &#43;0000 UTC">2021-08-18 15:16:22 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            it gets even more fun when you can generate images that are imperceivably different to the human eye, but produce different (and controlled) hash values.  (see shazam decoys and adversarial images).  not sure if collision is really the right term here though as similarity hashing intentionally hashes multiple images to the same hash.<p>in other news, my google phone just helpfully scanned all client side images, rounded up everything it thought was a meme and suggested i delete them to save space. i wonder if that feature has telemetry built in...
        </div>
        <div class="children">
            
        </div>
    </div>


        
            


    
    <div class="comment hasChildren">
        <p><a href="https://news.ycombinator.com/user?id=hughrr" target="_blank">hughrr</a>   <span class="timeago" data-date="2021-08-18 10:04:38 &#43;0000 UTC">2021-08-18 10:04:38 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            That’s end game. Now you can use it for targeted attacks against innocent people. This needs to be shut down and disposed of immediately. There is no other outcome which is socially acceptable for Apple.<p>I feel vindicated now. There are a lot of people saying that I’m insane as I’ve dumped the entire iOS ecosystem in the last week. But Craig was busy steamrolling out the marketing still only a couple of days back about how this is good for the world. No way am I going back.<p>Edit: I’m going to reply to some child comments here because I can’t be bothered to reply to each one. This is step one down the stairs to hell. If even one person is caught with this it will be leverage to move to step 2. Within a few years your entire frame buffer and camera will be working against you full time.
        </div>
        <div class="children">
            
                


    
    <div class="comment hasChildren">
        <p><a href="https://news.ycombinator.com/user?id=nbzso" target="_blank">nbzso</a>   <span class="timeago" data-date="2021-08-18 11:48:46 &#43;0000 UTC">2021-08-18 11:48:46 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            Don&#x27;t feel bad at all. I dumped macOS entirely from production workflow. I cannot work on computer knowing that something is &quot;scanning&quot; me and I am glad that my &quot;paranoid&quot; feeling stopped me to upgrade all office macs.<p>Billionaires at (Apple) don&#x27;t give a flying f*ck about users privacy. It is all vertical integration in the name of world domination. How removed from reality they are. This is week after Pegasus&#x2F;NSO and there is no law who requires them to &quot;scan&quot; on device. So the logical explanation is &quot;growth&quot;. 
Way to go Apple.
        </div>
        <div class="children">
            
                


    
    <div class="comment hasChildren">
        <p><a href="https://news.ycombinator.com/user?id=sandworm101" target="_blank">sandworm101</a>   <span class="timeago" data-date="2021-08-18 13:14:27 &#43;0000 UTC">2021-08-18 13:14:27 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            &gt;&gt; there is no law who requires them to &quot;scan&quot; on device.<p>There is.  Apple must comply with warrant requests.  If they have a system for scanning files on customer devices they must, if presented with a warrant, allow police access to that system.  We can quibble about jurisdictions and constitutional protections, but if the FBI shows up with a federal warrant demanding that Apple remotely scan Sandworm101&#x27;s phone for a particular image, Sandworm101&#x27;s phone is going to be scanned.
        </div>
        <div class="children">
            
                


    
    <div class="comment hasChildren">
        <p><a href="https://news.ycombinator.com/user?id=ethbr0" target="_blank">ethbr0</a>   <span class="timeago" data-date="2021-08-18 13:44:25 &#43;0000 UTC">2021-08-18 13:44:25 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            That&#x27;s an incomplete statement.<p>Currently, they must comply with warranty requests by scanning <i>if they have the ability to scan</i>.<p>If they have no such ability (say, because they designed their phones from a privacy-first perspective), the law makes no requirement that they <i>create</i> such a capability.<p>And that&#x27;s what pisses people off about this.
        </div>
        <div class="children">
            
                


    
    <div class="comment hasChildren">
        <p><a href="https://news.ycombinator.com/user?id=snowwrestler" target="_blank">snowwrestler</a>   <span class="timeago" data-date="2021-08-18 14:50:25 &#43;0000 UTC">2021-08-18 14:50:25 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            If Apple launches a system for comparing iCloud uploads to a third-party hash list, then adding the ability to do targeted scans for arbitrary additional law-enforcement-provided hashes would also be a form of creating capability.<p>The people getting pissed off about this have not, so far as I’ve seen, demonstrated why the law would require Apple to add the capability for targeted scans of arbitrary hashes.<p>Police can use a warrant to ask you for a video file they suspect you have. They can’t use a warrant to force you to videotape someone—even if you already own a video camera. The same principle applies to Apple.
        </div>
        <div class="children">
            
                


    
    <div class="comment hasChildren">
        <p><a href="https://news.ycombinator.com/user?id=sandworm101" target="_blank">sandworm101</a>   <span class="timeago" data-date="2021-08-18 15:01:15 &#43;0000 UTC">2021-08-18 15:01:15 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            &gt;&gt; They can’t use a warrant to force you to videotape someone<p>Maybe in the narrow context of US domestic child pornography investigations.  In the wider world it is very possible for police to force such things.  Even in the US, CALEA demands that certain companies develop abilities that they would not normally want (interception).  The principal that US companies need not actively participate in police investigations disappeared decades ago.  On the international level, all bets are off.<p><a href="https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Communications_Assistance_for_Law_Enforcement_Act" rel="nofollow">https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Communications_Assistance_for_...</a><p>&quot;by requiring that telecommunications carriers and manufacturers of telecommunications equipment <i>modify and design their equipment, facilities, and services</i> to ensure that they have built-in capabilities for targeted surveillance [...] USA telecommunications providers must <i>install new hardware or software, as well as modify old equipment</i>, so that it doesn&#x27;t interfere with the ability of a law enforcement agency (LEA) to perform real-time surveillance of any telephone or Internet traffic.&quot;
        </div>
        <div class="children">
            
                


    
    <div class="comment hasChildren">
        <p><a href="https://news.ycombinator.com/user?id=snowwrestler" target="_blank">snowwrestler</a>   <span class="timeago" data-date="2021-08-18 15:13:13 &#43;0000 UTC">2021-08-18 15:13:13 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            Can you explain why you think CALEA would apply to what Apple announced?<p>And why such application would have had to wait until Apple announced this? In other words, if the law can force Apple to do things in general, why does the law need to wait for Apple to announce certain capabilities first?
        </div>
        <div class="children">
            
                


    
    <div class="comment hasChildren">
        <p><a href="https://news.ycombinator.com/user?id=sandworm101" target="_blank">sandworm101</a>   <span class="timeago" data-date="2021-08-18 15:19:20 &#43;0000 UTC">2021-08-18 15:19:20 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            &gt;&gt; why you think CALEA would apply to what Apple announced?<p>Read what I stated.  I said no such thing.  I said that CALEA shows that companies can sometimes be forced to be do things they do not want to do, to take actions at the behest of law enforcement that they would not normally do.  CALEA would clearly not be applied to apple in this case.  It would be some other law&#x2F;warrant&#x2F;NSL that would force apple to do something it normally would not do.  CALEA stands as proof that such things have long been acceptable under US law.
        </div>
        <div class="children">
            
                


    
    <div class="comment ">
        <p><a href="https://news.ycombinator.com/user?id=snowwrestler" target="_blank">snowwrestler</a>   <span class="timeago" data-date="2021-08-18 15:35:22 &#43;0000 UTC">2021-08-18 15:35:22 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            I understand why you are worried that a more powerful law could be passed by Congress in the future.<p>I thought this particular conversation was about what Apple can be forced to do by a warrant issued under current law.
        </div>
        <div class="children">
            
        </div>
    </div>


            
        </div>
    </div>


            
        </div>
    </div>


            
        </div>
    </div>


            
                


    
    <div class="comment ">
        <p><a href="https://news.ycombinator.com/user?id=HelloNurse" target="_blank">HelloNurse</a>   <span class="timeago" data-date="2021-08-18 15:13:48 &#43;0000 UTC">2021-08-18 15:13:48 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            &gt; why the law would require Apple to add the capability for ...<p>This isn&#x27;t the sort of thing that is mandated by law, but rather requested behind the scenes by espionage, er, &quot;law enforcement&quot; agencies. They might be more or less friendly deals; nice monopoly you have here, it would be a shame if something happened to it...
        </div>
        <div class="children">
            
        </div>
    </div>


            
        </div>
    </div>


            
                


    
    <div class="comment hasChildren">
        <p><a href="https://news.ycombinator.com/user?id=tantalor" target="_blank">tantalor</a>   <span class="timeago" data-date="2021-08-18 14:14:35 &#43;0000 UTC">2021-08-18 14:14:35 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            Can a warrant compel them to develop the capability?
        </div>
        <div class="children">
            
                


    
    <div class="comment ">
        <p><a href="https://news.ycombinator.com/user?id=simcop2387" target="_blank">simcop2387</a>   <span class="timeago" data-date="2021-08-18 14:22:38 &#43;0000 UTC">2021-08-18 14:22:38 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            A warrant alone, very likely not.  A court order from a judge (slightly different than a warrant), possibly not nut it&#x27;s not exactly a well tread area of legislation&#x2F;law.  It would probably run afoul of the first amendment given some readings, but it would have to be tested in court and would likely be argued that requiring a backdoor via some law doesn&#x27;t cause compelled speech or that making that backdoor isn&#x27;t protected speech etc.  It&#x27;s a relatively new area to be explored.
        </div>
        <div class="children">
            
        </div>
    </div>


            
                


    
    <div class="comment hasChildren">
        <p><a href="https://news.ycombinator.com/user?id=sandworm101" target="_blank">sandworm101</a>   <span class="timeago" data-date="2021-08-18 14:32:48 &#43;0000 UTC">2021-08-18 14:32:48 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            Yes.  Lavabit.  Those warrants demanded that Lavabit alter its system to capture passwords and&#x2F;or decrypt stored email.  Lavbit instead decided to stop operating and delete everything rather than comply.  Such warrants have not been fully tested in courts but they do exist.
        </div>
        <div class="children">
            
                


    
    <div class="comment hasChildren">
        <p><a href="https://news.ycombinator.com/user?id=jamesdwilson" target="_blank">jamesdwilson</a>   <span class="timeago" data-date="2021-08-18 14:45:23 &#43;0000 UTC">2021-08-18 14:45:23 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            IIRC they gave up the information AND they shut down
        </div>
        <div class="children">
            
                


    
    <div class="comment ">
        <p><a href="https://news.ycombinator.com/user?id=sandworm101" target="_blank">sandworm101</a>   <span class="timeago" data-date="2021-08-18 15:13:34 &#43;0000 UTC">2021-08-18 15:13:34 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            They handed over a hard copy of their private key, a printout.  The FBI went to court to demand a machine-readable copy.  Then they shut down.
        </div>
        <div class="children">
            
        </div>
    </div>


            
        </div>
    </div>


            
        </div>
    </div>


            
                


    
    <div class="comment hasChildren">
        <p><a href="https://news.ycombinator.com/user?id=roenxi" target="_blank">roenxi</a>   <span class="timeago" data-date="2021-08-18 14:44:50 &#43;0000 UTC">2021-08-18 14:44:50 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            Have they been served a warrant?<p>Just because someone could force you to do something you don&#x27;t want to do is not really a major concern if you choose, preemptively, to do the thing. The concerning part here is Apple signalling that they have executives that don&#x27;t see phone scanning as a problem. That is a major black eye for their branding.
        </div>
        <div class="children">
            
                


    
    <div class="comment ">
        <p><a href="https://news.ycombinator.com/user?id=bcrl" target="_blank">bcrl</a>   <span class="timeago" data-date="2021-08-18 16:17:03 &#43;0000 UTC">2021-08-18 16:17:03 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            Apple&#x27;s warrant canary disappeared in 2014.
        </div>
        <div class="children">
            
        </div>
    </div>


            
        </div>
    </div>


            
                


    
    <div class="comment hasChildren">
        <p><a href="https://news.ycombinator.com/user?id=lumost" target="_blank">lumost</a>   <span class="timeago" data-date="2021-08-18 14:50:25 &#43;0000 UTC">2021-08-18 14:50:25 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            The law gets debatable here.  The warrant can ultimately be served only to the owner&#x2F;responsible party for the system.  If apple decides that they own all iPhones, a claim they&#x27;ve loosely made for a long time re Flash etc. Then they could be served a warrant on the device.<p>If they just sold the device to someone, then the police would have to issue a warrant to that individual.  My understanding is that as an individual your options are either to comply with the warrant or commit a separate crime destroying the evidence or refusing the warrant.
        </div>
        <div class="children">
            
                


    
    <div class="comment ">
        <p><a href="https://news.ycombinator.com/user?id=snowwrestler" target="_blank">snowwrestler</a>   <span class="timeago" data-date="2021-08-18 15:00:21 &#43;0000 UTC">2021-08-18 15:00:21 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            Apple does not claim they own all iPhones.<p>The legal principle is the same, actually. The law says you can install whatever software you want on an iPhone and Apple cannot stop you (jailbreaking is legal). But the law cannot force Apple to make it easy for you to do so.
        </div>
        <div class="children">
            
        </div>
    </div>


            
                


    
    <div class="comment ">
        <p><a href="https://news.ycombinator.com/user?id=ethbr0" target="_blank">ethbr0</a>   <span class="timeago" data-date="2021-08-18 17:48:23 &#43;0000 UTC">2021-08-18 17:48:23 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            I haven&#x27;t kept us with the latest cases, but I&#x27;m not sure where it falls on app-based SaaS.<p>I.e. If I use an app, on a system in which app updates are possible, which encrypts my data locally and stores it on their cloud, then the owner&#x2F;responsible party for the app is...?
        </div>
        <div class="children">
            
        </div>
    </div>


            
        </div>
    </div>


            
        </div>
    </div>


            
                


    
    <div class="comment ">
        <p><a href="https://news.ycombinator.com/user?id=" target="_blank"></a>   <span class="timeago" data-date="2021-08-18 14:04:16 &#43;0000 UTC">2021-08-18 14:04:16 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            
        </div>
        <div class="children">
            
        </div>
    </div>


            
                


    
    <div class="comment ">
        <p><a href="https://news.ycombinator.com/user?id=jodrellblank" target="_blank">jodrellblank</a>   <span class="timeago" data-date="2021-08-18 14:14:51 &#43;0000 UTC">2021-08-18 14:14:51 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            &gt; &quot;<i>the law makes no requirement that they create such a capability.</i>&quot;<p>and they have not created such a capability. This is built into the image upload libary of iCloud. This has less ability to &quot;scan the device&quot; than the iOS updater which can run arbitrary code supplied by Apple and read&#x2F;write anywhere on the system. This is less able to be extended to a general purpose scanner than that is.
        </div>
        <div class="children">
            
        </div>
    </div>


            
                


    
    <div class="comment hasChildren">
        <p><a href="https://news.ycombinator.com/user?id=treis" target="_blank">treis</a>   <span class="timeago" data-date="2021-08-18 14:50:18 &#43;0000 UTC">2021-08-18 14:50:18 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            Warrants are only part of the issue.  The bigger issue is civil and criminal liability.  If someone uploads child porn to iCloud and then shares it with someone else Apple is possessing and distributing child porn.<p>Today they aren&#x27;t liable because there is a law that shields them.  But that law comes with strings attached around assisting law enforcement. By doing an end run around those strings Apple is not holding up it&#x27;s end of the bargain and runs the risk of lawmakers removing the liability shield.<p>If that happens then it&#x27;s a whole new ball game.
        </div>
        <div class="children">
            
                


    
    <div class="comment hasChildren">
        <p><a href="https://news.ycombinator.com/user?id=brandon272" target="_blank">brandon272</a>   <span class="timeago" data-date="2021-08-18 15:30:03 &#43;0000 UTC">2021-08-18 15:30:03 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            &gt; If someone uploads child porn to iCloud and then shares it with someone else Apple is possessing and distributing child porn.<p>If someone sends CSAM using Federal Express, is FedEx legally liable for &quot;possessing and distributing&quot; that material? Does FedEx need to start opening up packages and scanning hard drives, DVDs, USB sticks, etc. to ensure that they don&#x27;t contain any CSAM or other illegal data?<p>I really struggle with the lengths that people are going to to justify these moves. If we can justify this, it&#x27;s pretty simple to justify a lot more surveillance as well. CSAM is not the only scourge in our society.<p>Reminder: Apple tells us that they consider privacy a &quot;fundamental human right&quot;[1]. That simply does not square with their recent announcement of on-device scanning, and some would argue that it does not square with scanning or content analysis <i>anywhere</i>, <i>especially</i> on behalf of government.<p>[1] <a href="https:&#x2F;&#x2F;apple.com&#x2F;privacy" rel="nofollow">https:&#x2F;&#x2F;apple.com&#x2F;privacy</a>
        </div>
        <div class="children">
            
                


    
    <div class="comment hasChildren">
        <p><a href="https://news.ycombinator.com/user?id=treis" target="_blank">treis</a>   <span class="timeago" data-date="2021-08-18 16:30:38 &#43;0000 UTC">2021-08-18 16:30:38 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            &gt;If someone sends CSAM using Federal Express, is FedEx legally liable for &quot;possessing and distributing&quot; that material?<p>Yes:<p><a href="https:&#x2F;&#x2F;www.dea.gov&#x2F;press-releases&#x2F;2014&#x2F;07&#x2F;18&#x2F;fedex-indicted-its-role-distributing-controlled-substances-and" rel="nofollow">https:&#x2F;&#x2F;www.dea.gov&#x2F;press-releases&#x2F;2014&#x2F;07&#x2F;18&#x2F;fedex-indicted...</a><p>That was for drugs but conceptually the same for CSAM.
        </div>
        <div class="children">
            
                


    
    <div class="comment hasChildren">
        <p><a href="https://news.ycombinator.com/user?id=brandon272" target="_blank">brandon272</a>   <span class="timeago" data-date="2021-08-18 16:55:28 &#43;0000 UTC">2021-08-18 16:55:28 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            This is not at all conceptually the same.<p>FedEx was not held liable simply because their service was used to mail illegal drugs. They were held liable because not only did they know about the specific instances in which it was mailed, they allegedly conspired with the shippers to facilitate the mailings. They were knowingly mailing packages to <i>parking lots</i> where drug dealers would wait to pick them up:<p>&gt; According to the indictment, as early as 2004, FedEx knew that it was delivering drugs to dealers and addicts.  FedEx’s couriers in Kentucky, Tennessee, and Virginia expressed safety concerns that were circulated to FedEx Senior management, including that FedEx trucks were stopped on the road by online pharmacy customers demanding packages of pills; that the delivery address was a parking lot, school, or vacant home where several car loads of people were waiting for the FedEx driver to arrive with their drugs; that customers were jumping on the FedEx trucks and demanding online pharmacy packages; and that FedEx drivers were threatened if they insisted on delivering packages to the addresses instead of giving the packages to customers who demanded them.<p>They had drug addicts literally stopping FedEx trucks on the street to intercept packages from online pharmacies. I don&#x27;t see how this specific situation is analogous to Apple at all.<p>The fact remains that FedEx is not scanning hard drives, DVD or other storage devices going through their network and they don&#x27;t appear to be breaking any laws by not doing that scanning.
        </div>
        <div class="children">
            
                


    
    <div class="comment hasChildren">
        <p><a href="https://news.ycombinator.com/user?id=treis" target="_blank">treis</a>   <span class="timeago" data-date="2021-08-18 18:56:17 &#43;0000 UTC">2021-08-18 18:56:17 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            &gt;They were held liable because not only did they know about the specific instances in which it was mailed, they allegedly conspired with the shippers to facilitate the mailings.<p>Apple knows that CSAM is being sent and conspires to do so (i.e. transmits the image).  Conceptually they are the same.<p>The rest of your post details the practical differences between sending physical packages and digital images.
        </div>
        <div class="children">
            
                


    
    <div class="comment hasChildren">
        <p><a href="https://news.ycombinator.com/user?id=brandon272" target="_blank">brandon272</a>   <span class="timeago" data-date="2021-08-18 22:20:59 &#43;0000 UTC">2021-08-18 22:20:59 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            I&#x27;m not even sure where to start here.<p>FedEx knows that CSAM is sent using its services in the same way that Apple does. That is to say that both companies know that CSAM <i>has</i> been sent historically and both know that is <i>possible</i> to send that type of material using its services, and one could argue that they should <i>assume</i> that their services are used to transfer that data.<p>Why does one of these companies have to go to such great depths to search out and report these materials and the other does not? If we suggest that Apple &quot;knows&quot; about CSAM on their network, we must also accept that Apple &quot;knows&quot; about many other crimes in which the devices they are sell are used in the planning and execution of those crimes. Why are they only focused on CSAM if they&#x27;d also have legal exposure in these other crimes simply for being a hardware and service provider?<p>Lastly, to suggest that Apple &quot;conspires&quot; to transmit this content is wholly inaccurate. Conspiracy implies two or more parties are in explicit agreement to commit a crime.
        </div>
        <div class="children">
            
                


    
    <div class="comment hasChildren">
        <p><a href="https://news.ycombinator.com/user?id=treis" target="_blank">treis</a>   <span class="timeago" data-date="2021-08-18 22:56:58 &#43;0000 UTC">2021-08-18 22:56:58 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            &gt;If we suggest that Apple &quot;knows&quot; about CSAM on their network, we must also accept that Apple &quot;knows&quot; about many other crimes in which the devices they are sell are used in the planning and execution of those crimes. Why are they only focused on CSAM if they&#x27;d also have legal exposure in these other crimes simply for being a hardware and service provider?<p>It&#x27;s not knowledge of a crime.  Apple is committing a crime by possessing and distributing CSAM.<p>&gt;Lastly, to suggest that Apple &quot;conspires&quot; to transmit this content is wholly inaccurate. Conspiracy implies two or more parties are in explicit agreement to commit a crime.<p>But that is exactly what they&#x27;re doing.  They are agreeing to possess and deliver images of which they know a portion are CSAM.
        </div>
        <div class="children">
            
                


    
    <div class="comment ">
        <p><a href="https://news.ycombinator.com/user?id=brandon272" target="_blank">brandon272</a>   <span class="timeago" data-date="2021-08-18 23:58:42 &#43;0000 UTC">2021-08-18 23:58:42 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            &gt; Apple is committing a crime by possessing and distributing CSAM.<p>Apple&#x27;s liability for content is limited by Section 230. If they find out about specific instances of CSAM on their servers they are required to remove it.<p>They are not required to hunt for it or otherwise search it out. They are not required to scan for it. Anything currently being done in that regard is voluntary.<p>&gt; But that is exactly what they&#x27;re doing. They are agreeing to possess and deliver images of which they know a portion are CSAM.<p>When you use Apple&#x27;s cloud services, you agree to multiple legally binding agreements that include provisions about not using their cloud services to store or transmit illegal materials including CSAM.<p>Someone using Apple&#x27;s services to do that in contravention of those agreements does not constitute them &quot;agreeing to possess and deliver&quot; that content.<p>What WOULD constitute them conspiracy and them agreeing to possess and deliver that content would be if they were informed of the <i>specific instance</i> of content being transmitted and delivered and did nothing about it or otherwise enabled its continued storage and distribution.
        </div>
        <div class="children">
            
        </div>
    </div>


            
        </div>
    </div>


            
        </div>
    </div>


            
        </div>
    </div>


            
        </div>
    </div>


            
        </div>
    </div>


            
        </div>
    </div>


            
        </div>
    </div>


            
        </div>
    </div>


            
                


    
    <div class="comment hasChildren">
        <p><a href="https://news.ycombinator.com/user?id=khuey" target="_blank">khuey</a>   <span class="timeago" data-date="2021-08-18 14:02:12 &#43;0000 UTC">2021-08-18 14:02:12 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            At least in the United States, this is absolutely false. A warrant cannot force them to do something they have no capability to do.
        </div>
        <div class="children">
            
                


    
    <div class="comment ">
        <p><a href="https://news.ycombinator.com/user?id=dheera" target="_blank">dheera</a>   <span class="timeago" data-date="2021-08-18 16:49:42 &#43;0000 UTC">2021-08-18 16:49:42 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            (a) They created the capability, that&#x27;s the problem<p>(b) The world is not the United States
        </div>
        <div class="children">
            
        </div>
    </div>


            
        </div>
    </div>


            
                


    
    <div class="comment ">
        <p><a href="https://news.ycombinator.com/user?id=snowwrestler" target="_blank">snowwrestler</a>   <span class="timeago" data-date="2021-08-18 14:37:49 &#43;0000 UTC">2021-08-18 14:37:49 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            I don’t think this is correct.<p>1) Even if Apple does roll out this system, it’s not a system for scanning files on customer devices. It’s a system for comparing photos being uploaded to iCloud to a single standardized (everyone has the same one) list of hashes.<p>2) Apple likely has the legal power to refuse to alter that list of hashes, by the same argument they used against the FBI’s request to bypass the unlock code on specific iPhones.<p>3) Any argument about what Apple will and will not need to do with this system needs to explain how Microsoft Defender (or other AV products) interact with law enforcement, since those are software systems that scan client devices (ALL files, typically) for signature matches.
        </div>
        <div class="children">
            
        </div>
    </div>


            
                


    
    <div class="comment hasChildren">
        <p><a href="https://news.ycombinator.com/user?id=" target="_blank"></a>   <span class="timeago" data-date="2021-08-18 13:47:57 &#43;0000 UTC">2021-08-18 13:47:57 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            
        </div>
        <div class="children">
            
                


    
    <div class="comment ">
        <p><a href="https://news.ycombinator.com/user?id=" target="_blank"></a>   <span class="timeago" data-date="2021-08-18 14:51:59 &#43;0000 UTC">2021-08-18 14:51:59 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            
        </div>
        <div class="children">
            
        </div>
    </div>


            
        </div>
    </div>


            
        </div>
    </div>


            
                


    
    <div class="comment hasChildren">
        <p><a href="https://news.ycombinator.com/user?id=Grustaf" target="_blank">Grustaf</a>   <span class="timeago" data-date="2021-08-18 13:14:19 &#43;0000 UTC">2021-08-18 13:14:19 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            How likely is it that you will have enough colliding images in your photo library to even trigger a review?<p>I&#x27;m guessing you need at least 5 images, perhaps much more, to trigger it. In any case, 1 image is definitely not enough.
        </div>
        <div class="children">
            
                


    
    <div class="comment hasChildren">
        <p><a href="https://news.ycombinator.com/user?id=sandworm101" target="_blank">sandworm101</a>   <span class="timeago" data-date="2021-08-18 13:22:07 &#43;0000 UTC">2021-08-18 13:22:07 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            For a normal random person, a grey man.  In the real world, a single collision could be enough for the police to acquire further access to people they are already looking at.  If the police want access to your phone for other reasons (drugs, taxes, illegal speech) they can use that one collision to get a warrant which will give them greater access.<p>It is akin to cops wanting to search a car.  They don&#x27;t need a warrant.  They only need to follow the car until it breaks any number of traffic laws.  Then the resulting traffic stops lets them talk to the driver, who seems evasive, which gives them probable cause, which gets them a warrant to perform a full search.  The accidental collision, week evidence of an offending image on the phone, opens the door to whatever other investigation they want to do.
        </div>
        <div class="children">
            
                


    
    <div class="comment ">
        <p><a href="https://news.ycombinator.com/user?id=merpnderp" target="_blank">merpnderp</a>   <span class="timeago" data-date="2021-08-18 14:22:39 &#43;0000 UTC">2021-08-18 14:22:39 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            Isn&#x27;t Apple seeding their database with fake hits so that a court can&#x27;t order them to turn over everyone who&#x27;s just a few shy of Apple&#x27;s threshold? So when this database leaks, won&#x27;t there be a lot of people accused of horrific crimes simply because Apple seeded their database with random hits?<p>Why would people pay a company to falsely accuse them of horrific crimes?
        </div>
        <div class="children">
            
        </div>
    </div>


            
                


    
    <div class="comment hasChildren">
        <p><a href="https://news.ycombinator.com/user?id=JKCalhoun" target="_blank">JKCalhoun</a>   <span class="timeago" data-date="2021-08-18 14:35:50 &#43;0000 UTC">2021-08-18 14:35:50 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            Since, as I have read, Microsoft and Google already do this, where are all the illicit cop take-downs? I have not heard of any.
        </div>
        <div class="children">
            
                


    
    <div class="comment hasChildren">
        <p><a href="https://news.ycombinator.com/user?id=sandworm101" target="_blank">sandworm101</a>   <span class="timeago" data-date="2021-08-18 14:55:12 &#43;0000 UTC">2021-08-18 14:55:12 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            They are scanning images on online accounts, stuff that is stored on their systems and system files on machines (files they own).  They are not, from what I have read, scanning customer-owned images stored on customer-owned devices.
        </div>
        <div class="children">
            
                


    
    <div class="comment hasChildren">
        <p><a href="https://news.ycombinator.com/user?id=snowwrestler" target="_blank">snowwrestler</a>   <span class="timeago" data-date="2021-08-18 15:06:54 &#43;0000 UTC">2021-08-18 15:06:54 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            Every AV (antivirus) software system scans customer-owned files on customer-owned devices. So the question is the same: if it’s easy for law enforcement to deputize such a system, where is the flood of cases built off of evidence gathered this way?
        </div>
        <div class="children">
            
                


    
    <div class="comment ">
        <p><a href="https://news.ycombinator.com/user?id=moe" target="_blank">moe</a>   <span class="timeago" data-date="2021-08-18 15:57:49 &#43;0000 UTC">2021-08-18 15:57:49 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            I feel like you are both chasing red herrings.<p>This is not about child porn nor about what AV software can or cannot do.<p>It&#x27;s about normalising mass surveillance and implementing populace control. They don&#x27;t want another Snowden to scare the public with reports about &quot;backdoors&quot; and mass privacy violations.<p>They want the coming generation to perceive it as normal. Because all phones do it, hasn&#x27;t it always been this way, and think of the children.<p>Oh, these dissidents in $distant_country that will be muted and killed using Apple&#x27;s shiny surveillance tech? Well, evil governments do what they do. But we are not like that. Over here it is only about the child predators. Trust us.<p>Apple has been an opponent of these developments for decades. Now they are spearheading it.
        </div>
        <div class="children">
            
        </div>
    </div>


            
                


    
    <div class="comment hasChildren">
        <p><a href="https://news.ycombinator.com/user?id=sandworm101" target="_blank">sandworm101</a>   <span class="timeago" data-date="2021-08-18 15:24:36 &#43;0000 UTC">2021-08-18 15:24:36 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            &gt;&gt;  Every AV (antivirus) software system<p>Mine doesn&#x27;t.  If Ubuntu or ClamAV is scanning all my photos and reporting the results to Canonical against my will then I will soon be having words with Mr. Linus.
        </div>
        <div class="children">
            
                


    
    <div class="comment hasChildren">
        <p><a href="https://news.ycombinator.com/user?id=snowwrestler" target="_blank">snowwrestler</a>   <span class="timeago" data-date="2021-08-18 15:42:02 &#43;0000 UTC">2021-08-18 15:42:02 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            If law enforcement can force software companies to adapt their software to serve law enforcement purposes, why aren’t Ubuntu or ClamAV doing so already? What makes them better at resisting requests from law enforcement than Apple?
        </div>
        <div class="children">
            
                


    
    <div class="comment ">
        <p><a href="https://news.ycombinator.com/user?id=sandworm101" target="_blank">sandworm101</a>   <span class="timeago" data-date="2021-08-18 17:14:38 &#43;0000 UTC">2021-08-18 17:14:38 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            &gt;&gt; why aren’t Ubuntu or ClamAV doing so already?<p>Because if some local police agency like the FBI were altering code on an open source project as large as ubuntu then the world would know about it pretty quickly.  The linux community would burn every bridge with Canonical and ubuntu would disappear overnight.  The idea of the FBI adding malware to ubuntu unnoticed, malware that openly scans files and reports to remote servers, is the stuff of comic books.
        </div>
        <div class="children">
            
        </div>
    </div>


            
                


    
    <div class="comment ">
        <p><a href="https://news.ycombinator.com/user?id=MichaelGroves" target="_blank">MichaelGroves</a>   <span class="timeago" data-date="2021-08-18 16:41:21 &#43;0000 UTC">2021-08-18 16:41:21 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            Canonical and ClamAV aren&#x27;t trying to curry favor with the US government to stave off antitrust action.
        </div>
        <div class="children">
            
        </div>
    </div>


            
        </div>
    </div>


            
        </div>
    </div>


            
        </div>
    </div>


            
                


    
    <div class="comment ">
        <p><a href="https://news.ycombinator.com/user?id=jasamer" target="_blank">jasamer</a>   <span class="timeago" data-date="2021-08-18 16:27:09 &#43;0000 UTC">2021-08-18 16:27:09 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            Apples solution only scans photos that will be uploaded, and the results of the scan require a server side to be evaluated. The client does not know whether a match occurred.
        </div>
        <div class="children">
            
        </div>
    </div>


            
        </div>
    </div>


            
        </div>
    </div>


            
                


    
    <div class="comment hasChildren">
        <p><a href="https://news.ycombinator.com/user?id=Grustaf" target="_blank">Grustaf</a>   <span class="timeago" data-date="2021-08-18 13:25:08 &#43;0000 UTC">2021-08-18 13:25:08 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            I&#x27;m not sure what you mean. If we are talking about Apple&#x27;s new feature, they will not even be alerted until you have enough matches.<p>And the police probably won&#x27;t be using any neural hashes if they have access to your device, so I&#x27;m not sure what you are talking about.
        </div>
        <div class="children">
            
                


    
    <div class="comment hasChildren">
        <p><a href="https://news.ycombinator.com/user?id=sandworm101" target="_blank">sandworm101</a>   <span class="timeago" data-date="2021-08-18 13:28:16 &#43;0000 UTC">2021-08-18 13:28:16 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            That is an Apple current policy, not any sort of law.  If the FBI wants to know if there are <i>any</i> collisions on a particular phone, that will be shared.  If the FBI wants to know of <i>all</i> single collisions, that too must be shared.  A corporation&#x27;s internal policy decisions are nothing when faced with a government official carrying a warrant.  If they have data indicating possible crimes, no matter how small, they can be forced to divulge it to investigators.<p>Unlike Lavabit, I don&#x27;t think Apple will ever close up shop to protect customer data.
        </div>
        <div class="children">
            
                


    
    <div class="comment hasChildren">
        <p><a href="https://news.ycombinator.com/user?id=Grustaf" target="_blank">Grustaf</a>   <span class="timeago" data-date="2021-08-18 13:32:40 &#43;0000 UTC">2021-08-18 13:32:40 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            Well of course if you believe that FBI can ask Apple to share anything and they will agree, and not tell us, then what is even the point of this discussion? Then they already have access to every photo, every email, every text. Because Apple controls those apps on your phone.
        </div>
        <div class="children">
            
                


    
    <div class="comment hasChildren">
        <p><a href="https://news.ycombinator.com/user?id=fsflover" target="_blank">fsflover</a>   <span class="timeago" data-date="2021-08-18 16:25:02 &#43;0000 UTC">2021-08-18 16:25:02 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            The difference is that now they <i>confirmed</i> that they do search your private images.
        </div>
        <div class="children">
            
                


    
    <div class="comment hasChildren">
        <p><a href="https://news.ycombinator.com/user?id=Grustaf" target="_blank">Grustaf</a>   <span class="timeago" data-date="2021-08-18 23:22:18 &#43;0000 UTC">2021-08-18 23:22:18 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            Why would that make it any likelier that they are also secretly searching your files, with another method? Seems completely orthogonal to me.
        </div>
        <div class="children">
            
                


    
    <div class="comment ">
        <p><a href="https://news.ycombinator.com/user?id=fsflover" target="_blank">fsflover</a>   <span class="timeago" data-date="2021-08-19 16:42:06 &#43;0000 UTC">2021-08-19 16:42:06 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            They showed that they are willing to do it in one case. Why not in other cases?
        </div>
        <div class="children">
            
        </div>
    </div>


            
        </div>
    </div>


            
        </div>
    </div>


            
        </div>
    </div>


            
                


    
    <div class="comment hasChildren">
        <p><a href="https://news.ycombinator.com/user?id=jasamer" target="_blank">jasamer</a>   <span class="timeago" data-date="2021-08-18 16:22:53 &#43;0000 UTC">2021-08-18 16:22:53 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            I thought the system is designed such that the threshold must be met to check the matches, i.e. it’s not possible to follow the FBIs request without changing the entire system.
        </div>
        <div class="children">
            
                


    
    <div class="comment ">
        <p><a href="https://news.ycombinator.com/user?id=brokenmachine" target="_blank">brokenmachine</a>   <span class="timeago" data-date="2021-08-19 01:44:37 &#43;0000 UTC">2021-08-19 01:44:37 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            int matchThreshold = 30;<p>if (numMatches &gt; matchThreshold) { snitch(customer) &amp;&amp; customer.life.ruin() }
        </div>
        <div class="children">
            
        </div>
    </div>


            
        </div>
    </div>


            
        </div>
    </div>


            
        </div>
    </div>


            
        </div>
    </div>


            
                


    
    <div class="comment hasChildren">
        <p><a href="https://news.ycombinator.com/user?id=gorbachev" target="_blank">gorbachev</a>   <span class="timeago" data-date="2021-08-18 14:15:51 &#43;0000 UTC">2021-08-18 14:15:51 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            For individual users the risk is very small, but when you have a billion users the chances of innocent people being caught up is pretty much 100%.<p>Platform owners like Google, Apple, Twitter and Facebook should really keep stuff like that in mind when they deploy algorithmic solutions like this.<p>Like dhosek said, the manual review step should reduce the risk quite a bit, though.
        </div>
        <div class="children">
            
                


    
    <div class="comment hasChildren">
        <p><a href="https://news.ycombinator.com/user?id=Grustaf" target="_blank">Grustaf</a>   <span class="timeago" data-date="2021-08-18 14:28:55 &#43;0000 UTC">2021-08-18 14:28:55 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            They did keep it in mind, that&#x27;s why they require 30 matches, which gets the false positive rate down to 1 in a trillion per year, so on average it will happen about 1 per millennium, given a billion users.<p>So that&#x27;s per photo library, not per photo. The rate per photo in their testing was 3 in 100 million, then they added a 30x safety margin and assumed it&#x27;s actually 1 in a million.<p><a href="https:&#x2F;&#x2F;www.zdnet.com&#x2F;article&#x2F;apple-to-tune-csam-system-to-keep-one-in-a-trillion-false-positive-deactivation-threshold&#x2F;" rel="nofollow">https:&#x2F;&#x2F;www.zdnet.com&#x2F;article&#x2F;apple-to-tune-csam-system-to-k...</a>
        </div>
        <div class="children">
            
                


    
    <div class="comment ">
        <p><a href="https://news.ycombinator.com/user?id=" target="_blank"></a>   <span class="timeago" data-date="2021-08-18 18:22:36 &#43;0000 UTC">2021-08-18 18:22:36 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            
        </div>
        <div class="children">
            
        </div>
    </div>


            
        </div>
    </div>


            
        </div>
    </div>


            
                


    
    <div class="comment hasChildren">
        <p><a href="https://news.ycombinator.com/user?id=JKCalhoun" target="_blank">JKCalhoun</a>   <span class="timeago" data-date="2021-08-18 14:33:18 &#43;0000 UTC">2021-08-18 14:33:18 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            Not sure why the downvotes. You are absolutely correct that a threshold in the number of positives (false or otherwise) must be met.<p>To be fair though we do not know what the threshold is. But I would guess even higher than 5 — I would presume 12 or more.<p>I&#x27;m no criminologist (IANAC) but when you read about someone getting busted with child pornography they have hundreds or thousand of images — not one, not five. They&#x27;re &quot;trading cards&quot; for these creeps.
        </div>
        <div class="children">
            
                


    
    <div class="comment ">
        <p><a href="https://news.ycombinator.com/user?id=cyanite" target="_blank">cyanite</a>   <span class="timeago" data-date="2021-08-18 20:46:33 &#43;0000 UTC">2021-08-18 20:46:33 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            Apple says they expect to use 30 as the threshold.
        </div>
        <div class="children">
            
        </div>
    </div>


            
        </div>
    </div>


            
                


    
    <div class="comment ">
        <p><a href="https://news.ycombinator.com/user?id=dhosek" target="_blank">dhosek</a>   <span class="timeago" data-date="2021-08-18 13:47:26 &#43;0000 UTC">2021-08-18 13:47:26 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            Apple has publicly stated 30. Plus, there would be a manual review of the images before forwarding to law enforcement. I don&#x27;t see this collision attack leading to innocent people being handed over to the police. The worst-case (best-case?) scenario is that Apple gives up on the whole hashing system and then there will be zero chance of there ever being end-to-end encryption of photos in iCloud.
        </div>
        <div class="children">
            
        </div>
    </div>


            
                


    
    <div class="comment ">
        <p><a href="https://news.ycombinator.com/user?id=floatingatoll" target="_blank">floatingatoll</a>   <span class="timeago" data-date="2021-08-18 14:52:29 &#43;0000 UTC">2021-08-18 14:52:29 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            I saw 30 mentioned somewhere, and I think that’s a more likely guess than 5. When men have a porn collection, it rarely has less than a hundred images, and so the threshold for CSAM detections can be set quite high in quantity, and enjoy a near-zero false positive rate aside from malicious hackers.<p>(And yes, 90% of CSAM abusers are men, so y’all will have to find some other way to weaken my argument.)
        </div>
        <div class="children">
            
        </div>
    </div>


            
        </div>
    </div>


            
                


    
    <div class="comment hasChildren">
        <p><a href="https://news.ycombinator.com/user?id=mschuster91" target="_blank">mschuster91</a>   <span class="timeago" data-date="2021-08-18 12:55:55 &#43;0000 UTC">2021-08-18 12:55:55 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            &gt; and there is no law who requires them to &quot;scan&quot; on device<p><i>Yet</i>. The demands by &quot;concerned parents&quot; (aka fronts for secret services, puritans&#x2F;other religious fundamentalists and law-and-order hardliners) to &quot;do something against child porn&quot; have grown ever more strong and insane over the last years. (And you can bet that what is used on CSAM will immediately be used to go after legal pornography, sex work, drug enforcement, ...)<p>Many current and powerful politicians don&#x27;t <i>understand</i> a single bit about computers, to the point of bragging of never having used one (e.g. Japan&#x27;s cybersecurity minister) or having assistants print out emails daily and transcribing handwritten responses (can&#x27;t say more than that this is the situation for <i>at least</i> two German MPs) - but there are young politicians who do, and will replace them over the next decade hopefully. That means it&#x27;s last chance for lobbying groups to get devastating laws passed through, and we must all be vigorous in spotting and preventing at least the worst of them.<p>And what is suspiciously lacking in Apple&#x27;s response: what are they going to do when they are compelled to extend the CSAM scanner by law in the US, India, China and&#x2F;or EU? It&#x27;s feasible for Apple to say they&#x27;ll just be sticking the middle finger towards markets such as Russia, Saudi-Arabia or similar tiny dictatorships, but the big markets? Apple can&#x27;t ignore these, and especially India and China are heavyweights.
        </div>
        <div class="children">
            
                


    
    <div class="comment hasChildren">
        <p><a href="https://news.ycombinator.com/user?id=Grustaf" target="_blank">Grustaf</a>   <span class="timeago" data-date="2021-08-18 13:27:47 &#43;0000 UTC">2021-08-18 13:27:47 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            Why would they extend the CSAM scanner? It would be much simpler to just use all the OCR and image classification functions they have already deployed.<p>CSAM scanning is only useful for areas where Apple really doesn&#x27;t want to even look at the actual material until they are extremely certain that it&#x27;s a match. If they want to detect anti-government propaganda or something, there would be no such concerns, they would just do a regex search of your inbox. Infinitely more convenient.
        </div>
        <div class="children">
            
                


    
    <div class="comment hasChildren">
        <p><a href="https://news.ycombinator.com/user?id=sandworm101" target="_blank">sandworm101</a>   <span class="timeago" data-date="2021-08-18 13:43:47 &#43;0000 UTC">2021-08-18 13:43:47 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            &gt;&gt; useful for areas where Apple really doesn&#x27;t want to even look at the actual material<p>Correct.  It has plausible deniability built in.  Apple is unable to verify that the images the government are looking for are actually CSAM.  They could be political.  They could be protest images.  They could be Winnie the Pooh.  Apple can plead ignorance as it blindly scans for whatever the requesting government asks it to scan for.<p>Nobody really minds that this system is going to be used for CSAM.  What everyone recognizes is how ripe this system is for abuse, how easily it can be leveraged by oppressive governments.  And Apple can play the innocent.
        </div>
        <div class="children">
            
                


    
    <div class="comment hasChildren">
        <p><a href="https://news.ycombinator.com/user?id=mlindner" target="_blank">mlindner</a>   <span class="timeago" data-date="2021-08-18 14:11:40 &#43;0000 UTC">2021-08-18 14:11:40 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            &gt; Nobody really minds that this system is going to be used for CSAM.<p>I beg to differ. It doesn&#x27;t matter how evil the content is, no scanning of my computers by outside parties, period. More so by scanning law enforcement can even plant legitimate child pornography on people&#x27;s computers and get convictions all the easier because the system self-reports.
        </div>
        <div class="children">
            
                


    
    <div class="comment ">
        <p><a href="https://news.ycombinator.com/user?id=Grustaf" target="_blank">Grustaf</a>   <span class="timeago" data-date="2021-08-18 14:18:21 &#43;0000 UTC">2021-08-18 14:18:21 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            It&#x27;s clearly not the scanning you are opposed to, it&#x27;s the reporting. Apple apps, well a lot of apps, already scan all your content, for classifying images, doing OCR, extracting dates and addresses etc. That is nothing new, it&#x27;s omnipresent.
        </div>
        <div class="children">
            
        </div>
    </div>


            
        </div>
    </div>


            
                


    
    <div class="comment hasChildren">
        <p><a href="https://news.ycombinator.com/user?id=jasamer" target="_blank">jasamer</a>   <span class="timeago" data-date="2021-08-18 16:34:07 &#43;0000 UTC">2021-08-18 16:34:07 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            What about the human reviewers at Apple? Those need to confirm matches, and they obviously look at the photos to do so. Apple can’t claim ignorance.<p>Also, it has to be two requesting governments.
        </div>
        <div class="children">
            
                


    
    <div class="comment ">
        <p><a href="https://news.ycombinator.com/user?id=sandworm101" target="_blank">sandworm101</a>   <span class="timeago" data-date="2021-08-18 16:49:13 &#43;0000 UTC">2021-08-18 16:49:13 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            The <i>current</i> Apple <i>policy</i> to use human reviewers.  That might change at a moment&#x27;s notice and there is no technical reason why Apple couldn&#x27;t bypass humans for certain requests.  One must always judge a new system three ways: how it was meant to be implemented, how it is actually implemented, and how it might be abused by bad actors in the future.
        </div>
        <div class="children">
            
        </div>
    </div>


            
        </div>
    </div>


            
                


    
    <div class="comment hasChildren">
        <p><a href="https://news.ycombinator.com/user?id=Grustaf" target="_blank">Grustaf</a>   <span class="timeago" data-date="2021-08-18 14:00:07 &#43;0000 UTC">2021-08-18 14:00:07 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            Except that Apple will review the photos once you&#x27;ve matched 30 of them, so it&#x27;s still not possible for the government to misuse it.
        </div>
        <div class="children">
            
                


    
    <div class="comment hasChildren">
        <p><a href="https://news.ycombinator.com/user?id=kps" target="_blank">kps</a>   <span class="timeago" data-date="2021-08-18 14:10:32 &#43;0000 UTC">2021-08-18 14:10:32 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            In China, iCloud is run by the government.
        </div>
        <div class="children">
            
                


    
    <div class="comment hasChildren">
        <p><a href="https://news.ycombinator.com/user?id=Grustaf" target="_blank">Grustaf</a>   <span class="timeago" data-date="2021-08-18 14:16:29 &#43;0000 UTC">2021-08-18 14:16:29 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            It&#x27;s actually not, but even if it were, that would be yet another reason CSAM scanning is completely irrelevant for government spying.<p>Any spying really. It&#x27;s much easier to just look at the images themselves.
        </div>
        <div class="children">
            
                


    
    <div class="comment ">
        <p><a href="https://news.ycombinator.com/user?id=ttflee" target="_blank">ttflee</a>   <span class="timeago" data-date="2021-08-18 15:29:34 &#43;0000 UTC">2021-08-18 15:29:34 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            &gt; It&#x27;s actually not<p>It&#x27;s Joe Wong&#x27;s joke, that it&#x27;s like peeing in the snow in a dark winter night, while there is a difference but it&#x27;s really hard to tell.
        </div>
        <div class="children">
            
        </div>
    </div>


            
        </div>
    </div>


            
        </div>
    </div>


            
                


    
    <div class="comment hasChildren">
        <p><a href="https://news.ycombinator.com/user?id=short_sells_poo" target="_blank">short_sells_poo</a>   <span class="timeago" data-date="2021-08-18 14:47:29 &#43;0000 UTC">2021-08-18 14:47:29 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            So some underpaid contractor reviewing &quot;visual derivatives&quot; that may or may not be CSAM completely prevents governments from misusing this in your mind?
        </div>
        <div class="children">
            
                


    
    <div class="comment hasChildren">
        <p><a href="https://news.ycombinator.com/user?id=Grustaf" target="_blank">Grustaf</a>   <span class="timeago" data-date="2021-08-18 16:18:36 &#43;0000 UTC">2021-08-18 16:18:36 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            Considering how high profile and incredibly sensitive this is, I doubt they will hire an underpaid contractor. It&#x27;s the opposite of app review, where the volume is very high but the stakes are low.
        </div>
        <div class="children">
            
                


    
    <div class="comment ">
        <p><a href="https://news.ycombinator.com/user?id=MichaelGroves" target="_blank">MichaelGroves</a>   <span class="timeago" data-date="2021-08-18 16:46:03 &#43;0000 UTC">2021-08-18 16:46:03 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            Facebook hires underpaid and poorly treated contractors to moderate exactly this same kind of content.  I see no reason to believe Apple will be any different, particularly a few months from now when the general public&#x27;s attention has shifted to other matters (assuming they&#x27;re even paying attention right now.   I don&#x27;t think the interests of HN are necessarily representative of the general public..)
        </div>
        <div class="children">
            
        </div>
    </div>


            
        </div>
    </div>


            
        </div>
    </div>


            
        </div>
    </div>


            
                


    
    <div class="comment hasChildren">
        <p><a href="https://news.ycombinator.com/user?id=ethbr0" target="_blank">ethbr0</a>   <span class="timeago" data-date="2021-08-18 13:49:40 &#43;0000 UTC">2021-08-18 13:49:40 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            Why even keep Apple in the loop? Why not just allow government to submit scanning models directly?<p>Which Apple will dutifully install and run, because they&#x27;re required by local laws.
        </div>
        <div class="children">
            
                


    
    <div class="comment hasChildren">
        <p><a href="https://news.ycombinator.com/user?id=jodrellblank" target="_blank">jodrellblank</a>   <span class="timeago" data-date="2021-08-18 14:18:28 &#43;0000 UTC">2021-08-18 14:18:28 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            &gt; &quot;<i>Which Apple will dutifully install and run, because they&#x27;re required by local laws.</i>&quot;<p>Which Apple have stated that they won&#x27;t do, and have designed the system so they can&#x27;t do that without it being found out: <a href="https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=28221082" rel="nofollow">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=28221082</a><p>Can&#x27;t you at least post accurate information about this system and support outrage based on facts instead of fantasy?
        </div>
        <div class="children">
            
                


    
    <div class="comment hasChildren">
        <p><a href="https://news.ycombinator.com/user?id=mschuster91" target="_blank">mschuster91</a>   <span class="timeago" data-date="2021-08-18 15:10:23 &#43;0000 UTC">2021-08-18 15:10:23 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            &gt; Which Apple have stated that they won&#x27;t do<p>For your random off-of-the-mill dictatorship, yes.<p>For the US? EU? China? India? No way they can refuse such a request from these markets. And if they could and get away with it, it would be a <i>very worrying</i> situation in itself regarding (democratic) control of government over global mega corporations.
        </div>
        <div class="children">
            
                


    
    <div class="comment hasChildren">
        <p><a href="https://news.ycombinator.com/user?id=jodrellblank" target="_blank">jodrellblank</a>   <span class="timeago" data-date="2021-08-18 15:28:04 &#43;0000 UTC">2021-08-18 15:28:04 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            If you believe &quot;the government can compel any company to do anything&quot;, then this system is no worse than any other.<p>Although how do you think Linus Torvalds &quot;manged to get away with refusing&quot; adding backdoors? <a href="https:&#x2F;&#x2F;www.techdirt.com&#x2F;articles&#x2F;20130919&#x2F;07485524578&#x2F;linus-torvalds-admits-he-was-approached-us-government-to-insert-backdoor-into-linux-did-he.shtml" rel="nofollow">https:&#x2F;&#x2F;www.techdirt.com&#x2F;articles&#x2F;20130919&#x2F;07485524578&#x2F;linus...</a>
        </div>
        <div class="children">
            
                


    
    <div class="comment ">
        <p><a href="https://news.ycombinator.com/user?id=mschuster91" target="_blank">mschuster91</a>   <span class="timeago" data-date="2021-08-18 16:58:29 &#43;0000 UTC">2021-08-18 16:58:29 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            Simple reason: every line of code in the Linux kernel passes many eyeballs in the open. Even if someone were to compel Linus Torvalds to commit a backdoor and he would comply with that order, people across many jurisdictions would notice this immediately, rendering the backdoor worthless.
        </div>
        <div class="children">
            
        </div>
    </div>


            
        </div>
    </div>


            
        </div>
    </div>


            
        </div>
    </div>


            
        </div>
    </div>


            
        </div>
    </div>


            
        </div>
    </div>


            
                


    
    <div class="comment hasChildren">
        <p><a href="https://news.ycombinator.com/user?id=jodrellblank" target="_blank">jodrellblank</a>   <span class="timeago" data-date="2021-08-18 14:08:05 &#43;0000 UTC">2021-08-18 14:08:05 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            &gt; &quot;<i>and what is suspiciously lacking in Apple&#x27;s response: what are they going to do when they are compelled to extend the CSAM scanner by law in the US, India, China and&#x2F;or EU?</i>&quot;<p>from <a href="https:&#x2F;&#x2F;daringfireball.net&#x2F;linked&#x2F;2021&#x2F;08&#x2F;09&#x2F;apple-csam-faq" rel="nofollow">https:&#x2F;&#x2F;daringfireball.net&#x2F;linked&#x2F;2021&#x2F;08&#x2F;09&#x2F;apple-csam-faq</a> - &quot;<i>we will not accede to any government’s request to expand it.</i>&quot;<p>From <a href="https:&#x2F;&#x2F;www.msn.com&#x2F;en-us&#x2F;news&#x2F;technology&#x2F;craig-federighi-says-apple-e2-80-99s-child-safety-scanning-will-have-e2-80-98multiple-levels-of-auditability-e2-80-99&#x2F;ar-AANhVhG" rel="nofollow">https:&#x2F;&#x2F;www.msn.com&#x2F;en-us&#x2F;news&#x2F;technology&#x2F;craig-federighi-sa...</a> - &quot;<i>“We ship the same software in China with the same database we ship in America, as we ship in Europe. If someone were to come to Apple [with a request to scan for data beyond CSAM], Apple would say no. But let’s say you aren’t confident. You don’t want to just rely on Apple saying no. You want to be sure that Apple couldn’t get away with it if we said yes,” he told the Journal. “There are multiple levels of auditability, and so we’re making sure that you don’t have to trust any one entity, or even any one country, as far as what images are part of this process.”</i>&quot; - Craig Federighi
        </div>
        <div class="children">
            
                


    
    <div class="comment ">
        <p><a href="https://news.ycombinator.com/user?id=jkestner" target="_blank">jkestner</a>   <span class="timeago" data-date="2021-08-18 14:20:24 &#43;0000 UTC">2021-08-18 14:20:24 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            A “government’s request” is different from a country’s law. “This process” is not the only process by which third parties can breach your privacy.
        </div>
        <div class="children">
            
        </div>
    </div>


            
        </div>
    </div>


            
        </div>
    </div>


            
        </div>
    </div>


            
                


    
    <div class="comment hasChildren">
        <p><a href="https://news.ycombinator.com/user?id=ksec" target="_blank">ksec</a>   <span class="timeago" data-date="2021-08-18 14:55:09 &#43;0000 UTC">2021-08-18 14:55:09 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            I actually want Apple to stand ground and implement this feature. Like you said the double down on PR and marketing was enough for me. I may not be dumping all iOS and Mac for now. But it was &quot;<i>the</i>&quot; definite signal and evidence this is no longer the old Steve Jobs&#x27;s Apple. It is like watching Mark Zuckerberg talking about privacy when he doesn&#x27;t understand anything about it. ( Or more like he has a different understand of privacy than most people )<p>If they stand ground it makes the decision a lot easier for others. If they dont they will continue to use the boiling frog tactics.<p>After all they dont think they are wrong and they are still <i>righteous</i>. And It would be far more interesting to see how the world folds. The whole computing market, Google, Apple and Microsoft are currently operating with a moat that is literally impenetrable. This will be a small push towards a new world of possibilities.<p>Yes. I really hope they stand ground. I wish I won the lottery I could put a few million into the Social Media echo chamber to support this. Or you know, <i>cough</i> certain Apple competitor could do this.
        </div>
        <div class="children">
            
                


    
    <div class="comment ">
        <p><a href="https://news.ycombinator.com/user?id=danuker" target="_blank">danuker</a>   <span class="timeago" data-date="2021-08-18 15:11:29 &#43;0000 UTC">2021-08-18 15:11:29 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            Chances are by the next sales report it will be forgotten. Let&#x27;s see how deep the memory hole goes.
        </div>
        <div class="children">
            
        </div>
    </div>


            
                


    
    <div class="comment hasChildren">
        <p><a href="https://news.ycombinator.com/user?id=mda" target="_blank">mda</a>   <span class="timeago" data-date="2021-08-18 17:43:34 &#43;0000 UTC">2021-08-18 17:43:34 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            &quot;Steve Jobs Apple&quot;?  I don&#x27;t think Jobs would give a damn about people crying about Apple&#x27;s decisions. I don&#x27;t know why people thinks he would be a smidgen better than whoever managing apple after him.
        </div>
        <div class="children">
            
                


    
    <div class="comment ">
        <p><a href="https://news.ycombinator.com/user?id=ksec" target="_blank">ksec</a>   <span class="timeago" data-date="2021-08-18 20:55:48 &#43;0000 UTC">2021-08-18 20:55:48 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            Because he actually understand privacy better than 99.9% of people in Silicon Valley. He is also a product person who understand how users feel. Compare to current Apple which is &quot;<i>still</i>&quot; trying to give me a technical explanation of what is and what&#x27;s not.
        </div>
        <div class="children">
            
        </div>
    </div>


            
        </div>
    </div>


            
                


    
    <div class="comment hasChildren">
        <p><a href="https://news.ycombinator.com/user?id=floatingatoll" target="_blank">floatingatoll</a>   <span class="timeago" data-date="2021-08-18 15:23:07 &#43;0000 UTC">2021-08-18 15:23:07 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            Steve Jobs would have implemented this in secret and never told us at all, same as they already did for iCloud photos so many years ago. That would have been a far better approach than today’s Apple is taking. Oh well.
        </div>
        <div class="children">
            
                


    
    <div class="comment hasChildren">
        <p><a href="https://news.ycombinator.com/user?id=dmix" target="_blank">dmix</a>   <span class="timeago" data-date="2021-08-18 16:25:39 &#43;0000 UTC">2021-08-18 16:25:39 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            There’s a significant leap from implementing something server side to on the consumer handheld devices themselves. Even if it’s just similar software it is regardless much more serious.<p>I personally thought the unencrypted backups was enough of a death-knell as it provides everything on your phone but anything with real-time on device access is always a gold mine for surveillance hawks.
        </div>
        <div class="children">
            
                


    
    <div class="comment hasChildren">
        <p><a href="https://news.ycombinator.com/user?id=floatingatoll" target="_blank">floatingatoll</a>   <span class="timeago" data-date="2021-08-18 17:17:30 &#43;0000 UTC">2021-08-18 17:17:30 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            It&#x27;s serious <i>to you</i>, but CSAM scanning is irrelevant to 99.99999% of Apple&#x27;s customers, and Jobs would never have allowed an announcement about migrating CSAM scanning <i>from</i> uploads to the cloud <i>to</i> the device uploading to the cloud. That&#x27;s an implementation detail that wouldn&#x27;t be relevant to discuss with outsiders. Instead, I expect he would have presented it in a closed session to the FBI and&#x2F;or Congress. Never to the press, not like this.
        </div>
        <div class="children">
            
                


    
    <div class="comment hasChildren">
        <p><a href="https://news.ycombinator.com/user?id=neolog" target="_blank">neolog</a>   <span class="timeago" data-date="2021-08-18 17:46:29 &#43;0000 UTC">2021-08-18 17:46:29 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            &gt; CSAM scanning is irrelevant to 99.99999% of Apple&#x27;s customers<p>How long until an group of governments tells Apple to add Tank Man to the list?
        </div>
        <div class="children">
            
                


    
    <div class="comment hasChildren">
        <p><a href="https://news.ycombinator.com/user?id=floatingatoll" target="_blank">floatingatoll</a>   <span class="timeago" data-date="2021-08-18 19:09:35 &#43;0000 UTC">2021-08-18 19:09:35 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            Why would they bother? That&#x27;s a terrible way to approach it.<p>Just pass legislation requiring in-country datacenters that can be decrypted by thoughtcrime enforcers, like Russia and China are doing. Trying to get this done via a CSAM list that&#x27;s absurdly closely audited would be a huge waste of time and not provide any significant benefit, <i>and</i> if such a request were ever made public, would likely result in severe political and economic sanctions.<p>That&#x27;s what everyone&#x27;s missing in this argument. There&#x27;s no <i>need</i> to be all underhanded and secretive when you can just pass laws and conduct military-backed demands upon companies using those laws. Trying to exploit the CSAM process would be a horrifically bad idea, and would result in public exposure and humiliation, rather than the much more useful outcome that simply passing a law would provide.
        </div>
        <div class="children">
            
                


    
    <div class="comment hasChildren">
        <p><a href="https://news.ycombinator.com/user?id=neolog" target="_blank">neolog</a>   <span class="timeago" data-date="2021-08-18 21:12:18 &#43;0000 UTC">2021-08-18 21:12:18 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            Without the technology deployed, Apple can (and did) say they don&#x27;t have the ability to break into users&#x27; phones.<p>If Apple deploys on-phone scanning, governments can just tell Apple to support a new list. It won&#x27;t be the NCMEC CSAM list. It will be a &quot;public safety and security&quot; list. I wouldn&#x27;t rule out underhandedness either. [1]<p>[1] <a href="https:&#x2F;&#x2F;www.nytimes.com&#x2F;2020&#x2F;07&#x2F;01&#x2F;technology&#x2F;china-uighurs-hackers-malware-hackers-smartphones.html" rel="nofollow">https:&#x2F;&#x2F;www.nytimes.com&#x2F;2020&#x2F;07&#x2F;01&#x2F;technology&#x2F;china-uighurs-...</a>
        </div>
        <div class="children">
            
                


    
    <div class="comment hasChildren">
        <p><a href="https://news.ycombinator.com/user?id=floatingatoll" target="_blank">floatingatoll</a>   <span class="timeago" data-date="2021-08-18 21:31:15 &#43;0000 UTC">2021-08-18 21:31:15 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            Apple already has technology deployed to perform binary file scans of every file on macOS and iOS, and the ability to at any time release signatures for those scans, that are very difficult for normal users to prevent updates for. They&#x27;ve had that for years, maybe even a decade by now, and so far to date we have seen no abuse of that list.<p>How is Apple&#x27;s new CSAM list somehow <i>increasing</i> the chances of Apple going rogue, given that we&#x27;ve all been living with that risk for the past X years?
        </div>
        <div class="children">
            
                


    
    <div class="comment hasChildren">
        <p><a href="https://news.ycombinator.com/user?id=neolog" target="_blank">neolog</a>   <span class="timeago" data-date="2021-08-18 21:45:46 &#43;0000 UTC">2021-08-18 21:45:46 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            What technology are you referring to as already deployed?
        </div>
        <div class="children">
            
                


    
    <div class="comment hasChildren">
        <p><a href="https://news.ycombinator.com/user?id=floatingatoll" target="_blank">floatingatoll</a>   <span class="timeago" data-date="2021-08-18 22:02:47 &#43;0000 UTC">2021-08-18 22:02:47 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            For macOS, I&#x27;m talking about XProtect and MRT. I don&#x27;t know the exact subsystem names on iOS, apologies.<p><a href="https:&#x2F;&#x2F;support.apple.com&#x2F;guide&#x2F;security&#x2F;protecting-against-malware-sec469d47bd8&#x2F;web" rel="nofollow">https:&#x2F;&#x2F;support.apple.com&#x2F;guide&#x2F;security&#x2F;protecting-against-...</a><p>Each system is closed source, provides a mechanism for checking content signatures against files on disk, and is thought to report telemetry to Apple when signatures are found.<p>How is CSAM scanning new and different from those existing closed-source systems?
        </div>
        <div class="children">
            
                


    
    <div class="comment hasChildren">
        <p><a href="https://news.ycombinator.com/user?id=neolog" target="_blank">neolog</a>   <span class="timeago" data-date="2021-08-18 22:39:09 &#43;0000 UTC">2021-08-18 22:39:09 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            I&#x27;d say the primary differences are that the CSAM scan is a perceptual hash rather than a regular file hash, and that the technical infrastructure of the CSAM system is designed from the ground up to be used against (rather than for) the user and report them individually to authorities for violation.
        </div>
        <div class="children">
            
                


    
    <div class="comment hasChildren">
        <p><a href="https://news.ycombinator.com/user?id=floatingatoll" target="_blank">floatingatoll</a>   <span class="timeago" data-date="2021-08-18 22:50:42 &#43;0000 UTC">2021-08-18 22:50:42 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            Do you have an alternate design in mind that is both &quot;used for the user&quot;, and is also effective at reporting CSAM content being uploaded from the device, without allowing CSAM abusers to opt-out of that reporting? I haven&#x27;t been able to come up with anything myself, but maybe you&#x27;ve had better luck.
        </div>
        <div class="children">
            
                


    
    <div class="comment ">
        <p><a href="https://news.ycombinator.com/user?id=neolog" target="_blank">neolog</a>   <span class="timeago" data-date="2021-08-19 00:27:38 &#43;0000 UTC">2021-08-19 00:27:38 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            I can only point to other people who know more than me.<p><a href="https:&#x2F;&#x2F;stratechery.com&#x2F;2021&#x2F;apples-mistake&#x2F;" rel="nofollow">https:&#x2F;&#x2F;stratechery.com&#x2F;2021&#x2F;apples-mistake&#x2F;</a> is a smart tech commentator<p><a href="https:&#x2F;&#x2F;www.nytimes.com&#x2F;2021&#x2F;08&#x2F;11&#x2F;opinion&#x2F;apple-iphones-privacy.html" rel="nofollow">https:&#x2F;&#x2F;www.nytimes.com&#x2F;2021&#x2F;08&#x2F;11&#x2F;opinion&#x2F;apple-iphones-pri...</a> are two security&#x2F;encryption experts
        </div>
        <div class="children">
            
        </div>
    </div>


            
        </div>
    </div>


            
        </div>
    </div>


            
        </div>
    </div>


            
        </div>
    </div>


            
        </div>
    </div>


            
        </div>
    </div>


            
        </div>
    </div>


            
                


    
    <div class="comment hasChildren">
        <p><a href="https://news.ycombinator.com/user?id=hughrr" target="_blank">hughrr</a>   <span class="timeago" data-date="2021-08-18 18:25:27 &#43;0000 UTC">2021-08-18 18:25:27 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            Exactly this. I see lots of people saying that Apple are forced to implement <i>something</i> via legislation as if it’s an excuse for it.<p>If they want to do business in China they will be forced by their legislation.<p>My entire argument is don’t build the mechanism.
        </div>
        <div class="children">
            
                


    
    <div class="comment hasChildren">
        <p><a href="https://news.ycombinator.com/user?id=floatingatoll" target="_blank">floatingatoll</a>   <span class="timeago" data-date="2021-08-18 19:19:39 &#43;0000 UTC">2021-08-18 19:19:39 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            China forced Apple by legislation to implement new iCloud algorithms for assigning China-region user data into China-hosted datacenters. Most countries, unlike the US, are <i>not</i> constrained by a requirement to only exercise previously-built mechanisms and not create new ones, in response to government demands. If China decides to require Apple to censor non-CSAM content on-device, they will do so <i>whether or not</i> CSAM content fingerprinting exists. That China has not done so is because they benefit greatly from Apple&#x27;s manufacturing and sales and do not wish to create a diplomatic incident with Apple.
        </div>
        <div class="children">
            
                


    
    <div class="comment hasChildren">
        <p><a href="https://news.ycombinator.com/user?id=cyanite" target="_blank">cyanite</a>   <span class="timeago" data-date="2021-08-18 20:44:56 &#43;0000 UTC">2021-08-18 20:44:56 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            &gt; China-hosted China-decryptable datacenters.<p>China hosted, yes, but Apple denies China-decryptable, so that’s speculation unless you have a good source.
        </div>
        <div class="children">
            
                


    
    <div class="comment ">
        <p><a href="https://news.ycombinator.com/user?id=floatingatoll" target="_blank">floatingatoll</a>   <span class="timeago" data-date="2021-08-18 20:59:11 &#43;0000 UTC">2021-08-18 20:59:11 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            Nope, that&#x27;s just me remembering wrong. Deleted those two words, thanks for the correction :)
        </div>
        <div class="children">
            
        </div>
    </div>


            
        </div>
    </div>


            
        </div>
    </div>


            
        </div>
    </div>


            
        </div>
    </div>


            
        </div>
    </div>


            
                


    
    <div class="comment hasChildren">
        <p><a href="https://news.ycombinator.com/user?id=tandav" target="_blank">tandav</a>   <span class="timeago" data-date="2021-08-18 17:26:42 &#43;0000 UTC">2021-08-18 17:26:42 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            until ios source code is closed all privacy claims is only backed by trust. They easily can do whatever they want if you&#x27;re not compiling from source. There&#x27;s no way to ensure your data is not leaving your iphone&#x2F;mac with some &quot;system&quot; network requests.
        </div>
        <div class="children">
            
                


    
    <div class="comment ">
        <p><a href="https://news.ycombinator.com/user?id=cyanite" target="_blank">cyanite</a>   <span class="timeago" data-date="2021-08-18 20:45:33 &#43;0000 UTC">2021-08-18 20:45:33 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            Security researchers and hackers routinely look at the code on the device, though.
        </div>
        <div class="children">
            
        </div>
    </div>


            
        </div>
    </div>


            
        </div>
    </div>


            
                


    
    <div class="comment hasChildren">
        <p><a href="https://news.ycombinator.com/user?id=ksec" target="_blank">ksec</a>   <span class="timeago" data-date="2021-08-18 20:48:15 &#43;0000 UTC">2021-08-18 20:48:15 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            Well Steve Jobs was the one that resisted PRISM and gave the middle finger to NSA.
        </div>
        <div class="children">
            
                


    
    <div class="comment ">
        <p><a href="https://news.ycombinator.com/user?id=floatingatoll" target="_blank">floatingatoll</a>   <span class="timeago" data-date="2021-08-18 20:56:13 &#43;0000 UTC">2021-08-18 20:56:13 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            NSA PRISM was and illegal and warrantless text analysis and search system for most communications on the Internet, collected all communications without bothering to filter at all, and had no protections in place to prevent people from randomly searching and reading content out of human curiosity.<p>Apple&#x27;s CSAM implementation protects the user against algorithm defects, does not expose the user to legal trouble until they have at least 30 human-verified visual matches of CSAM content, and occurs on-device using a small set of confirmed and audited signatures to ensure that CSAM scanning requires a central system only for verifying the unverified, blurred, positive matches.<p>I would hesitate to compare Steve Jobs&#x27; views on PRISM to his likely views on something that is so clearly opposed to it in so many ways. So I do not yet understand your viewpoint that Apple CSAM scanning and PRISM would have been treated equally by Steve. Help me understand?
        </div>
        <div class="children">
            
        </div>
    </div>


            
        </div>
    </div>


            
        </div>
    </div>


            
                


    
    <div class="comment ">
        <p><a href="https://news.ycombinator.com/user?id=" target="_blank"></a>   <span class="timeago" data-date="2021-08-18 15:08:35 &#43;0000 UTC">2021-08-18 15:08:35 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            
        </div>
        <div class="children">
            
        </div>
    </div>


            
        </div>
    </div>


            
                


    
    <div class="comment hasChildren">
        <p><a href="https://news.ycombinator.com/user?id=devwastaken" target="_blank">devwastaken</a>   <span class="timeago" data-date="2021-08-18 14:26:13 &#43;0000 UTC">2021-08-18 14:26:13 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            The collisions are supposedly reviewed, my concern is that the process for photodna isn&#x27;t going to always be the same, and we don&#x27;t actually have any knowledge as to whether their claims are true. Eventually they will phase out human intervention and replace with gameable AI. 3 letter agencies don&#x27;t need to review the actual images, they can easily get federal warrants based on some numbers.<p>Apple is content with putting in backdoors. Theres collusion behind the scenes here, they know it&#x27;s bad, but hey, it&#x27;s apple, it&#x27;s ran by manipulators and liars and forms it&#x27;s own cult.
        </div>
        <div class="children">
            
                


    
    <div class="comment hasChildren">
        <p><a href="https://news.ycombinator.com/user?id=atoav" target="_blank">atoav</a>   <span class="timeago" data-date="2021-08-18 14:44:24 &#43;0000 UTC">2021-08-18 14:44:24 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            One thing about this is that we are farther and farther away from &quot;if there is evidence for a crime a jury can understand it and rationally decide what do with it&quot; and we are getting closer to &quot;if this lightbulb is glowing the machine says they are guilty, so better trust us&quot;.<p>This kind of stuff should not be evidence, if anything it should be a indicator where to look.
        </div>
        <div class="children">
            
                


    
    <div class="comment ">
        <p><a href="https://news.ycombinator.com/user?id=" target="_blank"></a>   <span class="timeago" data-date="2021-08-18 16:44:16 &#43;0000 UTC">2021-08-18 16:44:16 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            
        </div>
        <div class="children">
            
        </div>
    </div>


            
                


    
    <div class="comment ">
        <p><a href="https://news.ycombinator.com/user?id=hannasanarion" target="_blank">hannasanarion</a>   <span class="timeago" data-date="2021-08-18 19:03:02 &#43;0000 UTC">2021-08-18 19:03:02 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            In what case do you think the hash will be introduced as evidence, where the actual image being hashed, which is a grey blob, cannot?
        </div>
        <div class="children">
            
        </div>
    </div>


            
        </div>
    </div>


            
        </div>
    </div>


            
                


    
    <div class="comment hasChildren">
        <p><a href="https://news.ycombinator.com/user?id=vegetablepotpie" target="_blank">vegetablepotpie</a>   <span class="timeago" data-date="2021-08-18 13:10:27 &#43;0000 UTC">2021-08-18 13:10:27 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            &gt; I’ve dumped the entire iOS ecosystem in the last week.<p>Not to go off topic from your main point, but what did you move to?
        </div>
        <div class="children">
            
                


    
    <div class="comment hasChildren">
        <p><a href="https://news.ycombinator.com/user?id=ByteWelder" target="_blank">ByteWelder</a>   <span class="timeago" data-date="2021-08-18 13:43:47 &#43;0000 UTC">2021-08-18 13:43:47 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            I&#x27;m not the one you responded to, but did something similar and feel like sharing:<p>- Desktop: Manjaro Gnome, for that amazing macOS-like desktop. It even does the 3 finger swipe up to see all your apps with Apple&#x27;s Touchpad.<p>- Phone: OnePlus 8T with microG variant of LineageOS  (alternatives: Pixel line-up), because that allows me to still receive push notifications. I have over 40 apps and only found 1 that didn&#x27;t work so far (which is Uber Eats, because they seem to require Google Advertisement ID). I pushed a modified Google Camera app to it, so my camera is better supported. I think only 3 out of 4 cameras are working, but I don&#x27;t care.<p>- Watch: Amazfit GTR 2e with the official app. Alternatively it should work with Gadgetbridge if you don&#x27;t want to use the offical app (&quot;Zepp&quot;). Amazfit GTR 2 is a better option if you want it to have WiFi and want to store music on it.
        </div>
        <div class="children">
            
                


    
    <div class="comment ">
        <p><a href="https://news.ycombinator.com/user?id=drvdevd" target="_blank">drvdevd</a>   <span class="timeago" data-date="2021-08-18 17:31:51 &#43;0000 UTC">2021-08-18 17:31:51 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            Thanks for sharing. Time to start compiling these lists.
        </div>
        <div class="children">
            
        </div>
    </div>


            
        </div>
    </div>


            
                


    
    <div class="comment hasChildren">
        <p><a href="https://news.ycombinator.com/user?id=hughrr" target="_blank">hughrr</a>   <span class="timeago" data-date="2021-08-18 13:13:21 &#43;0000 UTC">2021-08-18 13:13:21 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            Linux, dumbphone, DSLR. This was the final straw on a planned exit to be honest.
        </div>
        <div class="children">
            
                


    
    <div class="comment hasChildren">
        <p><a href="https://news.ycombinator.com/user?id=rootsudo" target="_blank">rootsudo</a>   <span class="timeago" data-date="2021-08-18 13:40:39 &#43;0000 UTC">2021-08-18 13:40:39 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            That&#x27;s interesting, I may just do the same thing as well - the camera is the largest thing of why I want to be on an phone.<p>I may just go LineageOS.
        </div>
        <div class="children">
            
                


    
    <div class="comment hasChildren">
        <p><a href="https://news.ycombinator.com/user?id=Sanzig" target="_blank">Sanzig</a>   <span class="timeago" data-date="2021-08-18 14:10:33 &#43;0000 UTC">2021-08-18 14:10:33 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            If you have a Pixel, or are willing to buy one, CalyxOS [1] may be worth a look. It&#x27;s a privacy-focused ROM that still integrates microG, so it&#x27;s compatible with most apps (unlike the other popular privacy-focused ROM GrapheneOS [2] which doesn&#x27;t support microG).<p>The big advantage to CalyxOS or GrapheneOS versus Lineage is they support re-locking the bootloader, which means that verified boot still works - important if your phone gets swiped.<p>[1] <a href="https:&#x2F;&#x2F;calyxos.org&#x2F;" rel="nofollow">https:&#x2F;&#x2F;calyxos.org&#x2F;</a>
 [2] <a href="https:&#x2F;&#x2F;grapheneos.org&#x2F;" rel="nofollow">https:&#x2F;&#x2F;grapheneos.org&#x2F;</a>
        </div>
        <div class="children">
            
                


    
    <div class="comment ">
        <p><a href="https://news.ycombinator.com/user?id=sphinxcdi" target="_blank">sphinxcdi</a>   <span class="timeago" data-date="2021-08-19 14:17:08 &#43;0000 UTC">2021-08-19 14:17:08 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            &gt; It&#x27;s a privacy-focused ROM that still integrates microG, so it&#x27;s compatible with most apps (unlike the other popular privacy-focused ROM GrapheneOS [2] which doesn&#x27;t support microG).<p>Many apps are also compatible with GrapheneOS and installing <a href="https:&#x2F;&#x2F;grapheneos.org&#x2F;usage#sandboxed-play-services" rel="nofollow">https:&#x2F;&#x2F;grapheneos.org&#x2F;usage#sandboxed-play-services</a> provides broader app compatibility.
        </div>
        <div class="children">
            
        </div>
    </div>


            
                


    
    <div class="comment ">
        <p><a href="https://news.ycombinator.com/user?id=rootsudo" target="_blank">rootsudo</a>   <span class="timeago" data-date="2021-08-18 21:40:47 &#43;0000 UTC">2021-08-18 21:40:47 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            Thank you for the info, I will go buy and experiment about this.
        </div>
        <div class="children">
            
        </div>
    </div>


            
        </div>
    </div>


            
        </div>
    </div>


            
        </div>
    </div>


            
        </div>
    </div>


            
                


    
    <div class="comment hasChildren">
        <p><a href="https://news.ycombinator.com/user?id=zepto" target="_blank">zepto</a>   <span class="timeago" data-date="2021-08-18 14:02:37 &#43;0000 UTC">2021-08-18 14:02:37 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            Vindicated?<p>This can’t be used for a targeted attack.  It’s no different from the previous false posting making this claim.
        </div>
        <div class="children">
            
                


    
    <div class="comment hasChildren">
        <p><a href="https://news.ycombinator.com/user?id=zekrioca" target="_blank">zekrioca</a>   <span class="timeago" data-date="2021-08-18 18:01:11 &#43;0000 UTC">2021-08-18 18:01:11 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            Imagine if WhatsApp and other apps added received photos to iPhone&#x27;s iCloud Photos gallery by default.. wait, they do.
        </div>
        <div class="children">
            
                


    
    <div class="comment ">
        <p><a href="https://news.ycombinator.com/user?id=zepto" target="_blank">zepto</a>   <span class="timeago" data-date="2021-08-18 18:12:23 &#43;0000 UTC">2021-08-18 18:12:23 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            Imagine if Apps had to ask permission before they could save photos…<p>Imagine if the local photo storage <i>is not</i> the same as iCloud Photo Library…<p>Imagine if anyone who cared could simply switch off iCloud Photo Library…
        </div>
        <div class="children">
            
        </div>
    </div>


            
        </div>
    </div>


            
        </div>
    </div>


            
                


    
    <div class="comment ">
        <p><a href="https://news.ycombinator.com/user?id=drvdevd" target="_blank">drvdevd</a>   <span class="timeago" data-date="2021-08-18 17:12:28 &#43;0000 UTC">2021-08-18 17:12:28 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            &gt; Within a few years your entire frame buffer and camera will be working against you full time.<p>This. This is hard enough to explain but with all the PR around whether the feature is good or bad it&#x27;s even more difficult. Putting this functionality on device is equivalent to modifying the hardware directly, because software is eating the world.
        </div>
        <div class="children">
            
        </div>
    </div>


            
                


    
    <div class="comment hasChildren">
        <p><a href="https://news.ycombinator.com/user?id=gjsman-1000" target="_blank">gjsman-1000</a>   <span class="timeago" data-date="2021-08-18 14:16:32 &#43;0000 UTC">2021-08-18 14:16:32 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            It&#x27;s not really end-game, because the original hashes are, themselves, hashed and not available (so you don&#x27;t have a hash to work towards). And second, even if you somehow managed to get over that huge leap, raw noise won&#x27;t pass Apple&#x27;s review, so you have to reverse-engineer a new image that looks like CSAM, to match a hash you don&#x27;t have. Big leaps required.
        </div>
        <div class="children">
            
                


    
    <div class="comment ">
        <p><a href="https://news.ycombinator.com/user?id=cyanite" target="_blank">cyanite</a>   <span class="timeago" data-date="2021-08-18 20:50:31 &#43;0000 UTC">2021-08-18 20:50:31 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            Technical nitpick: the hashes are not hashed (again), but rather cryptographically blinded. This is not reversible, so seems like a hash, but computations can be done on it which result can in some cases be seen by the server. This is actually all very clever, and described in a paper by Apple, linked from the technical summary article.
        </div>
        <div class="children">
            
        </div>
    </div>


            
        </div>
    </div>


            
                


    
    <div class="comment ">
        <p><a href="https://news.ycombinator.com/user?id=ballenf" target="_blank">ballenf</a>   <span class="timeago" data-date="2021-08-18 15:21:53 &#43;0000 UTC">2021-08-18 15:21:53 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            Is it theoretically possible that photo filter effect could raise the false positive rate to make the 1 &#x2F; a trillion number off by several order of magnitude?<p>Could be either malicious or accidental (less likely presumably).
        </div>
        <div class="children">
            
        </div>
    </div>


            
                


    
    <div class="comment ">
        <p><a href="https://news.ycombinator.com/user?id=Shorel" target="_blank">Shorel</a>   <span class="timeago" data-date="2021-08-18 15:23:09 &#43;0000 UTC">2021-08-18 15:23:09 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            That&#x27;s just a checkbox that indicates further action is required. All hashing algorithms have potential collisions.<p>Now, the hashes&#x27; database is not public, AFAIK it is in the hands of the FBI.
        </div>
        <div class="children">
            
        </div>
    </div>


            
                


    
    <div class="comment ">
        <p><a href="https://news.ycombinator.com/user?id=robertoandred" target="_blank">robertoandred</a>   <span class="timeago" data-date="2021-08-18 14:59:34 &#43;0000 UTC">2021-08-18 14:59:34 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            You feel vindicated because someone made one image look like another?
        </div>
        <div class="children">
            
        </div>
    </div>


            
                


    
    <div class="comment ">
        <p><a href="https://news.ycombinator.com/user?id=spullara" target="_blank">spullara</a>   <span class="timeago" data-date="2021-08-18 17:47:20 &#43;0000 UTC">2021-08-18 17:47:20 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            What exactly is the targeted attack? To get a bunch of someone&#x27;s random looking photos reviewed by a human?
        </div>
        <div class="children">
            
        </div>
    </div>


            
                


    
    <div class="comment ">
        <p><a href="https://news.ycombinator.com/user?id=dgb99" target="_blank">dgb99</a>   <span class="timeago" data-date="2021-08-18 14:56:16 &#43;0000 UTC">2021-08-18 14:56:16 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            can someone upvote me please? I need to add my account to my keybase profile.
        </div>
        <div class="children">
            
        </div>
    </div>


            
                


    
    <div class="comment ">
        <p><a href="https://news.ycombinator.com/user?id=elisbce" target="_blank">elisbce</a>   <span class="timeago" data-date="2021-08-18 10:22:34 &#43;0000 UTC">2021-08-18 10:22:34 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            You are insane.<p>1. Dumping iOS ecosystem and pick what? You think Android is better and won&#x27;t have this? iOS is the strongest mobile system in terms of privacy protection available to this date. Hell, the FBI doesn&#x27;t even need to ask Google to decrypt an Android.<p>2. Theoretically you can target attacks against anyone. It is just a matter of efforts. If you are a political target, they can already implant spywares around you to track and monitor you. They don&#x27;t even need to break your phone.<p>3. If you are not possessing CSAM materials and not one of those targets, then you are not worth the efforts to be attacked or monitored. They don&#x27;t care. And to be honest, this is the best(might be the only real) way to stay private.
        </div>
        <div class="children">
            
        </div>
    </div>


            
                


    
    <div class="comment hasChildren">
        <p><a href="https://news.ycombinator.com/user?id=user-the-name" target="_blank">user-the-name</a>   <span class="timeago" data-date="2021-08-18 10:13:01 &#43;0000 UTC">2021-08-18 10:13:01 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            It can&#x27;t. No actions are taken on hashes alone. The procedure is, if an account uploads some number of images with matching hashes, those images are verified by a human.<p>This <i>can</i> attack that system itself, though, by overloading those humans with too much work looking at random noise, but that requires quite a large organised effort. It also requires getting a hold of actual blacklisted hashes, which I doubt anyone has, unless they have <i>actual</i> child pornography.
        </div>
        <div class="children">
            
                


    
    <div class="comment hasChildren">
        <p><a href="https://news.ycombinator.com/user?id=reuben_scratton" target="_blank">reuben_scratton</a>   <span class="timeago" data-date="2021-08-18 10:17:53 &#43;0000 UTC">2021-08-18 10:17:53 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            &quot;Verified by a human&quot; - and that human will be an overworked, underpaid, overseas subcontractor who may well have an incentive to mash the &quot;Confirm match&quot; button from time to time to improve his performance.
        </div>
        <div class="children">
            
                


    
    <div class="comment ">
        <p><a href="https://news.ycombinator.com/user?id=nomel" target="_blank">nomel</a>   <span class="timeago" data-date="2021-08-18 16:48:16 &#43;0000 UTC">2021-08-18 16:48:16 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            The &quot;verified by a human&quot; is only the the last step of the process for Apple. After that, it&#x27;s given to NCMEC for review, then it&#x27;s reported to authorities. The Apple subcontractor isn&#x27;t the only one verifying things here. What you&#x27;re saying is not realistic.
        </div>
        <div class="children">
            
        </div>
    </div>


            
                


    
    <div class="comment hasChildren">
        <p><a href="https://news.ycombinator.com/user?id=apetrovic" target="_blank">apetrovic</a>   <span class="timeago" data-date="2021-08-18 12:50:37 &#43;0000 UTC">2021-08-18 12:50:37 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            I find it very hard to believe that Apple will put literal future of iPhone (just imagine the amount of bad press if one false negative comes out of review process and is reported to the authorities) to &quot;overworked, underpaid overseas subcontractor&quot;.<p>Apple is greedy, ruthless, tone-deaf machine, but I don&#x27;t think they&#x27;re stupid.
        </div>
        <div class="children">
            
                


    
    <div class="comment hasChildren">
        <p><a href="https://news.ycombinator.com/user?id=notriddle" target="_blank">notriddle</a>   <span class="timeago" data-date="2021-08-18 14:08:17 &#43;0000 UTC">2021-08-18 14:08:17 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            That’s already how app reviews work.
        </div>
        <div class="children">
            
                


    
    <div class="comment hasChildren">
        <p><a href="https://news.ycombinator.com/user?id=apetrovic" target="_blank">apetrovic</a>   <span class="timeago" data-date="2021-08-18 15:00:16 &#43;0000 UTC">2021-08-18 15:00:16 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            I think rejecting one of million apps and angering one developer is a bit different than reporting someone about a child pornography&#x2F;abuse, ruining their life and making a global scandal that can put future iPhone sales in jeopardy.
        </div>
        <div class="children">
            
                


    
    <div class="comment ">
        <p><a href="https://news.ycombinator.com/user?id=read_if_gay_" target="_blank">read_if_gay_</a>   <span class="timeago" data-date="2021-08-18 19:00:43 &#43;0000 UTC">2021-08-18 19:00:43 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            Barely anyone ever cared about privacy. Convenience, no matter how insignificant, always wins. I will be surprised if this is the last straw that finally gets the masses to care about privacy. In reality there might be a few scandals but at some point no one will care anymore and Apple will remain popular.
        </div>
        <div class="children">
            
        </div>
    </div>


            
        </div>
    </div>


            
        </div>
    </div>


            
        </div>
    </div>


            
                


    
    <div class="comment hasChildren">
        <p><a href="https://news.ycombinator.com/user?id=short_sells_poo" target="_blank">short_sells_poo</a>   <span class="timeago" data-date="2021-08-18 12:09:53 &#43;0000 UTC">2021-08-18 12:09:53 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            Beyond that, are we seriously going to ignore the fact that eventually this system will share the private photos of someone&#x27;s naked kid with some random subcontractor? How is that even remotely OK? Or that it will find and share actual CSAM with said subcontractor?
        </div>
        <div class="children">
            
                


    
    <div class="comment hasChildren">
        <p><a href="https://news.ycombinator.com/user?id=FabHK" target="_blank">FabHK</a>   <span class="timeago" data-date="2021-08-18 12:18:36 &#43;0000 UTC">2021-08-18 12:18:36 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            It only shares &quot;visual derivatives&quot; of images whose NeuralHash match the NeuralHash of known CSAM (either by being the same image (&quot;perceptually&quot;) or a collision).
        </div>
        <div class="children">
            
                


    
    <div class="comment hasChildren">
        <p><a href="https://news.ycombinator.com/user?id=short_sells_poo" target="_blank">short_sells_poo</a>   <span class="timeago" data-date="2021-08-18 14:32:16 &#43;0000 UTC">2021-08-18 14:32:16 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            That to me just sounds like weasel words to avoid having to say that it shares images. Let&#x27;s not beat about the bush, the &quot;visual derivative&quot; has to be good enough to identify what&#x27;s going on in it for the manual confirmation.<p>Are you actually arguing in good faith here at all? Because I can&#x27;t see how a &quot;visual derivative&quot; that&#x27;s nevertheless good enough for manual confirmation is any better than the source image?
        </div>
        <div class="children">
            
                


    
    <div class="comment ">
        <p><a href="https://news.ycombinator.com/user?id=cyanite" target="_blank">cyanite</a>   <span class="timeago" data-date="2021-08-18 20:52:37 &#43;0000 UTC">2021-08-18 20:52:37 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            &gt; Are you actually arguing in good faith here at all?<p>Are you? Because you just seemed to claim that it could match against innocent pictures of your naked children, but this tells me that you don’t understand that this system looks for <i>known</i> pictures, not for something that looks like naked children.<p>Edit: if you do, apologies, but then I’d say that Apple has suggested that it’s a low resolution version of the picture. This should be contrasted with server side scanning, where the server accesses all pictures fully.
        </div>
        <div class="children">
            
        </div>
    </div>


            
        </div>
    </div>


            
        </div>
    </div>


            
                


    
    <div class="comment hasChildren">
        <p><a href="https://news.ycombinator.com/user?id=stetrain" target="_blank">stetrain</a>   <span class="timeago" data-date="2021-08-18 13:28:06 &#43;0000 UTC">2021-08-18 13:28:06 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            This system does not use ML to find new CSAM images. It only checks for ones already in a known database. Your pictures of kids in the bathtub are not on the list.<p>What is show to the reviewer is a &quot;visual derivative&quot; which hasn&#x27;t been clearly defined. A thumbnail image? Something with a censored section? We don&#x27;t really know.
        </div>
        <div class="children">
            
                


    
    <div class="comment hasChildren">
        <p><a href="https://news.ycombinator.com/user?id=short_sells_poo" target="_blank">short_sells_poo</a>   <span class="timeago" data-date="2021-08-18 14:30:17 &#43;0000 UTC">2021-08-18 14:30:17 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            Yes I&#x27;m aware that it checks against a known database but clearly there can be collisions. So eventually it will share someone&#x27;s private images.
        </div>
        <div class="children">
            
                


    
    <div class="comment hasChildren">
        <p><a href="https://news.ycombinator.com/user?id=bengale" target="_blank">bengale</a>   <span class="timeago" data-date="2021-08-18 15:25:55 &#43;0000 UTC">2021-08-18 15:25:55 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            It would need to have 30 collisions before anything even took place, which realistically isn&#x27;t going to happen.
        </div>
        <div class="children">
            
                


    
    <div class="comment ">
        <p><a href="https://news.ycombinator.com/user?id=brokenmachine" target="_blank">brokenmachine</a>   <span class="timeago" data-date="2021-08-19 02:12:00 &#43;0000 UTC">2021-08-19 02:12:00 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            Most cameras nowadays can easily take a burst of 30 visually very similar images in a second.<p>Lots of people leave their cameras in burst mode.
        </div>
        <div class="children">
            
        </div>
    </div>


            
        </div>
    </div>


            
                


    
    <div class="comment hasChildren">
        <p><a href="https://news.ycombinator.com/user?id=robertoandred" target="_blank">robertoandred</a>   <span class="timeago" data-date="2021-08-18 14:39:22 &#43;0000 UTC">2021-08-18 14:39:22 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            Or rather, it will share some gray blob apparently.
        </div>
        <div class="children">
            
                


    
    <div class="comment hasChildren">
        <p><a href="https://news.ycombinator.com/user?id=short_sells_poo" target="_blank">short_sells_poo</a>   <span class="timeago" data-date="2021-08-18 15:23:39 &#43;0000 UTC">2021-08-18 15:23:39 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            What is the point of sharing a gray blob? How is that going to prove anything?
        </div>
        <div class="children">
            
                


    
    <div class="comment ">
        <p><a href="https://news.ycombinator.com/user?id=stetrain" target="_blank">stetrain</a>   <span class="timeago" data-date="2021-08-18 16:04:50 &#43;0000 UTC">2021-08-18 16:04:50 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            I think the gray blob refers to engineered hash collisions, like the example in the article link.
        </div>
        <div class="children">
            
        </div>
    </div>


            
        </div>
    </div>


            
        </div>
    </div>


            
        </div>
    </div>


            
        </div>
    </div>


            
        </div>
    </div>


            
        </div>
    </div>


            
                


    
    <div class="comment hasChildren">
        <p><a href="https://news.ycombinator.com/user?id=csande17" target="_blank">csande17</a>   <span class="timeago" data-date="2021-08-18 10:18:51 &#43;0000 UTC">2021-08-18 10:18:51 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            &gt; It also requires getting a hold of actual blacklisted hashes, which I doubt anyone has, unless they have <i>actual</i> child pornography.<p>I&#x27;ve never personally seen or looked for any images like this, but if they weren&#x27;t already proliferating online and widely available to criminals, why would we need to build an elaborate client-side scanning system to detect and report people who have copies of them?
        </div>
        <div class="children">
            
                


    
    <div class="comment hasChildren">
        <p><a href="https://news.ycombinator.com/user?id=user-the-name" target="_blank">user-the-name</a>   <span class="timeago" data-date="2021-08-18 10:22:44 &#43;0000 UTC">2021-08-18 10:22:44 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            Do you have access to darknet child pornography trading networks? I don&#x27;t.
        </div>
        <div class="children">
            
                


    
    <div class="comment hasChildren">
        <p><a href="https://news.ycombinator.com/user?id=tzs" target="_blank">tzs</a>   <span class="timeago" data-date="2021-08-18 13:04:01 &#43;0000 UTC">2021-08-18 13:04:01 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            OK, now I&#x27;m curious. How does darknet stuff actually work?<p>Is there some darknet search engine that you access over Tor and type whatever illegal thing you seek such as &quot;child porn&quot; or &quot;stolen credit cards&quot; or &quot;heroin&quot; or &quot;money laundering&quot; into and it points you to providers of those illegal goods and services reachable over some reasonably secure channel?<p>Or is it one of those things where someone already using a particular child porn or stolen card or money laundering or heroin selling site has to put you in contact with them?
        </div>
        <div class="children">
            
                


    
    <div class="comment ">
        <p><a href="https://news.ycombinator.com/user?id=quenix" target="_blank">quenix</a>   <span class="timeago" data-date="2021-08-18 13:36:21 &#43;0000 UTC">2021-08-18 13:36:21 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            Hmm. Tor onion addresses are generally super long and have a large random element to them, such as<p>facebookwkhpilnemxj7asaniu7vnjj.onion<p>where the &quot;Facebook&quot; part is there for convenience and may likely not be present for actual illegal addresses. As such, you probably won&#x27;t be able to find a CP website unless you actively brute force the entire onion address space.<p>I don&#x27;t know how the sharing actually happens, but I don&#x27;t think a real search engine for the dark net exists. There are websites that try (Ahlia, I think?) but they are super limited in scope.<p>I suspect addresses for drugs and CP are shared person-to-person.
        </div>
        <div class="children">
            
        </div>
    </div>


            
                


    
    <div class="comment ">
        <p><a href="https://news.ycombinator.com/user?id=colinmhayes" target="_blank">colinmhayes</a>   <span class="timeago" data-date="2021-08-18 13:40:54 &#43;0000 UTC">2021-08-18 13:40:54 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            dark.fail indexes darknet markets, but all the ones I&#x27;ve seen ban CSAM
        </div>
        <div class="children">
            
        </div>
    </div>


            
        </div>
    </div>


            
                


    
    <div class="comment ">
        <p><a href="https://news.ycombinator.com/user?id=" target="_blank"></a>   <span class="timeago" data-date="2021-08-18 10:26:21 &#43;0000 UTC">2021-08-18 10:26:21 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            
        </div>
        <div class="children">
            
        </div>
    </div>


            
        </div>
    </div>


            
        </div>
    </div>


            
                


    
    <div class="comment ">
        <p><a href="https://news.ycombinator.com/user?id=nabakin" target="_blank">nabakin</a>   <span class="timeago" data-date="2021-08-18 10:21:17 &#43;0000 UTC">2021-08-18 10:21:17 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            Agreed. This attack still has consequences for people but they will not run into any legal trouble as far as I&#x27;m aware.
        </div>
        <div class="children">
            
        </div>
    </div>


            
                


    
    <div class="comment ">
        <p><a href="https://news.ycombinator.com/user?id=atrus" target="_blank">atrus</a>   <span class="timeago" data-date="2021-08-18 10:17:44 &#43;0000 UTC">2021-08-18 10:17:44 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            I doubt someone who is able&#x2F;wanting to attack such a system would be unwilling to own some of the material themselves.
        </div>
        <div class="children">
            
        </div>
    </div>


            
                


    
    <div class="comment hasChildren">
        <p><a href="https://news.ycombinator.com/user?id=hughrr" target="_blank">hughrr</a>   <span class="timeago" data-date="2021-08-18 10:18:21 &#43;0000 UTC">2021-08-18 10:18:21 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            That assumes that the human review process is competent, your own images aren’t poisoned in some way (consider your own kids in the bath with some noise added) etc.<p>In the mean time they lock your account which means your entire digital life stops dead until their review process is done.<p>No way do I accept any of this.<p>Also the hashes are on the device I understand and it’s not going to be that difficult to extract them.
        </div>
        <div class="children">
            
                


    
    <div class="comment hasChildren">
        <p><a href="https://news.ycombinator.com/user?id=FabHK" target="_blank">FabHK</a>   <span class="timeago" data-date="2021-08-18 10:27:45 &#43;0000 UTC">2021-08-18 10:27:45 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            You:<p>&gt; the hashes are on the device I understand and it’s not going to be that difficult to extract them.<p>----<p><i>A Review of the Cryptography Behind the Apple PSI System</i>, Benny Pinkas, Dept. of Computer Science, Bar-Ilan University:<p>&gt; Do users learn the CSAM database? No user receives any CSAM photo, not even in encrypted form. Users receive a data structure of blinded fingerprints of photos in the CSAM database. Users cannot recover these fingerprints and therefore cannot use them to identify which photos are in the CSAM database.<p><a href="https:&#x2F;&#x2F;www.apple.com&#x2F;child-safety&#x2F;pdf&#x2F;Technical_Assessment_of_CSAM_Detection_Benny_Pinkas.pdf" rel="nofollow">https:&#x2F;&#x2F;www.apple.com&#x2F;child-safety&#x2F;pdf&#x2F;Technical_Assessment_...</a><p>----<p><i>The Apple PSI Protocol</i>, Mihir Bellare, Department of Computer Science and Engineering University of California, San Diego:<p>&gt; users do not learn the contents of the CSAM database.<p><a href="https:&#x2F;&#x2F;www.apple.com&#x2F;child-safety&#x2F;pdf&#x2F;Technical_Assessment_of_CSAM_Detection_Mihir_Bellare.pdf" rel="nofollow">https:&#x2F;&#x2F;www.apple.com&#x2F;child-safety&#x2F;pdf&#x2F;Technical_Assessment_...</a><p>----<p><i>A Concrete-Security Analysis of the Apple PSI Protocol</i>, Mihir Bellare, Department of Computer Science and Engineering University of California, San Diego:<p>&gt; the database of CSAM photos should not be made public or become known to the user.
Apple has found a way to detect and report CSAM offenders while respecting these privacy constraints.<p><a href="https:&#x2F;&#x2F;www.apple.com&#x2F;child-safety&#x2F;pdf&#x2F;Alternative_Security_Proof_of_Apple_PSI_System_Mihir_Bellare.pdf" rel="nofollow">https:&#x2F;&#x2F;www.apple.com&#x2F;child-safety&#x2F;pdf&#x2F;Alternative_Security_...</a>
        </div>
        <div class="children">
            
                


    
    <div class="comment ">
        <p><a href="https://news.ycombinator.com/user?id=hughrr" target="_blank">hughrr</a>   <span class="timeago" data-date="2021-08-18 10:42:04 &#43;0000 UTC">2021-08-18 10:42:04 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            So fundamentally there’s a lot of mechanism around keeping secrets but the source material is available. Hmm.<p><a href="https:&#x2F;&#x2F;youtu.be&#x2F;eU2Or5rCN_Y" rel="nofollow">https:&#x2F;&#x2F;youtu.be&#x2F;eU2Or5rCN_Y</a>
        </div>
        <div class="children">
            
        </div>
    </div>


            
        </div>
    </div>


            
                


    
    <div class="comment ">
        <p><a href="https://news.ycombinator.com/user?id=nyuszika7h" target="_blank">nyuszika7h</a>   <span class="timeago" data-date="2021-08-18 10:46:49 &#43;0000 UTC">2021-08-18 10:46:49 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            &gt; In the mean time they lock your account which means your entire digital life stops dead until their review process is done.<p>Do you have a source for this, or are you just making things up?
        </div>
        <div class="children">
            
        </div>
    </div>


            
                


    
    <div class="comment hasChildren">
        <p><a href="https://news.ycombinator.com/user?id=user-the-name" target="_blank">user-the-name</a>   <span class="timeago" data-date="2021-08-18 10:21:57 &#43;0000 UTC">2021-08-18 10:21:57 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            The human reviewer would be able to check against the exact image that generated the hash in the first place. Taking another completely unrelated image and perturbing it would be immediately obvious.
        </div>
        <div class="children">
            
                


    
    <div class="comment hasChildren">
        <p><a href="https://news.ycombinator.com/user?id=hughrr" target="_blank">hughrr</a>   <span class="timeago" data-date="2021-08-18 10:22:52 &#43;0000 UTC">2021-08-18 10:22:52 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            So there’s an office somewhere with computers full of illegal child porn that people are staring at and comparing your photos to?<p>There’s some irony in that.
        </div>
        <div class="children">
            
                


    
    <div class="comment hasChildren">
        <p><a href="https://news.ycombinator.com/user?id=henrikeh" target="_blank">henrikeh</a>   <span class="timeago" data-date="2021-08-18 10:30:00 &#43;0000 UTC">2021-08-18 10:30:00 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            Yes. That is called NCMEC in the US and it is a core aspect of how this whole process works.<p>If you don’t understand the details of this, I’ll recommend this podcast episode which sums it up and discusses the implications  <a href="https:&#x2F;&#x2F;atp.fm&#x2F;443" rel="nofollow">https:&#x2F;&#x2F;atp.fm&#x2F;443</a>
        </div>
        <div class="children">
            
                


    
    <div class="comment hasChildren">
        <p><a href="https://news.ycombinator.com/user?id=hughrr" target="_blank">hughrr</a>   <span class="timeago" data-date="2021-08-18 10:39:11 &#43;0000 UTC">2021-08-18 10:39:11 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            I am aware of the details. The episode of Brass Eye comes into context here, the relevant clip being as follows, showing exactly the issues of competence.<p><a href="https:&#x2F;&#x2F;youtu.be&#x2F;_U-7L1tmBAo" rel="nofollow">https:&#x2F;&#x2F;youtu.be&#x2F;_U-7L1tmBAo</a>
        </div>
        <div class="children">
            
                


    
    <div class="comment ">
        <p><a href="https://news.ycombinator.com/user?id=et2o" target="_blank">et2o</a>   <span class="timeago" data-date="2021-08-18 11:16:49 &#43;0000 UTC">2021-08-18 11:16:49 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            I don’t have much of an opinion here except that it is silly to write “I am aware of the details” when someone gives you a helpful explanation and you are obviously not aware of the details, as you had just asked a question betraying.
        </div>
        <div class="children">
            
        </div>
    </div>


            
                


    
    <div class="comment ">
        <p><a href="https://news.ycombinator.com/user?id=henrikeh" target="_blank">henrikeh</a>   <span class="timeago" data-date="2021-08-18 10:49:29 &#43;0000 UTC">2021-08-18 10:49:29 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            I’m sorry, what is your point? I legitimately don’t understand what you mean.
        </div>
        <div class="children">
            
        </div>
    </div>


            
        </div>
    </div>


            
        </div>
    </div>


            
        </div>
    </div>


            
        </div>
    </div>


            
        </div>
    </div>


            
                


    
    <div class="comment hasChildren">
        <p><a href="https://news.ycombinator.com/user?id=shmatt" target="_blank">shmatt</a>   <span class="timeago" data-date="2021-08-18 13:47:23 &#43;0000 UTC">2021-08-18 13:47:23 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            the blacklisted hashes are openly given to thousands of companies by a foundation called the National Center for Missing &amp; Exploited Children, in both PhotoDNA and MD5 versions of the hashes.<p>Yes, I can&#x27;t give you an open link, but I imagine at least hundreds of thousands of people have access to download them. Some companies with better security than others<p>It only takes 1 guy to sell it to an NSO-type company, who would then use it to target attacks for $$$
        </div>
        <div class="children">
            
                


    
    <div class="comment ">
        <p><a href="https://news.ycombinator.com/user?id=user-the-name" target="_blank">user-the-name</a>   <span class="timeago" data-date="2021-08-18 18:05:00 &#43;0000 UTC">2021-08-18 18:05:00 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            Where would that $$$ come from? Like I said, the only attack possible is to overload the human reviewers, and even that is difficult to pull off except as a coordinated effort by lots of people.
        </div>
        <div class="children">
            
        </div>
    </div>


            
        </div>
    </div>


            
        </div>
    </div>


            
                


    
    <div class="comment ">
        <p><a href="https://news.ycombinator.com/user?id=Grustaf" target="_blank">Grustaf</a>   <span class="timeago" data-date="2021-08-18 13:23:01 &#43;0000 UTC">2021-08-18 13:23:01 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            Working against you?<p>This technology doesn&#x27;t make it even an ounce easier for Apple, or some evil three letter agency, to spy on you. If they want to see if you&#x27;re a Trump supporter, anti-vaxxer or whatever, they already have access to all your photos and emails, on device. They even have OCR and classification of your photos. An intern could add a spying function in an afternoon.<p>if OCR(photo[i]).containsString(&quot;MAGA&quot;) { 
    reportUser()
}
        </div>
        <div class="children">
            
        </div>
    </div>


            
                


    
    <div class="comment hasChildren">
        <p><a href="https://news.ycombinator.com/user?id=argvargc" target="_blank">argvargc</a>   <span class="timeago" data-date="2021-08-18 14:48:08 &#43;0000 UTC">2021-08-18 14:48:08 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            It didn&#x27;t even last a week.<p>Introducing the people asking us to trust them with the responsibility of mass, automated crime accusations.
        </div>
        <div class="children">
            
                


    
    <div class="comment hasChildren">
        <p><a href="https://news.ycombinator.com/user?id=hannasanarion" target="_blank">hannasanarion</a>   <span class="timeago" data-date="2021-08-18 19:21:55 &#43;0000 UTC">2021-08-18 19:21:55 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            Why do you think somebody will accuse you of a crime because you have a photo of a grey blob?<p>The real world isn&#x27;t as stupid as the computer one. The justice system is not deterministic and automatic. Nobody is going to look at this grey blob and go &quot;welp, we have no choice but to throw you in prison forever&quot;
        </div>
        <div class="children">
            
                


    
    <div class="comment hasChildren">
        <p><a href="https://news.ycombinator.com/user?id=argvargc" target="_blank">argvargc</a>   <span class="timeago" data-date="2021-08-18 20:24:23 &#43;0000 UTC">2021-08-18 20:24:23 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            The real world and the computer world intersect. This is precisely typified by what is being discussed, surely?<p>Apple is trying to automate and computerise a process that was not automated previously, apply it to a huge number of people, and with disastrous potential consequences should their wonderful design be found lacking.<p>And within days, they have already utterly failed to provide one of their own self-stated and incredibly obvious requirements. What else could possibly go wrong?<p>Further, if you think that because the first pre-image failure found (in days...) was a grey blob, that this means all future possible cultivated collisions will only ever be grey blobs, well - good luck with that.
        </div>
        <div class="children">
            
                


    
    <div class="comment hasChildren">
        <p><a href="https://news.ycombinator.com/user?id=hannasanarion" target="_blank">hannasanarion</a>   <span class="timeago" data-date="2021-08-18 20:47:20 &#43;0000 UTC">2021-08-18 20:47:20 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            The process of making accusations and arrests is not automatic.<p>The Apple system sends flagged photos to reviewers, and if the reviewers find them suspicious, they send them to the poor souls at NCMEC, who will compare the flagged photo with the original illegal photo that&#x27;s supposedly a match, and  inform law enforcement if they are in fact a match.<p>Nobody will get cops at their door because somebody sent them a grey blob image.<p>The process that&#x27;s being &quot;automated&quot; is merely an automatic flag that initiates a several-step process of human review. Apple isn&#x27;t rolling out robocops.<p>Seriously, what are the &quot;disasterous consequences&quot; that you envision? What is the sequence of events where a hash collision leads to any inconvenience whatsoever for a user?
        </div>
        <div class="children">
            
                


    
    <div class="comment hasChildren">
        <p><a href="https://news.ycombinator.com/user?id=argvargc" target="_blank">argvargc</a>   <span class="timeago" data-date="2021-08-19 08:06:01 &#43;0000 UTC">2021-08-19 08:06:01 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            What part of &quot;they already totally fucked up how their own process is supposed to work&quot; don&#x27;t you understand?
        </div>
        <div class="children">
            
                


    
    <div class="comment ">
        <p><a href="https://news.ycombinator.com/user?id=hannasanarion" target="_blank">hannasanarion</a>   <span class="timeago" data-date="2021-08-19 15:47:59 &#43;0000 UTC">2021-08-19 15:47:59 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            What part of it is fucked up? They never promised that their hashing algorithm was uncollidable. The process is specifically designed to be tolerant of hash collisions (and in fact wouldn&#x27;t work otherwise, because the system needs to ignore small differences between copies of the illegal images, like one-pixel edits or color temperature differences or photocopies).<p>There are multiple stages of human review in the process and a high threshold for activation of the proccess <i>because</i> collisions are inevitable. The fact that collisions exist doesn&#x27;t impact the safety of the product whatsoever.<p>I ask again, what is the sequence of events where a hash collision from a benign image can lead to any consequence at all for the user?
        </div>
        <div class="children">
            
        </div>
    </div>


            
        </div>
    </div>


            
        </div>
    </div>


            
        </div>
    </div>


            
        </div>
    </div>


            
        </div>
    </div>


            
        </div>
    </div>


        
            


    
    <div class="comment ">
        <p><a href="https://news.ycombinator.com/user?id=smlss_sftwr" target="_blank">smlss_sftwr</a>   <span class="timeago" data-date="2021-08-18 16:50:51 &#43;0000 UTC">2021-08-18 16:50:51 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            &quot;it&#x27;s ok, there&#x27;s only a 1 in a trillion chance of this happening&quot;
        </div>
        <div class="children">
            
        </div>
    </div>


        
            


    
    <div class="comment ">
        <p><a href="https://news.ycombinator.com/user?id=th0ma5" target="_blank">th0ma5</a>   <span class="timeago" data-date="2021-08-18 17:05:40 &#43;0000 UTC">2021-08-18 17:05:40 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            Just visiting this page would trigger a bit right?
        </div>
        <div class="children">
            
        </div>
    </div>


        
            


    
    <div class="comment hasChildren">
        <p><a href="https://news.ycombinator.com/user?id=spoonjim" target="_blank">spoonjim</a>   <span class="timeago" data-date="2021-08-18 20:24:17 &#43;0000 UTC">2021-08-18 20:24:17 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            Any idea why Apple had to be too cute by half and not just scan these files server-side? Or why it doesn&#x27;t change their plan to do that, given the backlash?
        </div>
        <div class="children">
            
                


    
    <div class="comment hasChildren">
        <p><a href="https://news.ycombinator.com/user?id=cyanite" target="_blank">cyanite</a>   <span class="timeago" data-date="2021-08-18 20:32:51 &#43;0000 UTC">2021-08-18 20:32:51 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            I hope not, since doing the scans like this offers much more privacy for the user. Even when the user is too ignorant (sorry) to realize this.<p>But this will be evident by reading and understanding the technical description.
        </div>
        <div class="children">
            
                


    
    <div class="comment ">
        <p><a href="https://news.ycombinator.com/user?id=spoonjim" target="_blank">spoonjim</a>   <span class="timeago" data-date="2021-08-18 21:31:10 &#43;0000 UTC">2021-08-18 21:31:10 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            I&#x27;m not concerned with <i>this system</i> -- I&#x27;m concerned with the things that governments will now start to force Apple (and any computer vendor) to start scanning for on the client side. Once the client side scanning Rubicon is crossed, countries will want vendors to scan everything (not just iCloud folders) for everything (not just CSAM).
        </div>
        <div class="children">
            
        </div>
    </div>


            
        </div>
    </div>


            
        </div>
    </div>


        
            


    
    <div class="comment hasChildren">
        <p><a href="https://news.ycombinator.com/user?id=shadowgovt" target="_blank">shadowgovt</a>   <span class="timeago" data-date="2021-08-18 11:36:39 &#43;0000 UTC">2021-08-18 11:36:39 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            Does this algorithm work for the reverse goal (i.e. can content that would trip the CSAM hash be perturbed enough to avoid it without compromising quality of the underlying image)?<p>To my mind, that&#x27;s far more disquieting than the risk of someone staging and elaborate attack on an enemy&#x27;s device.
        </div>
        <div class="children">
            
                


    
    <div class="comment ">
        <p><a href="https://news.ycombinator.com/user?id=zug_zug" target="_blank">zug_zug</a>   <span class="timeago" data-date="2021-08-18 13:25:56 &#43;0000 UTC">2021-08-18 13:25:56 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            Why is this downvoted? This checks out -- A problem with local scanning is that the source code will always be available and thus the user can learn exactly which perturbations trick the system.
        </div>
        <div class="children">
            
        </div>
    </div>


            
        </div>
    </div>


        
            


    
    <div class="comment ">
        <p><a href="https://news.ycombinator.com/user?id=" target="_blank"></a>   <span class="timeago" data-date="2021-08-18 11:03:47 &#43;0000 UTC">2021-08-18 11:03:47 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            
        </div>
        <div class="children">
            
        </div>
    </div>


        
            


    
    <div class="comment ">
        <p><a href="https://news.ycombinator.com/user?id=" target="_blank"></a>   <span class="timeago" data-date="2021-08-18 11:28:06 &#43;0000 UTC">2021-08-18 11:28:06 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            
        </div>
        <div class="children">
            
        </div>
    </div>


        
            


    
    <div class="comment hasChildren">
        <p><a href="https://news.ycombinator.com/user?id=ptidhomme" target="_blank">ptidhomme</a>   <span class="timeago" data-date="2021-08-18 09:56:28 &#43;0000 UTC">2021-08-18 09:56:28 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            Yes, just like rape accusations. It doesn&#x27;t matter that you prove it was false afterwards.<p>Edit : well that was a hint to Assange of course. Probably not true in general. So yes, I mean <i>false</i> accusations.
        </div>
        <div class="children">
            
                


    
    <div class="comment ">
        <p><a href="https://news.ycombinator.com/user?id=dang" target="_blank">dang</a>   <span class="timeago" data-date="2021-08-18 18:45:00 &#43;0000 UTC">2021-08-18 18:45:00 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            We detached this subthread from <a href="https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=28219243" rel="nofollow">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=28219243</a>.
        </div>
        <div class="children">
            
        </div>
    </div>


            
                


    
    <div class="comment hasChildren">
        <p><a href="https://news.ycombinator.com/user?id=reayn" target="_blank">reayn</a>   <span class="timeago" data-date="2021-08-18 10:35:37 &#43;0000 UTC">2021-08-18 10:35:37 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            Why is this getting downvoted? It’s very true, just the accusation of committing such a crime (regardless of whether   the person was acquitted or not) can easily ruin many facets of a persons’ life.
        </div>
        <div class="children">
            
                


    
    <div class="comment hasChildren">
        <p><a href="https://news.ycombinator.com/user?id=hosteur" target="_blank">hosteur</a>   <span class="timeago" data-date="2021-08-18 11:34:38 &#43;0000 UTC">2021-08-18 11:34:38 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            Julian Assange and Jake Appelbaum being prime examples of this.
        </div>
        <div class="children">
            
                


    
    <div class="comment hasChildren">
        <p><a href="https://news.ycombinator.com/user?id=d33" target="_blank">d33</a>   <span class="timeago" data-date="2021-08-18 11:49:00 &#43;0000 UTC">2021-08-18 11:49:00 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            In case of Appelbaum, are there any solid reasons to believe that the accusations are untrue? For Assange, I think that the victim admitted that the accusation was fabricated, isn&#x27;t that the case?
        </div>
        <div class="children">
            
                


    
    <div class="comment hasChildren">
        <p><a href="https://news.ycombinator.com/user?id=coldtea" target="_blank">coldtea</a>   <span class="timeago" data-date="2021-08-18 12:17:26 &#43;0000 UTC">2021-08-18 12:17:26 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            Accusations must be proven true, not untrue by the accused.<p>And besides the &quot;victim&quot; in the latter case there was a whole lot of diplomatic pressure and political commotion to set him up, with carrots and sticks and the aid of friendly satellite states.
        </div>
        <div class="children">
            
                


    
    <div class="comment hasChildren">
        <p><a href="https://news.ycombinator.com/user?id=_jal" target="_blank">_jal</a>   <span class="timeago" data-date="2021-08-18 12:36:33 &#43;0000 UTC">2021-08-18 12:36:33 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            They must be proven true beyond a reasonable doubt to get a person in a funny robe to put them in jail.<p>Normal humans are not required to prove anything in order to think them.
        </div>
        <div class="children">
            
                


    
    <div class="comment hasChildren">
        <p><a href="https://news.ycombinator.com/user?id=coldtea" target="_blank">coldtea</a>   <span class="timeago" data-date="2021-08-18 12:45:21 &#43;0000 UTC">2021-08-18 12:45:21 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            &gt;<i>Normal humans are not required to prove anything in order to think them.</i><p>No, but decent human beings are required to not think them true just because they&#x27;re out there...
        </div>
        <div class="children">
            
                


    
    <div class="comment ">
        <p><a href="https://news.ycombinator.com/user?id=whimsicalism" target="_blank">whimsicalism</a>   <span class="timeago" data-date="2021-08-18 13:29:39 &#43;0000 UTC">2021-08-18 13:29:39 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            decent humans can have a lower standard for thinking someone did a crime than the criminal justice system does.<p>this is true whether you believe or disbelieve a rape claim, because if you disbelieve it you typically must believe the corrolary that the victim was committing the crime of lying to the police.
        </div>
        <div class="children">
            
        </div>
    </div>


            
        </div>
    </div>


            
                


    
    <div class="comment ">
        <p><a href="https://news.ycombinator.com/user?id=JohnWhigham" target="_blank">JohnWhigham</a>   <span class="timeago" data-date="2021-08-18 14:34:24 &#43;0000 UTC">2021-08-18 14:34:24 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            Which is why false rape allegations are so fucking dangerous, and are not nearly as punished as harshly as they should be.
        </div>
        <div class="children">
            
        </div>
    </div>


            
                


    
    <div class="comment hasChildren">
        <p><a href="https://news.ycombinator.com/user?id=Griffinsauce" target="_blank">Griffinsauce</a>   <span class="timeago" data-date="2021-08-18 15:24:23 &#43;0000 UTC">2021-08-18 15:24:23 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            *in the US
        </div>
        <div class="children">
            
                


    
    <div class="comment ">
        <p><a href="https://news.ycombinator.com/user?id=coldtea" target="_blank">coldtea</a>   <span class="timeago" data-date="2021-08-18 15:40:20 &#43;0000 UTC">2021-08-18 15:40:20 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            I&#x27;m not in the US.<p>And even if it didn&#x27;t hold in my country, it&#x27;s still a tenet I think is fair.<p>So, I didn&#x27;t intended invoke what&#x27;s the legal requirements, but what should be the &quot;right&quot; thing (and what I think should also be legally mandated, even if it&#x27;s not).
        </div>
        <div class="children">
            
        </div>
    </div>


            
        </div>
    </div>


            
        </div>
    </div>


            
                


    
    <div class="comment ">
        <p><a href="https://news.ycombinator.com/user?id=xpe" target="_blank">xpe</a>   <span class="timeago" data-date="2021-08-18 14:28:38 &#43;0000 UTC">2021-08-18 14:28:38 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            &gt; Accusations must be proven true, not untrue by the accused.<p>I respect this ethical claim; however, there is considerable variation in how legal systems operate in this domain.
        </div>
        <div class="children">
            
        </div>
    </div>


            
        </div>
    </div>


            
                


    
    <div class="comment ">
        <p><a href="https://news.ycombinator.com/user?id=enriquto" target="_blank">enriquto</a>   <span class="timeago" data-date="2021-08-18 12:12:52 &#43;0000 UTC">2021-08-18 12:12:52 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            &gt; solid reasons to believe that the accusations are untrue<p>this is not how the concept of accusation works
        </div>
        <div class="children">
            
        </div>
    </div>


            
        </div>
    </div>


            
        </div>
    </div>


            
        </div>
    </div>


            
                


    
    <div class="comment hasChildren">
        <p><a href="https://news.ycombinator.com/user?id=brigandish" target="_blank">brigandish</a>   <span class="timeago" data-date="2021-08-18 10:38:12 &#43;0000 UTC">2021-08-18 10:38:12 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            I think you may have attracted less downvotes if the phrasing was changed to &quot;Yes, just like <i>false accusations of rape</i>, it doesn&#x27;t matter that you prove it was false afterwards.&quot;<p>I also think that those downvoting you might&#x27;ve applied the principle of charity and taken the best interpretation of what you&#x27;ve written or at least <i>ask</i>.
        </div>
        <div class="children">
            
                


    
    <div class="comment hasChildren">
        <p><a href="https://news.ycombinator.com/user?id=sneak" target="_blank">sneak</a>   <span class="timeago" data-date="2021-08-18 10:39:22 &#43;0000 UTC">2021-08-18 10:39:22 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            All criminal accusations, <i>including true ones</i>, should be treated as false until the accused is <i>proven</i> guilty.<p>This is a fundamental tenet of human rights in western, small-l liberal free societies.<p>The fact that this is controversial these days is literally insane to me.<p>The consequences of throwing this fundamental system out the window is that you get the sort of nonsense that happened with Assange, where he was literally never even charged yet completely and thoroughly discredited due to headlines containing the word &quot;rape&quot; when no such thing ever happened.<p>(If you have been misled to believe otherwise, I encourage you to read the direct statements of the women involved.)
        </div>
        <div class="children">
            
                


    
    <div class="comment ">
        <p><a href="https://news.ycombinator.com/user?id=DebtDeflation" target="_blank">DebtDeflation</a>   <span class="timeago" data-date="2021-08-18 13:09:04 &#43;0000 UTC">2021-08-18 13:09:04 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            The real problem occurs long before a verdict is rendered in a court of law.  Someone gets arrested for a misdemeanor, they spend a couple of nights in jail until bail can be arranged, in the meantime they got fired for missing work, the car they were driving was towed&#x2F;impounded and will cost them hundreds of dollars to retrieve, they missed their rent payment and their landlord has begun eviction proceedings, etc.  The fact that they are found not guilty when their trial happens a year later is irrelevant.  Not to mention every future potential employer Googles their name and the first link that comes up is their arrest record.  The &quot;presumption of innocence&quot; is meaningless when so much damage is done long before a trial even starts.
        </div>
        <div class="children">
            
        </div>
    </div>


            
                


    
    <div class="comment hasChildren">
        <p><a href="https://news.ycombinator.com/user?id=Ensorceled" target="_blank">Ensorceled</a>   <span class="timeago" data-date="2021-08-18 12:10:30 &#43;0000 UTC">2021-08-18 12:10:30 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            &gt; All criminal accusations, including true ones, should be treated as false until the accused is proven guilty.<p>No, they need to be treated as unproven,  a very critical difference.<p>Just to be clear,  witness testimony, including testimony FROM THE VICTIM, is evidence of the crime.  Just for some reason, in rape cases, we go all wonky with this principle.
        </div>
        <div class="children">
            
                


    
    <div class="comment hasChildren">
        <p><a href="https://news.ycombinator.com/user?id=technothrasher" target="_blank">technothrasher</a>   <span class="timeago" data-date="2021-08-18 12:24:31 &#43;0000 UTC">2021-08-18 12:24:31 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            &gt; No, they need to be treated as unproven, a very critical difference.<p>Right. I have a jar of gumballs and tell you I think there is an even number of gumballs in it.  Do you believe me?  If you don&#x27;t, does that mean you believe there are in fact an odd number of gumballs in it?  No, you do not believe either that there are an odd or an even number, because you just don&#x27;t know.  For a criminal accusation , your initial belief should not be guilty or innocent, it should be, &quot;I just don&#x27;t know&quot;.
        </div>
        <div class="children">
            
                


    
    <div class="comment ">
        <p><a href="https://news.ycombinator.com/user?id=Ensorceled" target="_blank">Ensorceled</a>   <span class="timeago" data-date="2021-08-18 12:44:59 &#43;0000 UTC">2021-08-18 12:44:59 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            Yeah,  &quot;it is unverified or untested or unproven&quot; seems to be a very hard concept for &quot;black and white&quot; brains to process.  People keep following up &quot;it is unproven&quot; with &quot;well then it must be false&quot;.<p>EDIT: judging by the downvotes, they are also making the leap to the original gumball counter must be lying ...
        </div>
        <div class="children">
            
        </div>
    </div>


            
        </div>
    </div>


            
                


    
    <div class="comment hasChildren">
        <p><a href="https://news.ycombinator.com/user?id=ralfn" target="_blank">ralfn</a>   <span class="timeago" data-date="2021-08-18 12:28:06 &#43;0000 UTC">2021-08-18 12:28:06 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            1. By the public at large it should not be treated in any regard, false or not.<p>2. The state is the only authorized monopoly of violence and they should treat unproven and untrue as identical, and the only place where that decision is made is in a courtroom.<p>3. The &#x27;believe the victims&#x27; activists however are rightfully (IMHO) suggesting to break principle #2 because there is institutional and systemic supression of the rule of law being applied properly. I would consider this civil disobedience.<p>4. The state needs to get it&#x27;s act together and administer the law properly. Only through reform can they regain the legitimacy that is required for #2.<p>5. Those reforms should focus on racism, sexism and classism. It should focus on what part of the law is too ambiguous for either law enforcement and the judiciary branch.<p>6. This might include actually blinding the court, i.e. offer only verifiable facts that are admissable and can not be utilized as widgets for race, gender or social economic positions.<p>If you don&#x27;t think the legal system has a problem ask yourself why every lawyer recommends the defendant to wear a suit to court. How could it matter if the process was assumed to be without bias.
        </div>
        <div class="children">
            
                


    
    <div class="comment hasChildren">
        <p><a href="https://news.ycombinator.com/user?id=bscphil" target="_blank">bscphil</a>   <span class="timeago" data-date="2021-08-18 13:40:29 &#43;0000 UTC">2021-08-18 13:40:29 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            &gt; The state is the only authorized monopoly of violence and they should treat unproven and untrue as identical, and the only place where that decision is made is in a courtroom.<p>This is completely wrong. If the state treated all accusations as untrue prior to conviction, they would not send armed men to haul you to prison, bar you from release unless you can bail yourself out (or sometimes not at all), and not have a prosecutor charge you with a crime.<p>This is no petty distinction. In order to function in its judiciary role, the state in fact <i>must</i> distinguish between plausibly true accusations and not plausible accusations, and <i>must</i> treat certain plausibly true accusations as &quot;unproven&quot; but <i>not</i> untrue, and take steps to make sure the accused does not flee into another country or commit further crimes.
        </div>
        <div class="children">
            
                


    
    <div class="comment ">
        <p><a href="https://news.ycombinator.com/user?id=ralfn" target="_blank">ralfn</a>   <span class="timeago" data-date="2021-08-18 18:59:15 &#43;0000 UTC">2021-08-18 18:59:15 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            You are right. There is a lot more too it.<p>Although arresting and jailing someone is not being justice being administered, it is facilitating justice and fact finding and it itself is indeed an act of violence.
        </div>
        <div class="children">
            
        </div>
    </div>


            
        </div>
    </div>


            
                


    
    <div class="comment ">
        <p><a href="https://news.ycombinator.com/user?id=brigandish" target="_blank">brigandish</a>   <span class="timeago" data-date="2021-08-19 03:09:59 &#43;0000 UTC">2021-08-19 03:09:59 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            The legal system may have problems but that is about <i>implementation</i>, the fundamental principles of presumption of innocence, that no harm should come to the innocent, that all are equal before the law, and that truth comes before all other concerns; these are fine principles and I would say that problems with the legal system mostly stem from straying from these principles. Perhaps you disagree?
        </div>
        <div class="children">
            
        </div>
    </div>


            
                


    
    <div class="comment hasChildren">
        <p><a href="https://news.ycombinator.com/user?id=whimsicalism" target="_blank">whimsicalism</a>   <span class="timeago" data-date="2021-08-18 13:32:26 &#43;0000 UTC">2021-08-18 13:32:26 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            Sorry, don&#x27;t see how 3 in your &quot;syllogism&quot; is remotely true. If 10 employees of yours come to you and accuse another employee of sexual harassment and you fire that employee, at no point were you acting as the state or using violence.
        </div>
        <div class="children">
            
                





            
        </div>
    </div>


            
        </div>
    </div>


            
                


    
    <div class="comment hasChildren">
        <p><a href="https://news.ycombinator.com/user?id=Sebb767" target="_blank">Sebb767</a>   <span class="timeago" data-date="2021-08-18 12:18:47 &#43;0000 UTC">2021-08-18 12:18:47 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            Testimony is seen as wonky in all kinds of cases. The problem with testimony about rape, however, is that, in those cases, supporting evidence is rarer than usual and even when it exists, proving it was non-consensual at the time is even harder (especially when relationships or affairs are involved).
        </div>
        <div class="children">
            
                


    
    <div class="comment hasChildren">
        <p><a href="https://news.ycombinator.com/user?id=Ensorceled" target="_blank">Ensorceled</a>   <span class="timeago" data-date="2021-08-18 12:20:53 &#43;0000 UTC">2021-08-18 12:20:53 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            Yes,  rape is more difficult to prove than a number of other crimes.  That is no reason to jump to a default &quot;assume the accusation is false&quot;.
        </div>
        <div class="children">
            
                


    
    <div class="comment hasChildren">
        <p><a href="https://news.ycombinator.com/user?id=ziml77" target="_blank">ziml77</a>   <span class="timeago" data-date="2021-08-18 12:44:43 &#43;0000 UTC">2021-08-18 12:44:43 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            The reason to default to that is because a principle of our legal system is that people are innocent until proven guilty. On top of that proving someone guilty requires going beyond reasonable doubt.
        </div>
        <div class="children">
            
                


    
    <div class="comment hasChildren">
        <p><a href="https://news.ycombinator.com/user?id=Ensorceled" target="_blank">Ensorceled</a>   <span class="timeago" data-date="2021-08-18 12:50:22 &#43;0000 UTC">2021-08-18 12:50:22 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            The principle is the defendant is innocent until proven guilty,  NOT that the accuser is making a false accusation.<p>There is a reason the verdict is &quot;not guilty&quot; instead of &quot;innocent&quot;.   After a not guilty verdict the legal system still does not assume the accuser was making a false accusation.
        </div>
        <div class="children">
            
                


    
    <div class="comment hasChildren">
        <p><a href="https://news.ycombinator.com/user?id=Sebb767" target="_blank">Sebb767</a>   <span class="timeago" data-date="2021-08-18 12:56:03 &#43;0000 UTC">2021-08-18 12:56:03 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            &gt; There is a reason the verdict is &quot;not guilty&quot; instead of &quot;innocent&quot;.<p>Both verdicts exists, actually; the latter one is just far rarer (since it is both harder to prove and usually not what is argued about).
        </div>
        <div class="children">
            
                


    
    <div class="comment ">
        <p><a href="https://news.ycombinator.com/user?id=Ensorceled" target="_blank">Ensorceled</a>   <span class="timeago" data-date="2021-08-18 16:38:18 &#43;0000 UTC">2021-08-18 16:38:18 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            Yes,  and almost always a either a case of mistaken identity or emerging malfeasance by either witnesses or prosecution.<p>Ironically,  a provable case of false rape accusation might result in this verdict.
        </div>
        <div class="children">
            
        </div>
    </div>


            
        </div>
    </div>


            
        </div>
    </div>


            
        </div>
    </div>


            
                


    
    <div class="comment ">
        <p><a href="https://news.ycombinator.com/user?id=Sebb767" target="_blank">Sebb767</a>   <span class="timeago" data-date="2021-08-18 12:54:54 &#43;0000 UTC">2021-08-18 12:54:54 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            &gt; That is no reason to jump to a default &quot;assume the accusation is false&quot;.<p>That&#x27;s also not what I was trying to say. I only explained why testimony is far more doubted in rape cases than in most other cases. In general, I agree with you that accusations should be treated as unproven, so neither false nor true; we have a justice system to give a final verdict.
        </div>
        <div class="children">
            
        </div>
    </div>


            
                


    
    <div class="comment hasChildren">
        <p><a href="https://news.ycombinator.com/user?id=deelowe" target="_blank">deelowe</a>   <span class="timeago" data-date="2021-08-18 12:52:16 &#43;0000 UTC">2021-08-18 12:52:16 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            Depends. Are certain groups disproportionally more likely to benefit from said accusations?<p>Sorry, it needs to be said. It&#x27;s easier to take one side of an argument when it&#x27;s less likely to affect you personally.
        </div>
        <div class="children">
            
                


    
    <div class="comment ">
        <p><a href="https://news.ycombinator.com/user?id=whimsicalism" target="_blank">whimsicalism</a>   <span class="timeago" data-date="2021-08-18 13:34:11 &#43;0000 UTC">2021-08-18 13:34:11 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            Yes, generally powerful men disproportionately benefit from accusations that a rape claim is false.<p>No doubt some are false, but have not seen any evidence suggesting the majority are false.
        </div>
        <div class="children">
            
        </div>
    </div>


            
                


    
    <div class="comment hasChildren">
        <p><a href="https://news.ycombinator.com/user?id=Ensorceled" target="_blank">Ensorceled</a>   <span class="timeago" data-date="2021-08-18 12:54:46 &#43;0000 UTC">2021-08-18 12:54:46 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            What are you actually claiming here?  You stance is unclear.
        </div>
        <div class="children">
            
                


    
    <div class="comment hasChildren">
        <p><a href="https://news.ycombinator.com/user?id=deelowe" target="_blank">deelowe</a>   <span class="timeago" data-date="2021-08-18 14:54:42 &#43;0000 UTC">2021-08-18 14:54:42 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            That this is a contentious topic with biases on both sides. One group clearly benefits from any accusations being squashed before ever being investigated. Another benefits from never having to prove their accusations hold merit.<p>This is why due process is important.
        </div>
        <div class="children">
            
                


    
    <div class="comment ">
        <p><a href="https://news.ycombinator.com/user?id=Ensorceled" target="_blank">Ensorceled</a>   <span class="timeago" data-date="2021-08-18 16:39:44 &#43;0000 UTC">2021-08-18 16:39:44 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            I see this a lot in these discussions.  What do you think Trump or Cosby&#x27;s accusers gained by their accusations holding merit?
        </div>
        <div class="children">
            
        </div>
    </div>


            
        </div>
    </div>


            
        </div>
    </div>


            
        </div>
    </div>


            
        </div>
    </div>


            
        </div>
    </div>


            
                


    
    <div class="comment ">
        <p><a href="https://news.ycombinator.com/user?id=brigandish" target="_blank">brigandish</a>   <span class="timeago" data-date="2021-08-19 03:03:09 &#43;0000 UTC">2021-08-19 03:03:09 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            All criminal accusations need to be treated as unproven, of course, could not agree more. Also, all those accused of a crime (and those not!) need to be treated as innocent <i>until they are proven guilty</i>.
        </div>
        <div class="children">
            
        </div>
    </div>


            
                


    
    <div class="comment hasChildren">
        <p><a href="https://news.ycombinator.com/user?id=kergonath" target="_blank">kergonath</a>   <span class="timeago" data-date="2021-08-18 14:14:34 &#43;0000 UTC">2021-08-18 14:14:34 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            This is bonkers. We are innocent until proven guilty, not &quot;unproven&quot;.<p>Witness testimony on its own is circumstantial evidence in general. Witnesses are very unreliable.
        </div>
        <div class="children">
            
                


    
    <div class="comment ">
        <p><a href="https://news.ycombinator.com/user?id=Ensorceled" target="_blank">Ensorceled</a>   <span class="timeago" data-date="2021-08-18 18:56:26 &#43;0000 UTC">2021-08-18 18:56:26 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            We are innocent until proven guilty,  I never disputed that.<p>I&#x27;m saying the accusation, the witness testimony, is NOT assumed false, the accusation is simply unproven.<p>If the jury&#x2F;judge believes the testimony, they may convict on that testimony.   Then the accused is proven guilty.<p>That the accusation started off being false and then magically became true when when the jury believed it is the bonkers belief here.<p>The fact that witness testimony is unreliable is a big part of WHY the accused is presumed innocent.
        </div>
        <div class="children">
            
        </div>
    </div>


            
        </div>
    </div>


            
                


    
    <div class="comment ">
        <p><a href="https://news.ycombinator.com/user?id=weimerica" target="_blank">weimerica</a>   <span class="timeago" data-date="2021-08-18 12:26:28 &#43;0000 UTC">2021-08-18 12:26:28 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            &gt; Just to be clear, witness testimony, including testimony FROM THE VICTIM, is evidence of the crime. Just for some reason, in rape cases, we go all wonky with this principle.<p>People lie, people have memory issues. Complicating things is the lovely issue modernity has brought, of two adults getting intoxicated and fornicating followed by regret and rape accusations sometime later. Then we have weaponized accusations - just like we have keyboard warriors making false police reports to have their opposition receive a SWAT team visit, we have people that will make false accusations to get revenge after a perceived slight.<p>What is wonky to me is the people who hear an accusation and treat it like the Gospel, destroying the accused depriving them of any possible justice. This is what is truly, absolutely, criminally insane.
        </div>
        <div class="children">
            
        </div>
    </div>


            
                


    
    <div class="comment hasChildren">
        <p><a href="https://news.ycombinator.com/user?id=skhr0680" target="_blank">skhr0680</a>   <span class="timeago" data-date="2021-08-18 12:24:36 &#43;0000 UTC">2021-08-18 12:24:36 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            &gt; including testimony FROM THE VICTIM, is evidence of the crime. Just for some reason, in rape cases, we go all wonky with this principle.<p>In a criminal trial:<p>Victim: This person did it<p>Defendant: No I didn&#x27;t<p>Not guilty
        </div>
        <div class="children">
            
                


    
    <div class="comment hasChildren">
        <p><a href="https://news.ycombinator.com/user?id=Ensorceled" target="_blank">Ensorceled</a>   <span class="timeago" data-date="2021-08-18 12:47:21 &#43;0000 UTC">2021-08-18 12:47:21 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            That is not how it works, a large number of people are convicted on eye witness testimony from a single accuser.
        </div>
        <div class="children">
            
                


    
    <div class="comment ">
        <p><a href="https://news.ycombinator.com/user?id=elzbardico" target="_blank">elzbardico</a>   <span class="timeago" data-date="2021-08-18 12:55:11 &#43;0000 UTC">2021-08-18 12:55:11 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            Yes, they shouldn&#x27;t.
        </div>
        <div class="children">
            
        </div>
    </div>


            
        </div>
    </div>


            
        </div>
    </div>


            
        </div>
    </div>


            
                


    
    <div class="comment ">
        <p><a href="https://news.ycombinator.com/user?id=chitowneats" target="_blank">chitowneats</a>   <span class="timeago" data-date="2021-08-18 13:39:28 &#43;0000 UTC">2021-08-18 13:39:28 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            The West has largely abandoned &quot;small-l&quot; liberalism. Not yet formally in many cases but defacto in terms of government actions and the views of the people.
        </div>
        <div class="children">
            
        </div>
    </div>


            
                


    
    <div class="comment hasChildren">
        <p><a href="https://news.ycombinator.com/user?id=cma" target="_blank">cma</a>   <span class="timeago" data-date="2021-08-18 11:00:17 &#43;0000 UTC">2021-08-18 11:00:17 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            That&#x27;s the burden for incarceration, not for making a personal best guess about guilt.  Even far below best guess, would you send your kid with a camp councilor that you were 20% sure was guilty of something like that?
        </div>
        <div class="children">
            
                


    
    <div class="comment hasChildren">
        <p><a href="https://news.ycombinator.com/user?id=Sebb767" target="_blank">Sebb767</a>   <span class="timeago" data-date="2021-08-18 12:20:36 &#43;0000 UTC">2021-08-18 12:20:36 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            I probably would be doubtful even at 1%, sure. But there&#x27;s no denying that this ruins the life of 99 innocent people for every actual criminal.
        </div>
        <div class="children">
            
                


    
    <div class="comment ">
        <p><a href="https://news.ycombinator.com/user?id=sneak" target="_blank">sneak</a>   <span class="timeago" data-date="2021-08-18 18:26:16 &#43;0000 UTC">2021-08-18 18:26:16 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            <a href="https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Blackstone%27s_ratio" rel="nofollow">https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Blackstone%27s_ratio</a>
        </div>
        <div class="children">
            
        </div>
    </div>


            
        </div>
    </div>


            
        </div>
    </div>


            
        </div>
    </div>


            
                


    
    <div class="comment hasChildren">
        <p><a href="https://news.ycombinator.com/user?id=Ensorceled" target="_blank">Ensorceled</a>   <span class="timeago" data-date="2021-08-18 12:05:35 &#43;0000 UTC">2021-08-18 12:05:35 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            &gt; I also think that those downvoting you might&#x27;ve applied the principle of charity and taken the best interpretation of what you&#x27;ve written or at least ask.<p>There are far too many people who assume&#x2F;assert false accusations of rape are the norm for the principle of charity to apply here.
        </div>
        <div class="children">
            
                


    
    <div class="comment ">
        <p><a href="https://news.ycombinator.com/user?id=brigandish" target="_blank">brigandish</a>   <span class="timeago" data-date="2021-08-19 04:36:14 &#43;0000 UTC">2021-08-19 04:36:14 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            Is it the norm <i>here</i>? If not, then I suggest the principle should apply here.
        </div>
        <div class="children">
            
        </div>
    </div>


            
        </div>
    </div>


            
        </div>
    </div>


            
                


    
    <div class="comment hasChildren">
        <p><a href="https://news.ycombinator.com/user?id=OJFord" target="_blank">OJFord</a>   <span class="timeago" data-date="2021-08-18 11:06:31 &#43;0000 UTC">2021-08-18 11:06:31 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            Vouched, because as I understand the comment, people must not be reading past &#x27;rape&#x27; and just gut-flagging with completely the wrong impression.
        </div>
        <div class="children">
            
                


    
    <div class="comment hasChildren">
        <p><a href="https://news.ycombinator.com/user?id=account42" target="_blank">account42</a>   <span class="timeago" data-date="2021-08-18 11:36:46 &#43;0000 UTC">2021-08-18 11:36:46 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            &gt; people must not be reading past &#x27;rape&#x27; and just gut-flagging with completely the wrong impression.<p>Which is pretty ironic, considering that kind of reaction is exactly what the comment is about.
        </div>
        <div class="children">
            
                


    
    <div class="comment ">
        <p><a href="https://news.ycombinator.com/user?id=craftinator" target="_blank">craftinator</a>   <span class="timeago" data-date="2021-08-18 12:38:10 &#43;0000 UTC">2021-08-18 12:38:10 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            Build it and they will come.
        </div>
        <div class="children">
            
        </div>
    </div>


            
        </div>
    </div>


            
        </div>
    </div>


            
                


    
    <div class="comment ">
        <p><a href="https://news.ycombinator.com/user?id=da_big_ghey" target="_blank">da_big_ghey</a>   <span class="timeago" data-date="2021-08-18 19:45:27 &#43;0000 UTC">2021-08-18 19:45:27 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            Indeed yes, for instance statement like &quot;President denies allegations of beating his wife&quot; is damming all by itself.
        </div>
        <div class="children">
            
        </div>
    </div>


            
                


    
    <div class="comment ">
        <p><a href="https://news.ycombinator.com/user?id=thulle" target="_blank">thulle</a>   <span class="timeago" data-date="2021-08-18 11:52:10 &#43;0000 UTC">2021-08-18 11:52:10 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            Huh? In what way has the accusations against Assange been disproven? Is this one of the &quot;if he didn&#x27;t jump someone random it isn&#x27;t rape&quot;-arguments?
        </div>
        <div class="children">
            
        </div>
    </div>


            
        </div>
    </div>


        
            


    
    <div class="comment ">
        <p><a href="https://news.ycombinator.com/user?id=pthrowaway9000" target="_blank">pthrowaway9000</a>   <span class="timeago" data-date="2021-08-18 14:03:31 &#43;0000 UTC">2021-08-18 14:03:31 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            Told ya.
Sent from my S5 running lineageOS since 2015, now with MicroG.
        </div>
        <div class="children">
            
        </div>
    </div>


        
            


    
    <div class="comment ">
        <p><a href="https://news.ycombinator.com/user?id=sandworm101" target="_blank">sandworm101</a>   <span class="timeago" data-date="2021-08-18 13:12:53 &#43;0000 UTC">2021-08-18 13:12:53 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            Every bad day for Apple is another great day for Linux.<p>Smug mode activated.
        </div>
        <div class="children">
            
        </div>
    </div>


        
            


    
    <div class="comment ">
        <p><a href="https://news.ycombinator.com/user?id=farmerstan" target="_blank">farmerstan</a>   <span class="timeago" data-date="2021-08-18 14:06:58 &#43;0000 UTC">2021-08-18 14:06:58 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            Apple should be embarrassed for itself. The “one in trillion hash” was cracked in a week. Everyone associated with that should be fired.
        </div>
        <div class="children">
            
        </div>
    </div>


        
            


    
    <div class="comment hasChildren">
        <p><a href="https://news.ycombinator.com/user?id=rootsudo" target="_blank">rootsudo</a>   <span class="timeago" data-date="2021-08-18 13:21:30 &#43;0000 UTC">2021-08-18 13:21:30 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            And everyone was saying how <i>impossible</i> this was.<p>Not an expert, but I do enjoy how quickly this was absconded and disproven.
        </div>
        <div class="children">
            
                


    
    <div class="comment ">
        <p><a href="https://news.ycombinator.com/user?id=Ajedi32" target="_blank">Ajedi32</a>   <span class="timeago" data-date="2021-08-18 13:30:23 &#43;0000 UTC">2021-08-18 13:30:23 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            Impossible (or nearly so) for a cryptographically secure hash function, not a perceptual hash. When the stories on this first broke it wasn&#x27;t entirely clear which type of hash Apple was using for their CSAM detection. (I assume now we know they&#x27;re using NeuralHash?)
        </div>
        <div class="children">
            
        </div>
    </div>


            
        </div>
    </div>


        
            


    
    <div class="comment ">
        <p><a href="https://news.ycombinator.com/user?id=guywhocodes" target="_blank">guywhocodes</a>   <span class="timeago" data-date="2021-08-18 13:26:43 &#43;0000 UTC">2021-08-18 13:26:43 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            I was on the fence on this one because of Alex Stamos statements on this. I did expect to be able to trust him and in extension the Stanford Internet Observatory more.
        </div>
        <div class="children">
            
        </div>
    </div>


        
            


    
    <div class="comment ">
        <p><a href="https://news.ycombinator.com/user?id=Traubenfuchs" target="_blank">Traubenfuchs</a>   <span class="timeago" data-date="2021-08-18 13:41:43 &#43;0000 UTC">2021-08-18 13:41:43 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            Nothing about this ever made any sense. Any pedophile with an IQ over room temperature already deleted their iCloud at this point, with all the media buzz about it.
        </div>
        <div class="children">
            
        </div>
    </div>


        
            


    
    <div class="comment hasChildren">
        <p><a href="https://news.ycombinator.com/user?id=wisienkas" target="_blank">wisienkas</a>   <span class="timeago" data-date="2021-08-18 11:26:08 &#43;0000 UTC">2021-08-18 11:26:08 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            Not knowing too much of the NeuralHash model, but why are they using MD5 hash, they are known to have many collisions. 
We don&#x27;t use MD5 for private&#x2F;public keys for the same reason
        </div>
        <div class="children">
            
                


    
    <div class="comment hasChildren">
        <p><a href="https://news.ycombinator.com/user?id=enedil" target="_blank">enedil</a>   <span class="timeago" data-date="2021-08-18 11:43:02 &#43;0000 UTC">2021-08-18 11:43:02 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            We don&#x27;t use md5 for private&#x2F;public keys because md5 is a hashing algorithm, unrelated completely to encryption. Also, what are your reasons to believe that md5 has been used there?
        </div>
        <div class="children">
            
                


    
    <div class="comment ">
        <p><a href="https://news.ycombinator.com/user?id=Grollicus" target="_blank">Grollicus</a>   <span class="timeago" data-date="2021-08-18 11:58:21 &#43;0000 UTC">2021-08-18 11:58:21 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            Hashes are generally a part of the signature generation used with certificates. See for example &quot;What role do hashes play in TLS&#x2F;SSL certificate validation?&quot; -&gt; <a href="https:&#x2F;&#x2F;security.stackexchange.com&#x2F;questions&#x2F;67512&#x2F;what-role-do-hashes-play-in-tls-ssl-certificate-validation" rel="nofollow">https:&#x2F;&#x2F;security.stackexchange.com&#x2F;questions&#x2F;67512&#x2F;what-role...</a><p>In certificates, md5 - and sha1 - was used quite some time after it was known to be weak, I suspect OP was thinking of that.<p>This article seems to give a good summary what happened with sha1, mentions md5 in passing and links the related chromium issue: <a href="https:&#x2F;&#x2F;konklone.com&#x2F;post&#x2F;why-google-is-hurrying-the-web-to-kill-sha-1" rel="nofollow">https:&#x2F;&#x2F;konklone.com&#x2F;post&#x2F;why-google-is-hurrying-the-web-to-...</a>
        </div>
        <div class="children">
            
        </div>
    </div>


            
        </div>
    </div>


            
                


    
    <div class="comment hasChildren">
        <p><a href="https://news.ycombinator.com/user?id=Retr0id" target="_blank">Retr0id</a>   <span class="timeago" data-date="2021-08-18 12:04:05 &#43;0000 UTC">2021-08-18 12:04:05 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            What makes you think they&#x27;re using MD5 anywhere?<p>Even if they were, it wouldn&#x27;t matter, because NeuralHash is non-cryptographic by design.
        </div>
        <div class="children">
            
                


    
    <div class="comment hasChildren">
        <p><a href="https://news.ycombinator.com/user?id=wisienkas" target="_blank">wisienkas</a>   <span class="timeago" data-date="2021-08-19 08:34:22 &#43;0000 UTC">2021-08-19 08:34:22 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            I looked at the link and looked at the output from the algorithm for the 2 images which was a MD5 hash. so from that :)
        </div>
        <div class="children">
            
                


    
    <div class="comment ">
        <p><a href="https://news.ycombinator.com/user?id=Retr0id" target="_blank">Retr0id</a>   <span class="timeago" data-date="2021-08-19 14:30:27 &#43;0000 UTC">2021-08-19 14:30:27 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            But it isn&#x27;t MD5. It&#x27;s not even the same length as an MD5 hash. I am confused by your reasoning.
        </div>
        <div class="children">
            
        </div>
    </div>


            
        </div>
    </div>


            
        </div>
    </div>


            
        </div>
    </div>


        
            


    
    <div class="comment hasChildren">
        <p><a href="https://news.ycombinator.com/user?id=zaptheimpaler" target="_blank">zaptheimpaler</a>   <span class="timeago" data-date="2021-08-18 10:27:52 &#43;0000 UTC">2021-08-18 10:27:52 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            This is so overblown. Scanning images for CSAM seems to be a requirement followed by Facebook, Google, Insta and Snap already [1]:<p>&gt; To put this in perspective, in 2019 Facebook reported 65 million instances of CSAM on its platform, according to The New York Times. Google reported 3.5 million photos and videos, while Twitter and Snap reported “more than 100,000,” Apple, on the other hand, reported 3,000 photos.<p>ALL of those services are already scanning all your photos server side implying complete access to the photo for other purposes.<p>Apple went above and beyond what anyone else does and moved the scanning to be client side! I can&#x27;t believe they are getting shit for this. They are doing MORE than anyone else to protect your privacy, while still complying with federal laws. If you object to this, you should object to all cloud based photo storage services, and with the federal laws. Apple not only made it more private, but also transparently announced what they do and they are getting shit for it.<p>[1] <a href="https:&#x2F;&#x2F;www.engadget.com&#x2F;apple-child-safety-csam-detection-explainer-183029927.html" rel="nofollow">https:&#x2F;&#x2F;www.engadget.com&#x2F;apple-child-safety-csam-detection-e...</a>
        </div>
        <div class="children">
            
                


    
    <div class="comment hasChildren">
        <p><a href="https://news.ycombinator.com/user?id=kmbfjr" target="_blank">kmbfjr</a>   <span class="timeago" data-date="2021-08-18 11:08:34 &#43;0000 UTC">2021-08-18 11:08:34 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            It is not overblown.  I refuse to be a perpetual suspect in the possession and transfer of child pornography.  Having these checks on my phone is just a short hop to checking everything the camera sees.<p>I worked with a guy who was convicted of this very crime.  Do you know what the FBI installs on his computer and phone?  This kind of monitoring.<p>I am not going to be treated like I am on parole.
        </div>
        <div class="children">
            
                


    
    <div class="comment ">
        <p><a href="https://news.ycombinator.com/user?id=raxxorrax" target="_blank">raxxorrax</a>   <span class="timeago" data-date="2021-08-18 15:23:36 &#43;0000 UTC">2021-08-18 15:23:36 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            To form a collision here you basically use a partial image of the original. What it can do is leave some conclusion about the workings of their blackbox algorithm as the collision image ripped those parts that probably have no influence on the resulting hash. There will likely be more though. As long as you don&#x27;t have the original images, it will be hard to create &quot;fake&quot; ones.
        </div>
        <div class="children">
            
        </div>
    </div>


            
        </div>
    </div>


            
                


    
    <div class="comment hasChildren">
        <p><a href="https://news.ycombinator.com/user?id=WA" target="_blank">WA</a>   <span class="timeago" data-date="2021-08-18 10:40:35 &#43;0000 UTC">2021-08-18 10:40:35 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            Is it so hard to understand? Some people don’t use cloud storage for precisely the reason that the photos are not encrypted.<p>Now they can’t even use their phone for storing photos.<p>The thing with &quot;only when iCloud is enabled&quot; is only for now. It’s trivial to make Scanning all photos default in a future version.
        </div>
        <div class="children">
            
                


    
    <div class="comment hasChildren">
        <p><a href="https://news.ycombinator.com/user?id=nyuszika7h" target="_blank">nyuszika7h</a>   <span class="timeago" data-date="2021-08-18 10:53:28 &#43;0000 UTC">2021-08-18 10:53:28 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            That would require a software update and would definitely not go unnoticed. Would you rather they implement scanning on server side and never be able to enable end-to-end encryption for iCloud Photos? I imagine that might be the end goal, otherwise I don&#x27;t see why they wouldn&#x27;t have just done it on server side.<p>Sure, this system still has the potential to be abused, but if I had to choose between &quot;end-to-end encrypted with local scanning before upload, which can maybe be abused but has a higher chance of being noticed if it is, and can be disabled with a jailbreak if you&#x27;re really paranoid&quot; and &quot;not end-to-end encrypted, Apple can inspect my whole photo library whenever they desire, which can go completely unnoticed due to gag orders forcing them to hand data over to governments, not even requiring a software update, and you are completely powerless to do anything about it except for not using iCloud Photos at all&quot;, I would definitely choose the former.
        </div>
        <div class="children">
            
                


    
    <div class="comment ">
        <p><a href="https://news.ycombinator.com/user?id=sodality2" target="_blank">sodality2</a>   <span class="timeago" data-date="2021-08-18 12:07:40 &#43;0000 UTC">2021-08-18 12:07:40 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            False dichotomy. You should UNEQUIVOCALLY not be FORCED to choose one of those. You shouldn&#x27;t choose one of those at all. I do not need to be treated like a criminal.<p>Facebook et al scans server side because they have liability. Tell me why apple thinks they need to scan locally? They dont have liability, which is even worse. It means they&#x27;re doing it for other reasons. Plenty of people read that as &quot;wow  apple is so kindhearted they did this without the business incentive&quot;. And you know what? They didn&#x27;t. They might even have good intentions. But, as the saying goes, the path to hell is paved with good intentions.
        </div>
        <div class="children">
            
        </div>
    </div>


            
                


    
    <div class="comment ">
        <p><a href="https://news.ycombinator.com/user?id=WA" target="_blank">WA</a>   <span class="timeago" data-date="2021-08-18 11:15:53 &#43;0000 UTC">2021-08-18 11:15:53 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            Strawman.<p>I choose no cloud storage plus device that doesn’t scan my data.<p>You argue for the former, which isn’t implemented, vs the latter, which I assume is reality anyways for all cloud storage providers.<p>We can have this discussion if Apple implements the former. If this was the goal, they would’ve announced it like you suggested.
        </div>
        <div class="children">
            
        </div>
    </div>


            
                


    
    <div class="comment ">
        <p><a href="https://news.ycombinator.com/user?id=Hackbraten" target="_blank">Hackbraten</a>   <span class="timeago" data-date="2021-08-18 11:23:38 &#43;0000 UTC">2021-08-18 11:23:38 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            &gt; and can be disabled with a jailbreak if you&#x27;re really paranoid<p>Except that if I’m really paranoid, I’m not going to skip security updates. And those updates will probably render known jailbreak exploits useless.
        </div>
        <div class="children">
            
        </div>
    </div>


            
        </div>
    </div>


            
        </div>
    </div>


            
                


    
    <div class="comment ">
        <p><a href="https://news.ycombinator.com/user?id=CrimsonRain" target="_blank">CrimsonRain</a>   <span class="timeago" data-date="2021-08-18 12:59:25 &#43;0000 UTC">2021-08-18 12:59:25 &#43;0000 UTC</span> [ - ]</p>
        <div class="body">
            You&#x27;re wrong. Scanning is NOT required by US law. The law says IF [1] you know about CSAM, then you MUST report. If you don&#x27;t know, you don&#x27;t have to report. And it is not your legal duty scan. Law even has privacy sections [2]. But if you DO scan, then you must report.
 So all these CSAM scan is bullshit. Companies can stop scanning if they want to.<p>[1] 18 US Code 2258A<p>[2] 18 US Code 2258A -&gt; F.1, F.2, F.3
        </div>
        <div class="children">
            
        </div>
    </div>


            
        </div>
    </div>


        
    
</article>

    </main>
    <footer>
        <span class="h-logo">&copy; Hugo Hacker News</span><br/>
        Site created By <a href="https://davidejones.com" target="_blank">David E Jones</a> Powered by <a href="https://gohugo.io/" target="_blank">Hugo</a> and the <a href="https://github.com/HackerNews/API" target="_blank">Hacker News api</a>.
        <ul>
            
                <li><a href="/hugo-hn/guidelines/">Guidelines</a></li>
            
                <li><a href="/hugo-hn/faq/">FAQ</a></li>
            
                <li><a href="mailto:hn@ycombinator.com">Support</a></li>
            
                <li><a href="https://github.com/HackerNews/API">API</a></li>
            
                <li><a href="/hugo-hn/security/">Security</a></li>
            
                <li><a href="/hugo-hn/lists/">Lists</a></li>
            
                <li><a href="https://news.ycombinator.com/bookmarklet.html">Bookmarklet</a></li>
            
                <li><a href="/hugo-hn/dmca/">DMCA</a></li>
            
                <li><a href="http://www.ycombinator.com/apply/">Apply to YC</a></li>
            
                <li><a href="mailto:hn@ycombinator.com">Contact</a></li>
            
        </ul>
    </footer>

    
    
    
    
    
    
    <script type="text/javascript" src="https://davidejones.github.io/hugo-hn/main.01d140732eee0e8adfdb7a7f714755097c6676bfb8e8bf27645ce342b2ed12a481b08f6838c413c20bff3acf20f6159b3336339a220ff5ec5e45eb7877106361.js"  integrity="sha512-AdFAcy7uDorf23p/cUdVCXxmdr&#43;46L8nZFzjQrLtEqSBsI9oOMQTwgv/Os8g9hWbMzYzmiIP9exeRet4dxBjYQ=="  crossorigin="anonymous" defer></script>
</body>
</html>

